\begin{savequote}[45mm]
\ascii{Any fool can write code that a computer can understand. Good programmers write code that humans can understand.}
\qauthor{\ascii{- Martin Flower}}
\end{savequote}

\chapter{システムアーキテクチャ} 
\label{ch:architecture}

\begin{content}

本章では\tf{}のシステムアーキテクチャについて説明し、簡単な例を用いてグラフ構造の変換プロセスを説明します。最後に、セッション管理のメカニズムを掘り下げることで、\tf{}ランタイムの動作原理についての理解を深めます。

\end{content}

\section{システムアーキテクチャ}
	
\begin{content}

\refig{tf-architecture}に示すように、\tf{}のシステム構造は\ascii{C API}を境界として、システム全体を\emph{フロントエンド}と\emph{バックエンド}の2つのサブシステムに分割しています\footnote{実際には、バックエンドシステムにも\ascii{Client}のコードが存在し、フロントエンドシステムは\tf{}の外部プログラミングインターフェースです。後の章で、この問題について詳しく説明します。}。

\begin{enum}
  \eitem{フロントエンドシステム：プログラミングモデルを提供し、計算グラフの構築を担当します。}
  \eitem{バックエンドシステム：ランタイム環境を提供し、計算グラフの実行を担当します。} 
\end{enum}

\tf{}のシステム設計は優れた層状アーキテクチャに従っており、バックエンドシステムの設計と実装はさらに\ascii{4}層に分解できます。

\begin{enum}
  \eitem{ランタイム：ローカルモードと分散モードをそれぞれ提供し、大部分の設計と実装を共有しています。}
  \eitem{計算層：各\ascii{OP}の\ascii{Kernel}実装で構成されています。ランタイム時に、\ascii{Kernel}実装が\ascii{OP}の具体的な数学計算を実行します。} 
  \eitem{通信層：\ascii{gRPC}ベースでコンポーネント間のデータ交換を実現し、\ascii{IB}ネットワークをサポートするノード間で\ascii{RDMA}通信を実現できます。}
  \eitem{デバイス層：計算デバイスは\ascii{OP}実行の主要な担い手であり、\tf{}は多様な異種計算デバイスタイプをサポートしています。}
\end{enum}

グラフ操作の観点からシステムの動作を見ると、\tf{}ランタイムは計算グラフの構築、編成、および実行を完了します。

\begin{enum}
  \eitem{グラフの表現：計算グラフを構築しますが、グラフは実行しません。}
  \eitem{グラフの編成：計算グラフのノードを最適な実行計画でクラスター内の各計算デバイスに配置して実行します。} 
  \eitem{グラフの実行：トポロジカルソートに従ってグラフ内のノードを実行し、各\ascii{OP}の\ascii{Kernel}計算を開始します。}   
\end{enum}

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{figures/tf-architecture.png}
\caption{TensorFlowシステムアーキテクチャ}
 \label{fig:tf-architecture}
\end{figure}

\subsection{Client}

\ascii{Client}はフロントエンドシステムの主要な構成部分であり、多言語をサポートするプログラミング環境です。\ascii{Client}は\ascii{TensorFlow}のプログラミングインターフェースに基づいて計算グラフを構築します。現在、\ascii{TensorFlow}は\ascii{Python}と\ascii{C++}のプログラミングインターフェースが比較的完全であり、特に\ascii{Python}の\ascii{API}サポートが最も包括的です。また、他のプログラミング言語の\ascii{API}サポートも日々向上しています。

この時点では、\ascii{TensorFlow}はグラフ計算を実行せず、バックエンド計算エンジンと\ascii{Session}を確立し、\ascii{Session}をブリッジとして\ascii{Client}と\ascii{Master}の間のチャネルを確立し、\ascii{Protobuf}形式の\ascii{GraphDef}をシリアル化して\ascii{Master}に渡し、計算グラフの実行プロセスを開始するまで待機します。

\subsection{Master}

分散ランタイム環境では、\ascii{Client}が\code{Session.run}を実行すると、完全な計算グラフがバックエンドの\ascii{Master}に渡されます。この時点で、計算グラフは完全であり、通常\emph{\ascii{Full Graph}}と呼ばれます。その後、\ascii{Master}は\code{Session.run}から渡された\code{fetches, feeds}パラメータリストに基づいて\ascii{Full Graph}を逆方向にトラバースし、依存関係に従って剪定を行い、最終的に最小の依存サブグラフ（通常\ascii{Client Graph}と呼ばれる）を計算します。

次に、\ascii{Master}は\ascii{Client Graph}をタスク名に基づいて複数の\ascii{Graph Partition}に分割(\code{SplitByTask})します。各\ascii{Worker}は1つの\ascii{Graph Partition}に対応します。その後、\ascii{Master}は\ascii{Graph Partition}をそれぞれ対応する\ascii{Worker}に登録し、異なる\ascii{Worker}上でこれらの\ascii{Graph Partition}を並行して実行できるようにします。最後に、\ascii{Master}はすべての\ascii{Work}に通知して、対応する\ascii{Graph Partition}の実行プロセスを開始します。

\ascii{Work}間にデータ依存関係が存在する可能性がありますが、\ascii{Master}は両者間のデータ交換に関与せず、両者が互いに通信して独立してデータ交換を完了し、すべての計算が完了するまで待機します。

\subsection{Worker}

各タスクに対して、\tf{}は1つの\ascii{Worker}インスタンスを起動します。\ascii{Worker}は主に以下の\ascii{3}つの責任を担います：

\begin{enum}
  \eitem{\ascii{Master}からのリクエストを処理します。}
  \eitem{登録された\ascii{Graph Partition}をローカル計算デバイスセットに基づいて二次分割(\code{SplitByDevice})し、各計算デバイスに通知して各\ascii{Graph Partition}を並行して実行します。}
  \eitem{トポロジカルソートアルゴリズムに従って特定の計算デバイス上でローカルサブグラフを実行し、\ascii{OP}の\ascii{Kernel}実装をスケジュールします。} 
  \eitem{タスク間のデータ通信を調整します。}
\end{enum}

まず、\ascii{Worker}は\ascii{Master}から送信されたグラフ実行コマンドを受信します。この時点での計算グラフは\ascii{Worker}に対して完全であり、\ascii{Full Graph}とも呼ばれます。これは\ascii{Master}の1つの\ascii{Graph Partition}に対応します。次に、\ascii{Worker}は現在利用可能なハードウェア環境（\ascii{(GPU/CPU)}リソースを含む）に基づいて、\ascii{OP}デバイスの制約仕様に従って、グラフをさらに複数の\ascii{Graph Partition}に分割\code{(SplitByDevice)}します。各計算デバイスは1つの\ascii{Graph Partition}に対応します。その後、\ascii{Worker}はすべての\ascii{Graph Partition}の実行を開始します。最後に、各計算デバイスに対して、\ascii{Worker}は計算グラフ内のノード間の依存関係に従ってトポロジカルソートアルゴリズムを実行し、順次\ascii{OP}の\ascii{Kernel}実装を呼び出して\ascii{OP}の計算を完了します（典型的な多態実装技術）。

また、\ascii{Worker}は\ascii{OP}の計算結果を他の\ascii{Worker}に送信したり、他の\ascii{Worker}から送信された計算結果を受信したりして、\ascii{Worker}間のデータ相互作用を実現する責任があります。\tf{}は送信元デバイスと宛先デバイス間の\ascii{Send/Recv}を特別に実装しています。

\begin{enum}
  \eitem{ローカル\ascii{CPU}と\ascii{GPU}間では、\code{cudaMemcpyAsync}を使用して非同期コピーを実現します。}
  \eitem{ローカル\ascii{GPU}間では、エンドツーエンドの\ascii{DMA}操作を使用し、ホスト側\ascii{CPU}のコピーを回避します。} 
\end{enum}

タスク間の通信について、\tf{}は複数の通信プロトコルをサポートしています。

\begin{enum}
  \eitem{\ascii{gRPC over TCP}}
  \eitem{\ascii{RDMA over Converged Ethernet}} 
\end{enum}

さらに、\tf{}は\ascii{cuNCCL}ライブラリのサポートを開始し、複数の\ascii{GPU}間の通信を改善しています。

\subsection{Kernel}

\ascii{Kernel}は特定のハードウェアデバイス上での\ascii{OP}の特定の実装であり、\ascii{OP}の具体的な計算を実行する責任があります。現在、\ascii{TensorFlow}システムには\ascii{200}以上の標準\ascii{OP}が含まれており、数値計算、多次元配列操作、制御フロー、状態管理などが含まれています。

一般的に、各\ascii{OP}にはデバイスタイプに応じて最適化された\ascii{Kernel}実装が存在します。ランタイム時に、ランタイムは\ascii{OP}のデバイス制約仕様とローカルデバイスのタイプに基づいて、\ascii{OP}の特定の\ascii{Kernel}実装を選択し、その\ascii{OP}の計算を完了します。

多くの\ascii{Kernel}は\code{Eigen::Tensor}に基づいて実装されています。\code{Eigen::Tensor}は\ascii{C++}テンプレート技術を使用して、マルチコア\ascii{CPU/GPU}向けに効率的な並行コードを生成します。ただし、\ascii{TensorFlow}は柔軟に\ascii{cuDNN, cuNCCL, cuBLAS}を直接使用して、より効率的な\ascii{Kernel}を実装することもできます。

さらに、\ascii{TensorFlow}はベクトル化技術を実装し、高スループット、データ中心のアプリケーション要件や、モバイルデバイスでより効率的な推論を実現しています。複合\ascii{OP}のサブ計算プロセスの表現が困難な場合や、実行効率が低い場合、\ascii{TensorFlow}はさらに効率的な\ascii{Kernel}登録をサポートしており、その拡張性は非常に優れています。

\end{content}

\section{グラフ制御}

\begin{content}

最も簡単な例を通じて、さらに糸を解きほぐし、徐々に\tf{}計算グラフの制御と実行メカニズムを掘り下げていきます。

\subsection{クラスターの構築}

\refig{tf-1ps-1worker}に示すように、簡単な分散環境が存在すると仮定します：\ascii{1 PS + 1 Worker}、そしてこれを2つのタスクに分割します：

\begin{enum}
  \eitem{\ascii{ps0}: \code{/job:ps/task:0}でマークされ、モデルパラメータの保存と更新を担当します。}
  \eitem{\ascii{worker0}: \code{/job:worker/task:0}でマークされ、モデルのトレーニングを担当します。} 
\end{enum}

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/tf-1ps-1worker.png}
\caption{TensorFlowクラスター：\ascii{1 PS + 1 Worker}}
 \label{fig:tf-1ps-1worker}
\end{figure}

\subsection{グラフ構築}

\refig{tf-graph-construction}に示すように、\ascii{Client}は簡単な計算グラフを構築しました。まず、$w$と$x$を行列乗算し、次に切片$b$と要素ごとに加算し、最後に$s$に更新します。

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/tf-graph-construction.png}
\caption{グラフ構築}
 \label{fig:tf-graph-construction}
\end{figure}

\subsection{グラフ実行}
\refig{tf-graph-execution}に示すように、まず\ascii{Client}は\code{Session}インスタンスを作成し、\ascii{Master}との間のチャネルを確立します。次に、\ascii{Client}は\code{Session.run}を呼び出して計算グラフを\ascii{Master}に渡します。
その後、\ascii{Master}は1回の\ascii{Step}のグラフ計算プロセスを開始します。実行前に、\ascii{Master}は\emph{共通部分式の削除}や\emph{定数畳み込み}などの一連の最適化技術を適用します。最後に、\ascii{Master}はタスク間の協調を担当し、最適化された計算グラフを実行します。
\begin{figure}[!htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/tf-graph-execution.png}
\caption{グラフ実行}
\label{fig:tf-graph-execution}
\end{figure}
\subsubsection{グラフ分割}
\refig{tf-graph-split-by-task}に示すように、合理的なグラフ分割アルゴリズムが存在します。\ascii{Master}はモデルパラメータに関連する\ascii{OP}を1つのグループにまとめ、\ascii{ps0}タスク上に配置します。他の\ascii{OP}は別のグループにまとめられ、\ascii{worker0}タスク上で実行されます。
\begin{figure}[!htbp]
\centering
\includegraphics[width=1.0\textwidth]{figures/tf-graph-split-by-task.png}
\caption{グラフ分割：タスクによる分割}
\label{fig:tf-graph-split-by-task}
\end{figure}
\subsubsection{サブグラフ登録}
\refig{tf-register-graph}に示すように、グラフ分割プロセス中に計算グラフのエッジがノードまたはデバイスをまたぐ場合、\ascii{Master}はそのエッジを分割し、2つのノードまたはデバイス間に\ascii{Send}と\ascii{Recv}ノードを挿入してデータの転送を実現します。
ここで、\code{Send}と\code{Recv}ノードも\ascii{OP}ですが、特別な\ascii{OP}であり、内部ランタイムによって管理・制御され、ユーザーには見えません。また、これらはデータ通信のみに使用され、データ計算のロジックは含まれていません。
最後に、\ascii{Master}は\code{RegisterGraph}インターフェースを呼び出してサブグラフを対応する\ascii{Worker}に登録し、対応する\ascii{Worker}が計算を実行する責任を負います。
\begin{figure}[!htbp]
\centering
\includegraphics[width=1.0\textwidth]{figures/tf-register-graph.png}
\caption{サブグラフ登録：SendとRecvノードの挿入}
\label{fig:tf-register-graph}
\end{figure}
\subsubsection{サブグラフ計算}
\refig{tf-run-graph}に示すように、\ascii{Master}は\code{RunGraph}インターフェースを呼び出して、すべての\ascii{Worker}にサブグラフ計算の実行を通知します。ここで、\ascii{Worker}間は\code{RecvTensor}インターフェースを呼び出してデータ交換を完了できます。
\begin{figure}[!htbp]
\centering
\includegraphics[width=1.0\textwidth]{figures/tf-run-graph.png}
\caption{サブグラフ実行}
\label{fig:tf-run-graph}
\end{figure}
\end{content}
\section{セッション管理}
\begin{content}
次に、セッションの全ライフサイクルプロセスとグラフ制御との関連性を概説することで、ランタイムの内部動作メカニズムをさらに明らかにします。
\subsection{セッションの作成}
まず、\ascii{Client}が\emph{初めて}\code{tf.Session.run}を実行するとき、グラフ全体をシリアル化し、\ascii{gRPC}を通じて\code{CreateSessionRequest}メッセージを送信して、グラフを\ascii{Master}に渡します。
その後、\ascii{Master}は\code{MasterSession}インスタンスを作成し、グローバルに一意な\code{handle}で識別し、最終的に\code{CreateSessionResponse}を通じて\ascii{Client}に返します。\refig{tf-create-session-overview}に示すとおりです。
\begin{figure}[!h]
\centering
\includegraphics[width=0.7\textwidth]{figures/tf-create-session-overview.png}
\caption{セッションの作成}
\label{fig:tf-create-session-overview}
\end{figure}
\subsection{反復実行}
その後、\ascii{Client}は反復実行プロセスを開始し、各反復を1回の\ascii{Step}と呼びます。このとき、\ascii{Client}は\code{RunStepRequest}メッセージを\ascii{Master}に送信し、メッセージには\code{handle}識別子が含まれ、\ascii{Master}が対応する\code{MasterSession}インスタンスをインデックスするために使用されます。\refig{tf-run-step-overview}に示すとおりです。
\begin{figure}[!h]
\centering
\includegraphics[width=1.0\textwidth]{figures/tf-run-step-overview.png}
\caption{反復実行}
\label{fig:tf-run-step-overview}
\end{figure}
\subsubsection{サブグラフ登録}
\ascii{Master}が\code{RunStepRequest}メッセージを受信すると、グラフの剪定、分割、最適化などの操作を実行します。最終的に、タスク\ascii{(Task)}に従ってグラフを複数のサブグラフ断片\ascii{(Graph Partition)}に分割します。その後、\ascii{Master}は各\ascii{Worker}に\code{RegisterGraphRequest}メッセージを送信し、サブグラフ断片を各\ascii{Worker}ノードに順次登録します。
\ascii{Worker}が\code{RegisterGraphRequest}メッセージを受信すると、再度分割操作を実施し、最終的にデバイス\ascii{(Device)}に従ってグラフを複数のサブグラフ断片\ascii{(Graph Partition)}に分割します。\footnote{分散ランタイムでは、グラフ分割は2段階の分割プロセスを経ます。\ascii{Master}上ではタスクに従って分割し、\ascii{Worker}上ではデバイスに従って分割します。したがって、得られる結果はどちらもサブグラフ断片と呼ばれますが、範囲とサイズの違いのみが存在します。}
\ascii{Worker}がサブグラフ登録を完了すると、\code{RegisterGraphReponse}メッセージを返し、\code{graph_handle}識別子を含めます。これは、\ascii{Worker}が複数のサブグラフを並行して登録・実行できるため、各サブグラフを\code{graph_handle}で一意に識別するためです。
\subsubsection{サブグラフ実行}
\ascii{Master}がサブグラフ登録を完了すると、すべての\ascii{Worker}にブロードキャストしてすべてのサブグラフを並行実行します。このプロセスは、\ascii{Master}が\code{RunGraphRequest}メッセージを\ascii{Worker}に送信することで完了します。メッセージには\code{(session_handle, graph_handle, step_id)}三つ組の識別情報が含まれ、\ascii{Worker}が対応するサブグラフをインデックスするために使用されます。
\ascii{Worker}が\code{RunGraphRequest}メッセージを受信すると、\ascii{Worker}は\code{graph_handle}に基づいて対応するサブグラフをインデックスします。最終的に、\ascii{Worker}はローカルのすべての計算デバイスを起動してすべてのサブグラフを並行実行します。各サブグラフは個別の\code{Executor}内で実行され、\code{Executor}はトポロジカルソートアルゴリズムに従ってサブグラフ断片の計算を完了します。上記のアルゴリズムは以下のコードで形式的に記述できます。
\begin{leftbar}
\begin{python}
\end{python}
def run_partitions(rendezvous, executors_and_partitions, inputs, outputs):
  rendezvous.send(inputs)
  for (executor, partition) in executors_and_partitions: 
    executor.run(partition)
  rendezvous.recv(outputs)
\end{leftbar}
\subsubsection{データ交換}
2つのデバイス間でデータ交換が必要な場合、\ascii{Send/Recv}ノードの挿入によって完了されます。特に、2つの\ascii{Worker}間でデータ交換が必要な場合、プロセス間通信が必要になります。
このとき、受信側が能動的に\code{RecvTensorRequest}メッセージを送信側に送信し、送信側のメールボックスから対応する\ascii{Tensor}を取り出し、\code{RecvTensorResponse}を通じて返す必要があります。\refig{tf-recv-tensor-overview}に示すとおりです。
\begin{figure}[!h]
\centering
\includegraphics[width=0.7\textwidth]{figures/tf-recv-tensor-overview.png}
\caption{Worker間のデータ交換}
\label{fig:tf-recv-tensor-overview}
\end{figure}
\subsection{セッションの終了}
計算が完了すると、\ascii{Client}は\ascii{Master}に\code{CloseSessionReq}メッセージを送信します。\ascii{Master}がメッセージを受信すると、\code{MasterSession}が保持するすべてのリソースの解放を開始します。\refig{tf-close-session-overview}に示すとおりです。
\begin{figure}[!h]
\centering
\includegraphics[width=0.7\textwidth]{figures/tf-close-session-overview.png}
\caption{セッションの終了}
\label{fig:tf-close-session-overview}
\end{figure}
\end{content}
