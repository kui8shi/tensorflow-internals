Here is the Japanese translation of the text, with LaTeX formatting preserved and code segments abbreviated:

\begin{savequote}[45mm]
\ascii{コンピューターが理解できるコードを書くのは誰にでもできる。優れたプログラマーは人間が理解できるコードを書く。}
\qauthor{\ascii{- マーティン・ファウラー}}
\end{savequote}

\chapter{分散TensorFlow} 
\label{ch:distributed}

\begin{content}

\tf{}は分散環境で実行でき、計算グラフの実行プロセスを完了することができます。本章では、分散ランタイムの基本アーキテクチャと実行メカニズムに焦点を当てます。さまざまなサービスプロセス間の相互作用関係について詳しく説明します。また、分散環境におけるグラフ操作とそのセッションのライフサイクル制御の主要技術について詳しく分析します。

\end{content}

\section{分散モード}

\begin{content}

分散モードでは、\ascii{Client}が計算グラフの構築を担当し、その後\code{Session.run}を呼び出して計算グラフの実行プロセスを開始します。

\ascii{Master}プロセスは、計算グラフ実行のメッセージを受信すると、計算グラフの剪定、分割、最適化などの操作を開始します。最終的に、サブグラフを各\ascii{Worker}プロセスに分配して登録し、その後各\ascii{Worker}プロセスの並行実行をトリガーします。

\ascii{Worker}プロセスは、サブグラフ登録のメッセージを受信すると、ローカルの計算デバイスリソースに基づいて、計算サブグラフをさらに二次分割し、サブグラフを各計算デバイスに割り当てます。最後に、各計算デバイスの並行サブグラフ実行を開始します。\ascii{Worker}間にデータ交換が存在する場合、プロセス間通信を通じて対話を完了できます。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/distributed.png}
\caption{分散モード}
 \label{fig:distributed}
\end{figure}

\subsection{グラフ操作}

図\refig{dist-runtime}に示すように、\code{run\_step}の実行過程には、計算グラフの剪定、分割、実行という3つの重要なグラフ操作が含まれています。そのうち、分散ランタイムでは、グラフ分割は2段階の分割プロセスを経験しています。

\begin{enum}
  \eitem{\code{MasterSession}によって完了される一次分割：\code{SplitByWorker}または\code{SplitByTask}に従ってグラフ分割プロセスを完了します。}
  \eitem{\code{WorkerSession}によって完了される二次分割：\code{SplitByDevice}に従ってグラフ分割プロセスを完了します。}
\end{enum}

分散モードでは、グラフの剪定も\tf{}の部分実行の設計理念を体現しています。一方、グラフの分割と実行も\tf{}の並行実行の設計理念を体現しています。そのうち、グラフの剪定は\ascii{Master}上でのみ発生し、\ascii{Worker}上では発生しません。一方、グラフの分割は\ascii{Master}と\ascii{Worker}の両方で発生します。グラフの実行は\ascii{Worker}上でのみ発生し、\ascii{Master}上では発生しません。

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{figures/dist-runtime.png}
\caption{分散：グラフ操作}
 \label{fig:dist-runtime}
\end{figure}

\subsubsection{グラフ分割}

分散ランタイムの動作原理をよりよく理解するために、簡単な例を使用してグラフ操作の具体的なプロセスを説明します。図\refig{dist-exp-1}に示すように、単純な計算グラフが存在し、\code{f, c, a}が\code{/job:ps/task:0}上に配置され、それぞれ\code{CPU0, CPU1, CPU2}上にスケジュールされているとします。\code{g, h}は\code{/job:worker/task:0}上に配置され、同時に\code{GPU0}上にスケジュールされています。\code{b, d, e}は\code{/job:worker/task:1}上に配置され、\code{d, e}は\code{GPU0}上にスケジュールされ、\code{b}は\code{GPU1}上にスケジュールされています。

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{figures/dist-exp-1.png}
\caption{分散：グラフ分割}
 \label{fig:dist-exp-1}
\end{figure}

\subsubsection{データ交換}

図\refig{dist-exp-2}に示すように、デバイスをまたぐエッジに対して、ランタイムは自動的にエッジの分割を実施し、送信側と受信側にそれぞれ\code{Send}と\code{Recv}の末端ノードを挿入します。

プロセス間の\code{Send}と\code{Recv}ノードは、\code{GrpcRemoteRendezvous}を通じてデータ交換を実現します。例えば、\code{/job:ps/task:0}と\code{/job:worker/task:0}、\code{/job:ps/task:0}と\code{/job:worker/task:1}、または\code{/job:worker/task:0}と\code{/job:worker/task:1}の間は\code{GrpcRemoteRendezvous}を通じてデータ交換を完了します。

一方、プロセス内の\code{Send}と\code{Recv}ノードは、\code{IntraProcessRendezvous}を通じてデータ交換を実現します。例えば、\code{/job:worker/task:1}内に2つの\code{GPU}が存在し、それらの間は\code{IntraProcessRendezvous}を使用してデータ交換を実現します。\code{Rendezvous}の具体的な実装プロセスについては、後述でさらに詳しく説明します。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/dist-exp-2.png}
\caption{分散：データ交換}
 \label{fig:dist-exp-2}
\end{figure}

\subsection{形式化}

実際のシステム実装では、分散ランタイムは\ascii{C++}で実装されています。そのうち、\tf{}ランタイムの重要なパスは\code{run\_step}です。実際のシステム実装には多くの詳細が含まれているため、アルゴリズムの主要部分とロジックを見つけるのは容易ではありません。問題の記述を簡略化するために、\code{run\_step}の実装プロセスを形式的に記述します。

\subsubsection{Master::RunStep}

\ascii{Master}上では、主に\code{FullGraph}の剪定操作を完了し、\code{ClientGraph}を生成します。次に、\ascii{Worker}に従って\code{ClientGraph}を複数の\code{PartitionGraph}に分割します。最後に、\code{PartitionGraph}リストを各\ascii{Worker}に登録し、各\ascii{Worker}の並行実行を開始して\code{PartitionGraph}リストを実行します。

\begin{leftbar}
\begin{python}
def run_step(workers, full_graph, inputs, outputs):
  client_graph = prune(full_graph, inputs, outputs)
  partition_graphs = split(client_graph, workers)
  register_graphs(partition_graphs, inputs, outputs)
  run_graphs(partition_graphs, inputs, outputs)
\end{python}
\end{leftbar}

\subsubsection{Worker::RunStep}

特定の\ascii{Worker}ノード上で、\code{RegisterGraphRequest}メッセージを受信すると、ローカルデバイスセットに従って計算グラフをさらに複数の\code{PartitionGraph}に分割します。次に、各計算デバイス上で\code{Executor}を起動し、それに割り当てられた\code{PartitionGraph}を実行します。

ある計算デバイスが割り当てられた\code{PartitionGraph}の実行を完了すると、\code{ExecutorBarrier}のカウンターが1増加し、すべてのデバイスが\code{PartitionGraph}リストの実行を完了するまで、\code{barrier.wait()}のブロック操作が終了します。

デバイスをまたぐ\code{PartitionGraph}間にデータ依存関係が存在する可能性があり、それらの間では\code{Send/Recv}ノードを挿入して対話を完了します。実際、分散ランタイムでは、\code{Send/Recv}は\code{RpcRemoteRendezvous}を通じてデータ交換を完了します。

% \code{Send}节点将调用\code{RpcRemoteRendezvous::Send}，它委托\code{LocalRendezvous}数据放在本地。而\code{Recv}节点则根据标识调用\code{RpcRemoteRendezvous::Recv}获取数据。此时，可能存在两种情况。

% \begin{enum}
%   \eitem{原设备与目标设备在同一个\ascii{Worker}中：\code{RpcRemoteRendezvous::Recv}将委托\code{LocalRendezvous::Recv}从本地直接获取；}
%   \eitem{原设备与目标设备不在同一个\ascii{worker}中：\code{RpcRemoteRendezvous::Recv}向目标设备所在的\ascii{Worker}发送\code{RecvTensorRequest}请求；目标\ascii{Worker}将通过\code{LocalRendezvous::Recv}从本地获取数据，并返回\code{RecvTensorResponse}消息给对端。}
% \end{enum}

\begin{leftbar}
\begin{python}
def send_inputs(remote_rendezvous, inputs):
  for (key, tensor) in inputs:
    remote_rendezvous.send(key, tensor)

def do_run_partitions(executors_and_partitions):
  barrier = ExecutorBarrier(executors_and_partitions.size())
  for (executor, partition) in executors_and_partitions:
    executor.run(partition, barrier.on_done())  
  barrier.wait()

def recv_outputs(remote_rendezvous, outputs):
  for (key, tensor) in outputs:
    remote_rendezvous.recv(key, tensor)

def run_partitions(executors_and_partitions, inputs, outputs):
  remote_rendezvous = RpcRemoteRendezvous()
  send_inputs(remote_rendezvous, inputs)
  do_run_partitions(executors_and_partitions)
  recv_outputs(remote_rendezvous, outputs)

def run_step(devices, full_graph, inputs, outputs):
  executors_and_partitions = split(full_graph, devices)
  run_partitions(executors_and_partitions, inputs, outputs)
\end{python}
\end{leftbar}

\subsection{ドメインモデル}

図\refig{cc-dist-model}に示すように、\tf{}分散ランタイムには洗練されたドメインモデルが存在します。

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{figures/cc-dist-model.png}
\caption{分散：ドメインモデル}
 \label{fig:cc-dist-model}
\end{figure}

\subsubsection{Cluster}

\ascii{Cluster}は\ascii{ClusterSpec}を使用して記述され、1つまたは複数の\ascii{Job}に分割できます。1つの\ascii{Job}は1つまたは複数の\ascii{Task}を含みます。つまり、\ascii{TensorFlow}クラスターは計算グラフを実行するタスクセット\ascii{(Task Set)}で構成されています。

各\ascii{Task}は独立して個別のマシン上で実行できるほか、1台のマシン上で複数の\ascii{Task}を実行することもできます（例：単一マシンの複数\ascii{CPU}、または単一マシンの複数\ascii{GPU}）。

\subsubsection{Job}

目的が同じ\ascii{Task}を同じ\ascii{Job}にグループ化します。各\ascii{Job}は\code{job\_id}で一意に識別されます。

一般的に、分散深層学習のモデルトレーニングプロセスには、2種類の基本的な\ascii{Job}タイプが存在します：

\begin{enum}
  \eitem{\ascii{ps}：モデルパラメーターの保存と更新を担当します。}
  \eitem{\ascii{worker}：計算集約型のモデルトレーニングと推論を担当します。}
\end{enum}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{figures/py-dist-ps-worker.png}
\caption{分散モデルトレーニング：PSとWorkerの間の相互作用}
 \label{fig:py-dist-ps-worker}
\end{figure}

\subsubsection{Task}

一般的に、分散ランタイムでは、\ascii{Task}は独立したプロセスで実行され、その上で\code{tf.train.Server}インスタンスが実行されます。そのうち、\ascii{Task}は\code{job\_id:task\_index}の2つ組で一意に識別されます。

\subsubsection{Server}

\ascii{Server}は\ascii{Task}のサービスプロセスを表し、外部に\code{MasterService}と\code{WorkerService}サービスを提供します。つまり、\ascii{Server}は同時に\ascii{Master}と\ascii{Worker}の2つの役割を果たすことができます。

\subsection{クラスターの構築}

\tf{}の分散ランタイムでは、各\ascii{Task}が\ascii{Server}を起動し、外部に\code{MasterService}サービスと\code{WorkerService}サービスを提供します。そのうち、\ascii{TensorFlow}クラスターの構築には2つの基本的なステップが含まれます：

\begin{enum}
  \eitem{\code{tf.train.ClusterSpec}を作成し、クラスター内の\ascii{Task}のデプロイ情報を記述し、\ascii{Job}の方法で組織します。}
  \eitem{各\ascii{Task}に対して、\code{tf.train.Server}インスタンスを起動します。}
\end{enum}

\subsubsection{クラスター設定}

\code{ClusterSpec}はクラスター内の\ascii{Task}のデプロイ情報を記述し、\ascii{Job}の方法で組織します。一般的に、分散実行モードでは、各\ascii{Task}に対して1つのプロセスを起動します。したがって、\code{ClusterSpec}は同時に\tf{}分散ランタイムのプロセス分布状況も記述しています。

例えば、\ascii{TensorFlow}クラスターが存在し、\code{ps}と\code{worker}の2つの\ascii{Job}で構成されているとします。そのうち、\code{ps}は\code{ps0:2222, ps1:2222}上にデプロイされています。\code{worker}は\code{worker0:2222, worker1:2222, worker2:2222}上にデプロイされています。

\begin{leftbar}
\begin{python}
tf.train.ClusterSpec({
  "worker": [
    "worker0:2222",   # /job:worker/task:0
    "worker1:2222",   # /job:worker/task:1
    "worker2:2222"    # /job:worker/task:2
  ],  
  "ps": [
    "ps0:2222",       # /job:ps/task:0
    "ps1:2222"        # /job:ps/task:0
  ]})
\end{python}
\end{leftbar}

この例では、\ascii{Task}のインデックスは明示的に指定されていません。デフォルトでは、\ascii{Job}の\ascii{Task}セット内で、\ascii{Task}インデックスは0から始まり、順番に自動増加します。

\subsubsection{Protobuf記述}

\begin{leftbar}
\begin{python}
message JobDef {
  string name = 1;
  map<int32, string> tasks = 2;
}

message ClusterDef {
  repeated JobDef job = 1;
}
\end{python}
\end{leftbar}

そのうち、\code{tasks}のキーは\code{task\_index}を表し、値は\code{host:port}を表します。

\end{content}

\section{Masterサービス}

\begin{content}

\code{MasterService}は\ascii{RPC}サービスです。\ascii{Client}が\code{target}に基づいて\ascii{Server}インスタンスに接続すると、\ascii{Server}は\ascii{Master}の役割を果たし、外部に\code{MasterService}サービスを提供します。

そのうち、\ascii{Client}と\ascii{Master}の間の相互作用は\code{MasterService}で定義されたインターフェース仕様に従います。つまり、\code{MasterService}は\ascii{Client}が\ascii{Master}にアクセスするための共通契約を定義し、複数の\code{WorkerService}の実行プロセスを調整および制御する責任を負います。

\subsection{インターフェース定義}

\code{master\_service.proto}ファイルでは、\code{MasterService}のすべてのインターフェースが定義されています。一方、\code{master.proto}ファイルでは、各インターフェースのメッセージボディが定義されています。

\begin{leftbar}
\begin{c++}
service MasterService {
  rpc CreateSession(CreateSessionRequest) 
      returns (CreateSessionResponse);
  
  rpc ExtendSession(ExtendSessionRequest) 
      returns (ExtendSessionResponse);

  rpc PartialRunSetup(PartialRunSetupRequest) 
      returns (PartialRunSetupResponse);

  rpc RunStep(RunStepRequest) 
      returns (RunStepResponse);
  
  rpc CloseSession(CloseSessionRequest) 
      returns (CloseSessionResponse);
  
  rpc ListDevices(ListDevicesRequest) 
      returns (ListDevicesResponse);

  rpc Reset(ResetRequest) 
      returns (ResetResponse);
}
\end{c++}
\end{leftbar}

\subsection{サービスへのアクセス}

一般的に、\ascii{Client}はインターフェース\code{MasterInterface}を使用してリモートの\code{MasterService}サービスを取得します。特に、\code{MasterInterface}のすべてのインターフェースは同期インターフェースであり、\ascii{Client}がリモートの\code{MasterService}サービスにアクセスするのがローカル関数を呼び出すかのようになります。

注意すべきは、\code{RunStepRequest/RunStepResponse}メッセージには比較的大きな\code{Tensor}インスタンスが含まれている可能性があるため、不要なオブジェクトのコピーを避けるために、メッセージラッパーの特殊な実装が行われています。

\begin{leftbar}
\begin{c++}
// Abstract interface for communicating with the TensorFlow Master service.
//
// This interface supports both RPC-based master implementations, and
// in-process master implementations that do not require an RPC roundtrip.
struct MasterInterface {
  virtual ~MasterInterface() {}
  
  virtual Status CreateSession(
      CallOptions* call_options,
      const CreateSessionRequest* request,
      CreateSessionResponse* response) = 0;

  virtual Status ExtendSession(
      CallOptions* call_options,
      const ExtendSessionRequest* request,
      ExtendSessionResponse* response) = 0;

  virtual Status PartialRunSetup(
      CallOptions* call_options,
      const PartialRunSetupRequest* request,
      PartialRunSetupResponse* response) {
    return errors::Unimplemented(
      "Partial run not implemented for master");
  }

  virtual Status RunStep(
      CallOptions* call_options,
      RunStepRequestWrapper* request,
      MutableRunStepResponseWrapper* response) = 0;

  // Wrapper classes for the `MasterService.RunStep` message.
  //
  // The `RunStepRequest/RunStepResponse` message can contain 
  // potentially large tensor data as part of its `feed/fetch` 
  // submessages.
  virtual Status RunStep(
    CallOptions* call_options,
    const RunStepRequest* request,
    RunStepResponse* response) {
    std::unique_ptr<RunStepRequestWrapper> wrapped_request(
        new ProtoRunStepRequest(request));
    std::unique_ptr<MutableRunStepResponseWrapper> wrapped_response(
        new NonOwnedProtoRunStepResponse(response));
    return RunStep(call_options, 
        wrapped_request.get(), 
        wrapped_response.get());
  }

  // Returns a request object for use in calls to
  // `RunStep()`. Ownership is transferred to the caller.
  virtual MutableRunStepRequestWrapper* CreateRunStepRequest() {
    return new MutableProtoRunStepRequest;
  }

  // Returns a response object for use in calls to
  // `RunStep()`. Ownership is transferred to the caller.
  virtual MutableRunStepResponseWrapper* CreateRunStepResponse() {
    return new OwnedProtoRunStepResponse;
  }

  virtual Status CloseSession(
    CallOptions* call_options,
    const CloseSessionRequest* request,
    CloseSessionResponse* response) = 0;

  virtual Status ListDevices(
    CallOptions* call_options,
    const ListDevicesRequest* request,
    ListDevicesResponse* response) = 0;

  virtual Status Reset(
    CallOptions* call_options, const ResetRequest* request,
    ResetResponse* response) = 0;
};
\end{c++}
\end{leftbar}

図\refig{dist-master-interface}に示すように、\code{MasterInterface}には2つの基本的な実装が存在します。

\begin{enum}
  \eitem{分散：\ascii{gRPC}ベースの\code{GrpcRemoteMaster}実装、\ascii{Client}と\ascii{Master}は2つの異なるプロセスにデプロイされます。}
  \eitem{ローカルモード：関数呼び出しベースの\code{LocalMaster}実装、\ascii{Client}と\ascii{Master}は同じプロセス内にあります。}
\end{enum}

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/dist-master-interface.png}
\caption{\code{MasterInterface}}
 \label{fig:dist-master-interface}
\end{figure}

分散モードでは、\code{GrpcRemoteMaster}は以下のような擬似コードを使用し、\ascii{gRPC}を通じてリモートの\code{MasterService}サービスを取得します。

\begin{leftbar}
\begin{c++}
stub = NewStub("/job:worker/replica:0/task:0")
handle = stub->CreateSession({graph_def})
do {
  stub->RunStep(handle, feeds, fetches);
} while (!should_stop());
stub->CloseSession({handle})
\end{c++}
\end{leftbar}

\subsection{RPCプロセス}

図\refig{dist-client-master-interaction}に示すように、\ascii{Client}は\code{MasterInterface}を通じてリモートの\ascii{MasterService}サービスを取得します。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/dist-client-master-interaction.png}
\caption{ClientがMasterServiceを取得する原理}
 \label{fig:dist-client-master-interaction}
\end{figure}

そのうち、\code{GrpcRemoteMaster}は\ascii{gRPC}クライアントの一種の実装であり、最終的に\code{Stub}を通じてリモートの\ascii{Master}上の\code{GrpcMasterService}サービスを取得し、その動作がローカル関数呼び出しのように見えるようにします。そのうち、\code{GrpcMasterService}は\code{MasterService}で定義されたすべてのサービスインターフェースを実装しており、\code{MasterService}の実際のサービスエンティティです。

\begin{remark}
厳密に言えば、\script{GrpcSession, ClientMaster, GrpcRemoteMaster}はすべて\ascii{Client}実装の一部です。通常理解されているような、\ascii{Python}フロントエンドシステムが完全な\ascii{Client}実装であり、バックエンドの\ascii{C++}システムには\ascii{Client}の実装が含まれていないということではありません。
\end{remark}

\subsection{メッセージ定義}

次に、各インターフェースのメッセージ定義を詳しく見ていきます。そのうち、最も重要なのは各サービスの識別子を認識することです。例えば、\ascii{Master}は複数の\ascii{Client}からアクセスでき、各\ascii{Client}に対応する\code{MasterSession}インスタンスを生成します。したがって、\code{GrpcSession}は\code{MasterSession}ハンドルを保持し、\ascii{Client}が\ascii{Master}のサービスを取得できるようにしています。

\subsubsection{CreateSession}

図\refig{dist-ms-create-sess-req}に示すように、\code{CreateSessionRequest}メッセージには初期の計算グラフが含まれており、\code{target}で指定された\ascii{Master}との接続を確立します。\ascii{Master}がリクエストメッセージを受信すると、対応する\code{MasterSession}インスタンスを作成し、\code{session\_handle}を使用してその\code{MasterSession}インスタンスを一意に識別します。

\ascii{Master}の論理処理が完了すると、\code{CreateSessionResponse}メッセージを通じて\ascii{Client}に返します。そのうち、\code{CreateSessionResponse}メッセージには\code{session\_handle}が含まれており、これを通じて\ascii{Client}側の\code{GrpcSession}と\ascii{Master}側の\code{MasterSession}の関連付けが確立されます。その後、\ascii{Client}と\ascii{Master}のすべての相互作用において、リクエストメッセージに\code{session\_handle}を含めることで、\ascii{Master}はそれを通じて対応する\code{MasterSession}インスタンスをインデックスします。

また、\code{CreateSessionResponse}には初期の\code{graph\_version}も含まれており、これは後続の\code{ExtendSession}操作で元の計算グラフに新しいノードを追加するために使用されます。

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/dist-ms-create-sess-req.png}
\caption{\code{CreateSession}}
 \label{fig:dist-ms-create-sess-req}
\end{figure}

\begin{leftbar}
\begin{c++}
message CreateSessionRequest {
  GraphDef graph_def = 1;
  ConfigProto config = 2;
  string target = 3;
}

message CreateSessionResponse {
  string session_handle = 1;
  int64 graph_version = 2;
}
\end{c++}
\end{leftbar}

\subsubsection{ExtendSession}

\ascii{CreateSession}が成功した後、後続の\ascii{Client}は\code{ExtendSession}を通じて、拡張するサブグラフを\ascii{Master}に送信し、元の計算グラフの規模を拡大することができます（サブグラフを追加することしかできず、ノードの修正や削除はできません）。

図\refig{dist-ms-extend-sess-req}に示すように、リクエストメッセージには\code{current\_graph\_version}を含める必要があり、\ascii{Master}側でバージョンの一致を検証します。\code{ExtendSession}の論理処理が完了すると、レスポンスメッセージに\code{new\_graph\_version}が含まれ、次回の\code{ExtendSession}操作に使用されます。そのうち、初期の\code{graph\_version}は\code{CreateSessionResponse}によって\ascii{Client}に送られます。

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{figures/dist-ms-extend-sess-req.png}
\caption{\code{ExtendSession}}
 \label{fig:dist-ms-extend-sess-req}
\end{figure}

\begin{leftbar}
\begin{c++}
message ExtendSessionRequest {
  string session_handle = 1;

  // REQUIRED: The nodes to be added to the session's graph. 
  // If any node has the same name as an existing node, 
  // the operation will fail with ILLEGAL\_ARGUMENT.
  GraphDef graph_def = 2;

  // REQUIRED: The version number of the graph to be extended. 
  // This will be tested against the current server-side version 
  // number, and the operation will fail with FAILED\_PRECONDITION 
  // if they do not match.
  int64 current_graph_version = 3;
}

message ExtendSessionResponse {
  // The new version number for the extended graph, 
  // to be used in the next call to ExtendSession.
  int64 new_graph_version = 4;
}
\end{c++}
\end{leftbar}

\subsubsection{RunStep}

一般的に、クライアント側で\code{RunStep}を繰り返し実行します。図\refig{dist-ms-run-step-req}に示すように、各\code{RunStep}の実行プロセスで、\ascii{Client}はリクエストメッセージに\code{feed, fetch, target}を含めます。これらはそれぞれ入力の\ascii{NamedTensor}リスト、出力する\ascii{Tensor}の名前リスト、実行する\ascii{OP}の名前リストを表します。レスポンスメッセージには\code{tensor}が含まれており、\code{fetch}の名前リストに対応する出力の\ascii{Tensor}リストを表します。

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{figures/dist-ms-run-step-req.png}
\caption{\code{RunStep}}
 \label{fig:dist-ms-run-step-req}
\end{figure}

\begin{leftbar}
\begin{c++}
message RunStepRequest {
  string session_handle = 1;

  repeated NamedTensorProto feed = 2;
  repeated string fetch = 3;
  repeated string target = 4;

  RunOptions options = 5;
  string partial_run_handle = 6;
}

message RunStepResponse {
  repeated NamedTensorProto tensor = 1;
  RunMetadata metadata = 2;
}
\end{c++}
\end{leftbar}

\subsubsection{CloseSession}

計算が完了したら、セッションを閉じて、システムの計算リソースを解放する必要があります。図\refig{dist-ms-closs-sess}に示すように、\ascii{Client}は\code{CloseSession}を\ascii{Master}に送信して、計算リソースの解放プロセスを開始します。

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{figures/dist-ms-closs-sess.png}
\caption{\code{CloseSession}}
 \label{fig:dist-ms-closs-sess}
\end{figure}

\begin{leftbar}
\begin{c++}
message CloseSessionRequest {
  string session_handle = 1;
}

message CloseSessionResponse {
}
\end{c++}
\end{leftbar}

\end{content}

\section{Workerサービス}

\begin{content}

\code{WorkerService}も\ascii{gRPC}サービスであり、ローカルデバイスセットでローカルサブグラフの実行をスケジュールする責任があります。これは\ascii{Worker}へのアクセスのインターフェース仕様を定義しています。つまり、\code{master\_service.proto}で定義されたインターフェースです。

\ascii{Master}は\code{ClusterSpec}情報に基づいて、クラスター内の他の\ascii{Server}インスタンスを見つけます。この時、これらの\ascii{Server}インスタンスは\ascii{Worker}の役割を果たします。\ascii{Master}はサブグラフを各\ascii{Worker}ノードに分散し、各\ascii{Worker}ノードのサブグラフ計算の実行プロセスを開始します。

\ascii{Worker}間にデータ依存関係が存在する場合、プロセス間通信を通じて対話を完了します。そのうち、\ascii{Master}と\ascii{Worker}の間、\ascii{Worker}と\ascii{Worker}の間の対話は\code{WorkerService}で定義されたインターフェース仕様に従います。

\subsection{インターフェース定義}

\code{worker\_service.proto}ファイルでは、\code{WorkerService}のすべてのインターフェースが定義されています。一方、\code{worker.proto}ファイルでは、各インターフェースのメッセージボディが定義されています。

\begin{leftbar}
\begin{c++}
service WorkerService {
  rpc GetStatus(GetStatusRequest) 
      returns (GetStatusResponse);

  rpc CreateWorkerSession(CreateWorkerSessionRequest)
      returns (CreateWorkerSessionResponse);

  rpc RegisterGraph(RegisterGraphRequest) 
      returns (RegisterGraphResponse);

  rpc DeregisterGraph(DeregisterGraphRequest) 
      returns (DeregisterGraphResponse);

  rpc RunGraph(RunGraphRequest) 
      returns (RunGraphResponse);

  rpc CleanupGraph(CleanupGraphRequest) 
      returns (CleanupGraphResponse);

  rpc CleanupAll(CleanupAllRequest) 
      returns (CleanupAllResponse);

  rpc RecvTensor(RecvTensorRequest) 
      returns (RecvTensorResponse) {
  }

  rpc Logging(LoggingRequest) 
      returns (LoggingResponse);

  rpc Tracing(TracingRequest) 
      returns (TracingResponse);
}
\end{c++}
\end{leftbar}

\subsection{サービスへのアクセス}

一般的に、\ascii{Master/Worker}はインターフェース\code{WorkerInterface}を使用してリモートの\code{WorkerService}サービスを取得します。そのうち、\code{WorkerInterface}は\code{WorkerService}への非同期アクセスのインターフェースを定義しています。\code{MasterInterface}と同様に、\code{RunGraphRequest/RunGraphResponse}に比較的大きな\code{Tensor}が含まれている可能性があるため、不要なオブジェクトのコピーを避けるために、メッセージのラッパーの特殊な実装が行われています。

\begin{leftbar}
\begin{c++}
struct WorkerInterface {
  // async interfaces.
  virtual void GetStatusAsync(
      const GetStatusRequest* request,
      GetStatusResponse* response,
      StatusCallback done) = 0;

  virtual void CreateWorkerSessionAsync(
      const CreateWorkerSessionRequest* request,
      CreateWorkerSessionResponse* response, 
      StatusCallback done) = 0;

  virtual void RegisterGraphAsync(
      const RegisterGraphRequest* request,
      RegisterGraphResponse* response,
      StatusCallback done) = 0;

  virtual void DeregisterGraphAsync(
      const DeregisterGraphRequest* request,
      DeregisterGraphResponse* response,
      StatusCallback done) = 0;

  virtual void RunGraphAsync(
      CallOptions* opts, 
      RunGraphRequestWrapper* request,
      MutableRunGraphResponseWrapper* repsonse,
      StatusCallback done) = 0;

  // Wrapper classes for the `WorkerService.RunGraph` message.
  //
  // The `RunGraphRequest/RunGraphResponse` message can contain 
  // potentially large tensor data as part of its `send/response`
  // submessages.
  virtual void RunGraphAsync(
      CallOptions* opts, 
      const RunGraphRequest* request,
      RunGraphResponse* response, 
      StatusCallback done) {
    RunGraphRequestWrapper* wrapped_request = 
        new ProtoRunGraphRequest(request);
    MutableRunGraphResponseWrapper* wrapped_response =
        new NonOwnedProtoRunGraphResponse(response);
    RunGraphAsync(opts, wrapped_request, wrapped_response,
        [wrapped_request, wrapped_response, done](const Status& s) {
            done(s);
            delete wrapped_request;
            delete wrapped_response;
        });
  }

  // Returns a request object for use in calls to
  // `RunGraphAsync()`. Ownership is transferred to the caller.
  virtual MutableRunGraphRequestWrapper* CreateRunGraphRequest() {
    return new MutableProtoRunGraphRequest;
  }

  // Returns a response object for use in calls to
  // `RunGraphAsync()`. Ownership is transferred to the caller.
  virtual MutableRunGraphResponseWrapper* CreateRunGraphResponse() {
    return new OwnedProtoRunGraphResponse;
  }

  virtual void CleanupGraphAsync(
      const CleanupGraphRequest* request,
      CleanupGraphResponse* response,
      StatusCallback done) = 0;

  virtual void CleanupAllAsync(
      const CleanupAllRequest* request,
      CleanupAllResponse* response,
      StatusCallback done) = 0;

  virtual void RecvTensorAsync(
      CallOptions* opts,
      const RecvTensorRequest* request,
      TensorResponse* response,
      StatusCallback done) = 0;

  virtual void LoggingAsync(
      const LoggingRequest* request,
      LoggingResponse* response, 
      StatusCallback done) = 0;

  virtual void TracingAsync(
      const TracingRequest* request,
      TracingResponse* response, 
      StatusCallback done) = 0;
};
\end{c++}
\end{leftbar}

\code{WorkerInterface}は同期アクセスインターフェースも定義しています。同期インターフェースは\code{CallAndWait}アダプタを通じて、間接的に非同期インターフェース上に実装されています。特に、同期インターフェースにより、\ascii{Master/Worker}がリモートの\code{WorkerService}を呼び出すのが、ローカル関数を呼び出すかのようになります。

\begin{leftbar}
\begin{c++}
struct WorkerInterface {
  // sync interfaces.
  Status GetStatus(
      const GetStatusRequest* request,
      GetStatusResponse* response) {
    return CallAndWait(&ME::GetStatusAsync, request, response);
  }

  Status CreateWorkerSession(
      const CreateWorkerSessionRequest* request,
      CreateWorkerSessionResponse* response) {
    return CallAndWait(&ME::CreateWorkerSessionAsync, request, response);
  }

  Status RegisterGraph(
      const RegisterGraphRequest* request,
      RegisterGraphResponse* response) {
    return CallAndWait(&ME::RegisterGraphAsync, request, response);
  }

  Status DeregisterGraph(
      const DeregisterGraphRequest* request,
      DeregisterGraphResponse* response) {
    return CallAndWait(&ME::DeregisterGraphAsync, request, response);
  }

  Status CleanupGraph(
      const CleanupGraphRequest* request,
      CleanupGraphResponse* response) {
    return CallAndWait(&ME::CleanupGraphAsync, request, response);
  }

  Status CleanupAll(
      const CleanupAllRequest* request,
      CleanupAllResponse* response) {
    return CallAndWait(&ME::CleanupAllAsync, request, response);
  }

  Status Logging(
      const LoggingRequest* request, 
      LoggingResponse* response) {
    return CallAndWait(&ME::LoggingAsync, request, response);
  }

  Status Tracing(
      const TracingRequest* request, 
      TracingResponse* response) {
    return CallAndWait(&ME::TracingAsync, request, response);
  }
 
 private:
  typedef WorkerInterface ME;

  template <typename Method, typename Req, typename Resp>
  Status CallAndWait(Method func, const Req* req, Resp* resp) {
    Status ret;
    Notification n;
    (this->*func)(req, resp, [&ret, &n](const Status& s) {
      ret = s;
      n.Notify();
    });
    n.WaitForNotification();
    return ret;
  }
};
\end{c++}
\end{leftbar}

特に、\code{WorkerInterface}で生成されたインスタンスは\code{WorkerCacheInterface::ReleaseWorker}が削除の責任を負います。したがって、ここでは外部による\code{WorkerInterface}インスタンスの不正な削除を避けるために、\code{WorkerInterface}のデストラクタを\code{protected}に制限し、\code{WorkerCacheInterface}をフレンドとして宣言しています。

\begin{leftbar}
\begin{c++}
struct WorkerInterface {
 protected:
  virtual ~WorkerInterface() {}
  friend class WorkerCacheInterface;
};
\end{c++}
\end{leftbar}

図\refig{dist-worker-interface}に示すように、\code{WorkerService}には2つの実装が存在します。そのうち、ローカルモードでは直接\code{GrpcWorker}を使用します。分散モードでは、\ascii{Worker}は別のプロセス内にデプロイされ、\code{GrpcRemoteWorker}を使用します。

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/dist-worker-interface.png}
\caption{\code{WorkerInterface}インターフェース}
 \label{fig:dist-worker-interface}
\end{figure}

\subsection{RPCプロセス}

図\refig{dist-worker-interaction}に示すように、分散モードでは、\code{GrpcRemoteWorker}は\ascii{gRPC}クライアントの一種の実装であり、最終的に\code{Stub}を通じてリモートの\ascii{Worker}上の\code{GrpcWorkerService}サービスを取得し、その動作がローカル関数呼び出しのように見えるようにします。そのうち、\code{GrpcWorkerService}は\code{WorkerService}で定義されたすべてのサービスインターフェースを実装しています。

\begin{remark}
厳密に言えば、\script{GrpcRemoteWorker}は\ascii{Master}または対向の\ascii{Worker}実装の一部です。
\end{remark}

一方、ローカルモードでは、\code{GrpcWorker}の関数呼び出しを通じて、直接\ascii{WorkerService}のサービスを取得し、追加のネットワーク転送のオーバーヘッドを避けています。

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{figures/dist-worker-interaction.png}
\caption{\code{MasterService}を取得するRPCプロセス}
 \label{fig:dist-worker-interaction}
\end{figure}

\subsection{メッセージ定義}

次に、\code{WorkerService}の各インターフェースのメッセージ定義を詳しく見ていきます。そのうち、最も重要なのは各サービスの識別子を認識することです。\code{WorkerSession}の作成時、\code{MasterSession}の識別子が\ascii{Worker}に渡され、\code{MasterSession}が複数の所属する\code{WorkerSession}インスタンスを統一管理できるようになります。

\ascii{Worker}が初めて\code{RegisterGraph}を完了すると、\ascii{Master}に一意の\code{graph\_handle}を返し、そのグラフインスタンスを識別します。したがって、クラスター内では\code{(session\_handle, graph\_handle)}の2つ組を使用して、そのグラフインスタンスを一意に識別できます。

\ascii{Master}が各\ascii{Worker}に並行して\code{RunGraph}を実行するよう通知する際、異なる\code{step}を区別するために、\ascii{Master}はグローバルに一意の\code{step\_id}を生成し、\code{RunGraph}を通じて各\ascii{Worker}に渡します。

\begin{enum}
  \eitem{\code{session\_handle}: \code{MasterSession}インスタンスの作成時に自動生成され、\code{CreateSessionResponse}を通じて\ascii{Client}に渡されます。\code{CreateWorkerSessionRequest}を通じて\ascii{Worker}に渡されます。}  
  \eitem{\code{graph\_id}: 初回の\code{RegisterGraph}時に\code{Worker}によって生成され、\code{RegisterGraphResponse}を通じて\code{Master}に渡されます。}
  \eitem{\code{step\_id}: 毎回の\code{RunStep}時に\code{Master}が一意の識別子を生成し、\code{RunGraphRequest}を通じて\ascii{Worker}に渡されます。} 
\end{enum}

\subsubsection{CreateWorkerSession}

図\refig{dist-worker-create-worker-sess}に示すように、\code{CreateWorkerSessionRequest}メッセージには\code{MasterSession}が割り当てた\code{session\_handle}が含まれています。\ascii{Worker}がリクエストメッセージを受信すると、\code{WorkerSession}インスタンスを生成し、\code{session\_handle}を使用してその\ascii{Worker}内でそのインスタンスを一意に識別します。

同じクラスター内で、1つの\code{MasterSession}インスタンスに対して、他の\ascii{Worker}は同じ\code{session\_handle}を受け取ります。このようにして、その\code{MasterSession}インスタンスはそれに所属するすべての\code{WorkerSession}インスタンスを統一管理できるようになります。

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/dist-worker-create-worker-sess.png}
\caption{\code{CreateWorkerSession}}
 \label{fig:dist-worker-create-worker-sess}
\end{figure}

\begin{leftbar}
\begin{c++}
message CreateWorkerSessionRequest {
  string session_handle = 1;
  ServerDef server_def = 2;
}

message CreateWorkerSessionResponse {
}
\end{c++}
\end{leftbar}

\subsubsection{RegisterGraph}

図\refig{dist-worker-register-graph}に示すように、\code{RegisterGraphRequest}メッセージには\code{MasterSession}が割り当てた\code{session\_handle}と、そのサブグラフインスタンス\ascii{graph\_def}が含まれています。\code{Worker}がサブグラフの登録と初期化を完了すると、\ascii{Master}にそのサブグラフの\code{graph\_handle}を返します。

注意すべきは、\code{Master}は\code{RegisterGraph}を1回だけ実行するということです。計算グラフのノードが再スケジュールされるか、\code{Master}プロセスが再起動されない限り実行されません。

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/dist-worker-register-graph.png}
\caption{\code{RegisterGraph}}
 \label{fig:dist-worker-register-graph}
\end{figure}

\begin{leftbar}
\begin{c++}
message RegisterGraphRequest {
  string session_handle = 1;

  GraphDef graph_def = 2;
  bool has_control_flow = 3 [deprecated = true];

  GraphOptions graph_options = 4;
  DebugOptions debug_options = 5;
}

message RegisterGraphResponse {
  string graph_handle = 1;
}
\end{c++}
\end{leftbar}

\subsubsection{DeregisterGraph}

図\refig{dist-worker-deregister-graph}に示すように、\code{Worker}ノード上のサブグラフが不要になったとき（例えば、計算グラフが再スケジュールされ、グラフ内のノードが再配置された場合）、\code{Master}は\ascii{Worker}に\code{DeregisterGraph}メッセージを送信し、\code{Worker}がそのサブグラフインスタンスを登録解除できるようにします。

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/dist-worker-deregister-graph.png}
\caption{\code{DeregisterGraph}}
 \label{fig:dist-worker-deregister-graph}
\end{figure}

\begin{leftbar}
\begin{c++}
message DeregisterGraphRequest {
  string session_handle = 2;
  string graph_handle = 1;
}

message DeregisterGraphResponse {
}
\end{c++}
\end{leftbar}

\subsubsection{RunGraph}

\ascii{Worker}ノード上に登録されたサブグラフを実行する際、異なる\code{step}を区別するために、\ascii{Master}は一意の\code{step\_id}を生成し、各\ascii{Worker}に渡します。各\ascii{Worker}は\code{step\_id}を通じてデータの協調を実現します。

さらに、\code{RunGraphRequest}には\code{send, recv\_key}が含まれており、それぞれサブグラフの入力\code{Tensor}の識別子とデータ、およびサブグラフの出力\code{Tensor}の識別子を表します。\code{RunGraphResponse}は\code{recv\_key}に対応する\code{Tensor}リストを返します。

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/dist-worker-run-graph.png}
\caption{\code{RunGraph}}
 \label{fig:dist-worker-run-graph}
\end{figure}

\begin{leftbar}
\begin{c++}
message RunGraphRequest {
  string session_handle = 8;
  string graph_handle = 1;
  int64 step_id = 2;

  ExecutorOpts exec_opts = 5;

  repeated NamedTensorProto send = 3;
  repeated string recv_key = 4;

  bool is_partial = 6;
  bool is_last_partial_run = 7;
}

message RunGraphResponse {
  repeated NamedTensorProto recv = 1;

  // execution stats
  StepStats step_stats = 2;
  CostGraphDef cost_graph = 3;
  repeated GraphDef partition_graph = 4;
}
\end{c++}
\end{leftbar}

\subsubsection{RecvTensor}

ある\code{step}の実行中に、2つの\ascii{Worker}がデータを交換する必要がある場合、消費者は生産者に\code{RecvTensorRequest}メッセージを送信し、\code{(step\_id, rendezvous\_key)}の2つ組を含めて、対向の\ascii{Worker}の対応する\code{Tensor}データをリクエストし、\code{RecvTensorResponse}を通じて返します。

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/dist-worker-recv-tensor.png}
\caption{\code{RecvTensor}}
 \label{fig:dist-worker-recv-tensor}
\end{figure}

\begin{leftbar}
\begin{c++}
message RecvTensorRequest {
  int64 step_id = 1;
  string rendezvous_key = 2;

  // If true, use an out-of-band DMA mechanism to transfer the
  // received tensor.
  bool dma_ok = 3;

  // Optional information on client-side device locality.
  DeviceLocality client_locality = 4;

  // Optional information on server-side device locality.
  DeviceLocality server_locality = 5;

  // Optional information needed by the RPC subsystem.
  google.protobuf.Any transport_options = 6;
}

message RecvTensorResponse {
  // The tensor as a proto.
  TensorProto tensor = 1;

  // If true, this tensor was the output of a dead node, and the
  // content is invalid.
  bool is_dead = 2;

  // The time at which tensor was available and started to be returned.
  int64 send_start_micros = 3;

  // Optional additional information about how to receive the tensor,
  // e.g. in the event that `RecvTensorRequest.dma\_ok` was true.
  google.protobuf.Any transport_options = 4;
}
\end{c++}
\end{leftbar}

\end{content}

\section{サーバー}

\begin{content}

\code{Server}は\ascii{gRPC}ベースのサーバーで、ローカルデバイスセットの管理を担当します。外部に\code{MasterService}サービスと\code{WorkerService}サービスを提供し、\ascii{Master}と\ascii{Worker}の両方の役割を果たす能力があります。

\subsection{ドメインモデル}

図\refig{cc-server-model}に示すように、\code{GrpcServer}が\ascii{Master}の役割を果たす場合、外部に\code{MasterService}サービスを提供します。そのうち、アクセスする各\ascii{Client}に対して\code{MasterSession}インスタンスを起動し、グローバルに一意の\code{session\_handle}でそれを識別します。つまり、\ascii{Master}は複数の\ascii{Client}からアクセスできますが、1つの\ascii{Client}は特定の\ascii{Master}にのみアクセスできます。

\code{GrpcServer}が\ascii{Worker}の役割を果たす場合、外部に\code{WorkerService}サービスを提供します。そのうち、各\ascii{Worker}は複数の\ascii{Master}に計算サービスを提供でき、計算サービスをリクエストする各\code{MasterSession}に対して対応する\code{WorkerSession}インスタンスを生成し、対応する\code{MasterSession}からの計算グラフの\emph{登録}と\emph{実行}コマンドを待ちます。

\code{GrpcServer}インスタンス全体は\code{grpc::Server}プロセス上に搭載され、特定のポートのメッセージを監視し、メッセージが到着すると自動的に\code{MasterService}または\code{WorkerService}の対応するメッセージ処理コールバック関数にディスパッチします。

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{figures/cc-server-model.png}
\caption{Serverドメインモデル}
 \label{fig:cc-server-model}
\end{figure}

\subsubsection{Protobuf記述}

\code{protocol}が\code{grpc}の場合、システムランタイムは\ascii{gRPC}実装ベースの\code{GrpcServer}インスタンスを有効にします。さらに、\code{ConfigProto}を通じてランタイムパラメータの設定を実現できます。つまり、\tf{}のアーキテクチャは外部に開放されています。例えば、\code{protocol}の拡張を通じて新しい通信プロトコルをサポートし、新しいプロトコルベースの\code{Server}インスタンスを実装できます。

\begin{leftbar}
\begin{python}
message ServerDef {
  ClusterDef cluster = 1;
  
  string job_name = 2;
  int32 task_index = 3;

  ConfigProto default_session_config = 4;
  string protocol = 5;
}
\end{python}
\end{leftbar}

\subsubsection{サービスの相互接続}

図\refig{cc-server-interact}に示すように、1つの\ascii{Server}インスタンスは\code{tf.train.ClusterSpec}を通じてクラスター内の他の\ascii{Server}インスタンスと相互接続を実現します。

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{figures/cc-server-interact.png}
\caption{サービスの相互接続}
 \label{fig:cc-server-interact}
\end{figure}

図\refig{cc-server-interact-1}に示すように、\ascii{Client}がいずれかの\ascii{Server}にアクセスすると、そのサーバーは\ascii{Master}の役割を果たし、他の\ascii{Server}は\ascii{Worker}の役割を果たします。特に、\ascii{Client}がアクセスする\ascii{Server}も\ascii{Worker}の役割を果たします。

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{figures/cc-server-interact-1.png}
\caption{単一Clientがクラスターにアクセス}
 \label{fig:cc-server-interact-1}
\end{figure}

図\refig{cc-server-interact-2}に示すように、複数の\ascii{Client}が異なる\ascii{Server}インスタンスにそれぞれアクセスする可能性があります。この時、\ascii{Client}がアクセスする\ascii{Server}インスタンスは\ascii{Master}の役割を果たします。しかし、そのの\ascii{Server}インスタンスは、クラスター内の他の\ascii{Server}インスタンスに対しては\ascii{Worker}の役割を果たします。

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{figures/cc-server-interact-2.png}
\caption{複数Clientがクラスターにアクセス}
 \label{fig:cc-server-interact-2}
\end{figure}

特に、\ascii{Client}と\ascii{Master}は同じプロセス内にデプロイできます。この場合、\ascii{Client}と\ascii{Master}の間の対話はより簡単になり、両者は直接関数呼び出しを使用し、\ascii{gRPC}対話の追加オーバーヘッドを避けます。同様に、同じ\ascii{Server}内で、\ascii{Master}と\ascii{Worker}を同じプロセス内にデプロイすることができます。この場合、\ascii{Master}と\ascii{Worker}の間は直接関数呼び出しを使用します。

\subsection{状態機械}

図\refig{dist-grpc-server-state-machine}に示すように、\code{GrpcServer}は\code{grpc::Server}ベースのサーバーで、シンプルな状態機械を管理および維持しています。

\code{GrpcServer}は\code{New}状態で\code{grpc::Server}サービスを起動しますが、外部にサービスを提供しません。\code{Started}状態でサービスを起動し、外部に\code{MasterService}と\code{WorkerService}の\code{RPC}メッセージサービスを提供します。最終的に、\code{Stopped}状態で\code{MasterService}と\code{WorkerService}サービスを停止します。

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{figures/dist-grpc-server-state-machine.png}
\caption{GrpcServer状態機械}
 \label{fig:dist-grpc-server-state-machine}
\end{figure}

\subsubsection{サービスの作成}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{figures/dist-grpc-server-factory.png}
\caption{Serverインスタンスの多態的作成}
 \label{fig:dist-grpc-server-factory}
\end{figure}

\begin{leftbar}
\begin{c++}
struct GrpcServerFactory : ServerFactory {
  bool AcceptsOptions(const ServerDef& server_def) override {
    return server_def.protocol() == "grpc";
  }

  Status NewServer(const ServerDef& server_def,
      std::unique_ptr<ServerInterface>* out_server) override {
    GrpcServer::Create(server_def, Env::Default(), out_server);
    return Status::OK();
  }
};
\end{c++}
\end{leftbar}

\begin{leftbar}
\begin{c++}
void GrpcServer::Create(
    const ServerDef& server_def, Env* env,
    std::unique_ptr<ServerInterface>* out_server) {
  auto ret = std::make_unique<GrpcServer>(server_def, env);
  ret->Init();
  *out_server = std::move(ret);
}
\end{c++}
\end{leftbar}

図\refig{cc-server-model}に示すように、\code{GrpcServer::Init}は\code{GrpcServer}ドメインオブジェクトの初期化を完了し、主に以下の3つの基本プロセスを含みます。

\begin{enum}
  \eitem{\code{MasterEnv}インスタンスの初期化;}  
  \eitem{\code{WorkerEnv}インスタンスの初期化;}  
  \eitem{\code{grpc::Server}の作成と起動}    
    \begin{enum}
    \eitem{\code{MasterService}の初期化}      
    \begin{nitemize}
      \eitem{\code{Master}インスタンスの作成;}  
      \eitem{\code{MasterService}インスタンスの作成;}
    \end{nitemize}
    \eitem{\code{WorkerService}の初期化}          
    \begin{nitemize}          
      \eitem{\code{Worker}インスタンスの作成;}  
      \eitem{\code{WorkerService}インスタンスの作成。}
    \end{nitemize}      
    \end{enum}
\end{enum}

\code{GrpcServer}インスタンスの初期化プロセス全体をよりよく理解するために、ここでは実装の一部を再構成しました。まず、\code{MasterEnv, WorkerEnv}インスタンスを初期化します。次に、\code{grpc::Server}サーバーを作成して起動します。

\begin{leftbar}
\begin{c++}
void GrpcServer::Init() {
  InitMasterEnv();
  InitWorkerEnv();
  StartGrpcServer();
}
\end{c++}
\end{leftbar}

\subsubsection{MasterEnvの初期化}

\code{MasterEnv}は\code{Master}ランタイムのコンテキスト環境を保持し、\code{GrpcServer}と同じライフサイクルを持つため、\code{Master}のランタイム全体で可視です。

図\refig{dist-master-env}に示すように、\code{LocalDevices}はローカルデバイスセットを取得するために使用されます。\code{WorkerCacheFactory}は\code{WorkerCacheInterface}インスタンスを作成するために使用されます。\code{WorkerCacheInterface}は\code{MasterInterface}インスタンスを作成するために使用され、後者はリモートの\code{MasterService}サービスを呼び出すために使用されます。\code{MasterSessionFactory}は\code{MasterSession}インスタンスを作成するために使用されます。\code{OpRegisteryInterface}は特定の\code{OP}のメタデータを照会するために使用されます。\code{Env}はクロスプラットフォームの\ascii{API}インターフェースを取得するために使用されます。そのうち、\code{WorkerCacheInterface}の作成プロセスについては後で詳しく説明します。

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{figures/dist-master-env.png}
\caption{\code{MasterEnv}モデル}
 \label{fig:dist-master-env}
\end{figure}

\subsubsection{WorkerEnvの初期化}

\code{WorkerEnv}は\code{Worker}ランタイムのコンテキスト環境を保持し、\code{GrpcServer}と同じライフサイクルを持つため、\code{Worker}のランタイム全体で可視です。

図\refig{dist-worker-env}に示すように、\code{LocalDevices}はローカルデバイスセットを取得するために使用されます。\code{DeviceManager}はローカルデバイスセットとリモートデバイスセットを管理するために使用されます。\code{SessionManager}は\code{WorkerSession}のコレクションを管理するために使用されます。\code{RendezvousManager}は\code{Rendezvous}インスタンスセットを管理するために使用されます。\code{ThreadPool}は計算プールから自動的にスレッドを割り当て、\ascii{OP}の\ascii{Kernel}演算子の実行を開始します。\code{Env}はクロスプラットフォームの\ascii{API}インターフェースを取得するために使用されます。

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{figures/dist-worker-env.png}
\caption{\code{WorkerEnv}モデル}
 \label{fig:dist-worker-env}
\end{figure}

\subsubsection{grpc::Serverの起動}

システムの実装では、ビルダーパターンを使用して\code{grpc::Server}インスタンスを作成しています。まず、\code{grpc::Server}のサービスオプションを設定します。次に、\code{MasterService}インスタンスと\code{WorkerService}インスタンスをそれぞれ構築します。最後に、\code{builder.BuildAndStart}メソッドを呼び出して\code{grpc::Server}サーバーを起動します。

注意すべきは、\code{grpc::Server}起動時、\code{GrpcServer}はまだ\code{New}状態にあり、
\code{grpc::Server}はまだ外部に\code{MasterService}サービスと\code{WorkerService}サービスを提供していないということです。\code{GrpcServer}が\code{Started}状態に移行するまで、\code{grpc::Server}は実際に外部に\code{MasterService}サービスと\code{WorkerService}サービスを提供します。

\begin{leftbar}
\begin{c++}
void InitServerBuilder(::grpc::ServerBuilder& builder) {
  builder.AddListeningPort(
    strings::StrCat("0.0.0.0:", GetRequestedPort()),
    GetServerCredentials(server_def_), &bound_port_);
  builder.SetMaxMessageSize(std::numeric_limits<int32>::max());
  builder.SetOption(
      std::unique_ptr<::grpc::ServerBuilderOption>(new NoReusePortOption));
}

void GrpcServer::StartGrpcServer() {
  ::grpc::ServerBuilder builder;

  InitServerBuilder(builder);
  InitMasterService(builder);
  InitWorkerService(builder);

  server_ = builder.BuildAndStart();  
}
\end{c++}
\end{leftbar}

\code{grpc::Server}が外部に提供する\code{MasterService}サービスのエンティティが\code{GrpcMasterService}インスタンスであることは容易に分かります。メッセージが到着すると、自動的に\code{GrpcMasterService}インスタンスの対応するメッセージ処理関数がコールバックされます。そのうち、メッセージ処理関数では、その業務ロジックの処理は完全に\code{Master}のドメインオブジェクトに依存しています。

\begin{leftbar}
\begin{c++}
std::unique_ptr<Master> GrpcServer::CreateMaster(
    MasterEnv* master_env) {
  return std::make_unique<Master>(master_env);
}

AsyncServiceInterface* NewGrpcMasterService(
    Master* master, ::grpc::ServerBuilder* builder) {
  return new GrpcMasterService(master, builder);
}

void GrpcServer::InitMasterService() {
  master_impl_ = CreateMaster(&master_env_);
  master_service_ = NewGrpcMasterService(
      master_impl_.get(), &builder);  
}
\end{c++}
\end{leftbar}

同様に、\code{grpc::Server}が外部に提供する\code{WorkerService}サービスのエンティティは\code{GrpcWorkerService}インスタンスです。メッセージが到着すると、自動的に\code{GrpcWorkerService}インスタンスの対応するメッセージ処理関数がコールバックされます。そのうち、メッセージ処理関数では、その業務ロジックの処理は完全に\code{GrpcWorker}のドメインオブジェクトに依存しています。

\begin{leftbar}
\begin{c++}
std::unique_ptr<GrpcWorker> NewGrpcWorker(WorkerEnv* env) {
  return std::unique_ptr<GrpcWorker>(new GrpcWorker(env));
}

AsyncServiceInterface* NewGrpcWorkerService(
    GrpcWorker* worker, ::grpc::ServerBuilder* builder) {
  return new GrpcWorkerService(worker, builder);
}

void GrpcServer::InitWorkerService(::grpc::ServerBuilder& builder) {
  worker_impl_ = NewGrpcWorker(&worker_env_);
  worker_service_ = NewGrpcWorkerService(
    worker_impl_.get(), &builder);
}
\end{c++}
\end{leftbar}

\subsubsection{サービスの開始}

\code{New}状態では、\code{grpc::Server}はすでに起動していますが、まだ外部に\code{MasterService}サービスと\code{WorkerService}サービスを提供していません。\code{GrpcServer::Start}メソッドを呼び出した後、\code{GrpcServer}の状態が\code{New}から\code{Started}状態に移行し、2つの独立したスレッドを起動して、それぞれ\code{MasterService}と\code{WorkerService}のメッセージハンドラを開始します。この時点で、\code{GrpcServer}は正式に外部に\code{MasterService}と\code{WorkerService}を提供します。

\begin{leftbar}
\begin{c++}
Status GrpcServer::Start() {
  mutex_lock l(mu_);
  switch (state_) {
    case NEW: {
      master_thread_.reset(
          env_->StartThread(ThreadOptions(), "TF_master_service",
                            [this] { master_service_->HandleRPCsLoop(); }));
      worker_thread_.reset(
          env_->StartThread(ThreadOptions(), "TF_worker_service",
                            [this] { worker_service_->HandleRPCsLoop(); }));
      state_ = STARTED;
      return Status::OK();
    }
    case STARTED:
      LOG(INFO) << "Server already started(" << target() << ")";    
      return Status::OK();
    case STOPPED:
    default:
      CHECK(false);
  }
}
\end{c++}
\end{leftbar}

\subsubsection{サービスの終了待機}

\code{MasterService}サービスと\code{WorkerService}サービスを持続的に外部に提供するために、\code{TF\_master\_service}スレッドと\code{TF\_worker\_service}スレッドにそれぞれ\code{join}操作を実行し、メインスレッドを中断させ、これら2つのスレッドが終了するまで待機する必要があります。

\code{GrpcServer::Join}メソッドを呼び出すことで、\code{GrpcServer}が\code{Started}または\code{Stoped}状態にある場合、自動的に\code{Thread}のデストラクタを呼び出します。

\begin{leftbar}
\begin{c++}
Status GrpcServer::Join() {
  mutex_lock l(mu_);
  switch (state_) {
    case NEW:
      // Prevent the server from being started subsequently.
      state_ = STOPPED;
      return Status::OK();
    case STARTED:
    case STOPPED:
      master_thread_.reset();
      worker_thread_.reset();
      return Status::OK();
    default:
      CHECK(false);
  }
}
\end{c++}
\end{leftbar}

例えば、\code{C++}標準ライブラリに基づいて実装された\code{StdThread}では、そのデストラクタが\code{std::thread}の\code{join}メソッドを呼び出します。


\begin{leftbar}
\begin{c++}
struct StdThread : Thread {
  StdThread(const ThreadOptions&, const string&, 
      std::function<void()> fn)
    : thread_(fn) {
  }

  ~StdThread() override { 
    thread_.join(); 
  }

 private:
  std::thread thread_;
};
\end{c++}
\end{leftbar}

\subsubsection{サービスの終了}

残念ながら、現在の\code{GrpcServer}は優雅に終了することができません。したがって、エンジニアリング実践環境では、\tf{}の分散ランタイムはしばしば\code{Kubernetes}に頼って、\code{GrpcServer}サービスの自動管理を実現しています。

\begin{leftbar}
\begin{c++}
Status GrpcServer::Stop() {
  mutex_lock l(mu_);
  switch (state_) {
    case NEW:
      state_ = STOPPED;
      return Status::OK();
    case STARTED:
      return errors::Unimplemented(
          "Clean shutdown is not currently implemented");
    case STOPPED:
      LOG(INFO) << "Server already stopped(" << target() << ")";
      return Status::OK();
    default:
      CHECK(false);
  }
}
\end{c++}
\end{leftbar}

\subsection{WorkerCacheInterfaceの作成}

\code{GrpcServer}の状態機械モデルを紹介した後、以前に残された問題に戻ります。\code{MasterEnv}は\code{WorkerCacheInterface}インスタンスを保持しており、これは\code{WorkerInterface}の照会または遅延作成に使用されます。そのうち、\code{WorkerInterface}はリモートの\code{WorkerSerivice}サービスにアクセスするために使用されます。

\subsubsection{ファクトリメソッド：GrpcServer::WorkerCacheFactory}

\code{MasterEnv}の初期化時、\code{GrpcServer::WorkerCacheFactory}ファクトリメソッドを呼び出して\code{WorkerCacheInterface}インスタンスを作成します。そのうち、\code{WorkerCacheFactoryOptions}は\code{ServerDef}と同等で、\code{ClusterDef}および\code{job\_name:task\_index}情報を含んでいます。したがって、\code{ParseChannelSpec}を経て得られた\code{GrpcChannelSpec}インスタンスは\code{ClusterSpec}と同等で、クラスターの基本設定情報を含んでいます。

\begin{leftbar}
\begin{c++}
Status GrpcServer::WorkerCacheFactory(
    const WorkerCacheFactoryOptions& options,
    WorkerCacheInterface** worker_cache) {

  GrpcChannelSpec channel_spec;
  TF_RETURN_IF_ERROR(ParseChannelSpec(options, &channel_spec));

  std::unique_ptr<GrpcChannelCache> channel_cache(
      NewGrpcChannelCache(channel_spec, GetChannelCreationFunction()));

  string name_prefix = strings::StrCat(
      "/job:", *options.job_name, "/replica:0",
      "/task:", options.task_index);

  *worker_cache = NewGrpcWorkerCacheWithLocalWorker(
      channel_cache.release(), worker_impl_.get(), name_prefix);
  return Status::OK();
}
\end{c++}
\end{leftbar}

\subsubsection{ファクトリメソッド：NewGrpcChannelCache}

\code{NewGrpcChannelCache}は\code{GrpcChannelCache}インスタンスを作成するために使用されます。\code{GrpcChannelCache}は\code{Worker}の名前に基づいて、対応する\code{grpc::Channel}インスタンスを取得または遅延作成できます。そのうち、1つの\ascii{Job}に対して1つの\code{SparseGrpcChannelCache}インスタンスが作成され、\code{MultiGrpcChannelCache}は複数の\code{SparseGrpcChannelCache}を保持します。これは典型的な複合パターンの適用で、後で\code{GrpcChannelCache}の設計について詳しく説明します。

\begin{leftbar}
\begin{c++}
GrpcChannelCache* NewGrpcChannelCache(
    const GrpcChannelSpec& spec,
    ChannelCreationFunction channel_func) {
  std::vector<GrpcChannelCache*> caches;
  for (auto& job : spec.host_ports_jobs()) {
    caches.push_back(
        new SparseGrpcChannelCache(
            job.job_id, job.host_ports, channel_func));
  }
  return new MultiGrpcChannelCache(caches);
}
\end{c++}
\end{leftbar}

\subsubsection{ファクトリメソッド：NewGrpcWorkerCacheWithLocalWorker}

一方、ファクトリメソッド\code{NewGrpcWorkerCacheWithLocalWorker}は、ローカルの\code{Worker}を持つ\code{GrpcWorkerCache}インスタンスを作成するために使用されます。


\begin{leftbar}
\begin{c++}
WorkerCacheInterface* NewGrpcWorkerCacheWithLocalWorker(
    GrpcChannelCache* cc, WorkerInterface* local_worker,
    const string& local_target) {
  return new GrpcWorkerCache(cc, local_worker, local_target);
}
\end{c++}
\end{leftbar}

\subsubsection{ファクトリメソッド：GrpcServer::GetChannelCreationFunction}

\code{GetChannelCreationFunction}の設計は\ascii{C++}関数型プログラミングの考え方を使用しており、\code{grpc::Channel}インスタンスを作成するための関数オブジェクトを返します。しかし残念なことに、既存の\code{NewHostPortGrpcChannel}関数は\code{ChannelCreationFunction}インターフェースと一致しません。したがって、ここでは\code{ConvertToChannelCreationFunction}というアダプターを使用して、\code{NewHostPortGrpcChannel}を\code{ChannelCreationFunction}に変換しています。

\begin{leftbar}
\begin{c++}
using SharedGrpcChannelPtr = std::shared_ptr<::grpc::Channel>;
using ChannelCreationFunction = std::function<SharedGrpcChannelPtr(string)>;

Status NewHostPortGrpcChannel(const string& target,
    SharedGrpcChannelPtr* channel) {
  ::grpc::ChannelArguments args;
  args.SetInt("grapc.arg.max.message_length", 
              std::numeric_limits<int32>::max());
  args.SetInt("grpc.testing.fixed_reconnect_backoff_ms", 
              1000);

  *channel = ::grpc::CreateCustomChannel(
      "dns:///" + target, ::grpc::InsecureChannelCredentials(), args);
  return Status::OK();
}

ChannelCreationFunction ConvertToChannelCreationFunction(
  const std::function<Status(string, SharedGrpcChannelPtr*)>& new_channel) {
  return [new_channel_func](const string& target) -> SharedGrpcChannelPtr {
    SharedGrpcChannelPtr channel_ptr;
    if (new_channel(target, &channel_ptr).ok()) {
      return channel_ptr;
    } else {
      return nullptr;
    }
  };
}

ChannelCreationFunction GrpcServer::GetChannelCreationFunction() const {
  return ConvertToChannelCreationFunction(NewHostPortGrpcChannel);
}
\end{c++}
\end{leftbar}

ここまでで、\code{GrpcChannelCache}と\code{WorkerCacheInterface}の作成プロセスを整理しましたが、これらは何のために使用されるのでしょうか？実際、\code{WorkerCacheInterface}は\code{WorkerInterface}インスタンスを取得するために使用され、後者はリモートの\code{WorkerSerivice}サービスにアクセスするために使用されます。その動作原理は非常にシンプルです。

\begin{enum}
  \eitem{クラスター内のすべての\code{Worker}の名前リストを取得します。}
  \eitem{\code{Worker}の名前に基づいて\ascii{RPC}チャネルを作成します。}  
  \eitem{\code{Worker}の\ascii{RPC}チャネルに基づいて、\code{GrpcRemoteWorker}インスタンスを作成します。}
\end{enum}

そのうち、\code{GrpcRemoteWorker}は\code{WorkerInterface}の具体的な実装です。\code{GrpcChannelCache}は\code{Worker}の名前の取得と、\code{Worker}に対応する\code{grpc::Channel}の作成を担当します。

\subsection{Workerの RPC チャネルの作成}

\code{GrpcChannelCache}はクラスター内のリモート\code{Worker}の\ascii{RPC}チャネルの取得または作成に使用されます。そのうち、\code{ListWorkers}はクラスター内の\code{Worker}の名前リストを返すために使用されます。\code{TranslateTask}は\code{Worker}の名前を\code{host:port}のアドレス情報に変換するために使用されます。\code{FindWorkerChannel}はキャッシュから\code{grpc::Channel}インスタンスを検索します。見つからない場合は、アドレス情報に基づいて動的に\code{grpc::Channel}インスタンスを作成し、キャッシュに追加します。

\begin{leftbar}
\begin{c++}
typedef std::shared_ptr<::grpc::Channel> SharedGrpcChannelPtr;

struct GrpcChannelCache {
  virtual ~GrpcChannelCache() {}
  virtual void ListWorkers(std::vector<string>* workers) const = 0;
  virtual SharedGrpcChannelPtr FindWorkerChannel(const string& target) = 0;
  virtual string TranslateTask(const string& task) = 0;
};
\end{c++}
\end{leftbar}

\subsubsection{暗黙的ツリー}

図\refig{dist-grpc-channel-cache}に示すように、\code{GrpcChannelCache}クラス階層は暗黙的な「ツリー」構造に従っています。\code{SparseGrpcChannelCache}はツリーノードであり、各インスタンスは1つの\ascii{Job}インスタンスに対応します。一方、\code{MultiGrpcChannelCache}は複数の\code{SparseGrpcChannelCache}インスタンスを保持し、各インスタンスは複数の\ascii{Job}インスタンスに対応します。

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{figures/dist-grpc-channel-cache.png}
\caption{GRPCチャネルの組み合わせ作成}
 \label{fig:dist-grpc-channel-cache}
\end{figure}

\subsubsection{キャッシュメカニズム}

\code{grpc::Channel}インスタンスをリアルタイムで作成するたびのオーバーヘッドを避けるために、\code{CachingGrpcChannelCache}が導入されました。これは\code{grpc::Channel}の検索プロセスでキャッシュ技術を使用しています。キャッシュで検索に失敗した場合、\code{FindChannelOnce}を呼び出して動的に\code{grpc::Channel}インスタンスを遅延作成し、キャッシュに追加します。

\begin{leftbar}
\begin{c++}
struct CachingGrpcChannelCache : GrpcChannelCache {
  SharedGrpcChannelPtr FindWorkerChannel(const string& target) override {
    SharedGrpcChannelPtr ch = nullptr;
    {
      mutex_lock l(mu_);
      ch = gtl::FindPtrOrNull(channels_, target);
      if (ch) {
        return ch;
      }
    }
    ch = FindChannelOnce(target);
    if (ch) {
      mutex_lock l(mu_);
      channels_.insert({target, ch});
    }
    return ch;
  }

 protected:
  virtual SharedGrpcChannelPtr FindChannelOnce(const string& target) = 0;

 private:
  mutex mu_;
  std::unordered_map<string, SharedGrpcChannelPtr> channels_;
};
\end{c++}
\end{leftbar}

\subsubsection{リーフノード}

\code{SparseGrpcChannelCache}の各インスタンスは1つの\ascii{Job}インスタンスに対応し、特定の\ascii{Job}用の対応する\code{grpc::Channel}インスタンスセットを作成します。各\ascii{Task}は1つの\code{grpc::Channel}に対応します。

そのうち、\code{FindChannelOnce}は\code{TranslateTask}を呼び出して\code{Worker}名から対応する\code{task\_id}を抽出し、次に\code{host\_ports\_}から\code{host:port}のアドレス情報をインデックスし、このアドレスを使用してファクトリメソッド\code{channel\_func\_}を呼び出して対応する\code{grpc::Channel}インスタンスを作成します。したがって、主に以下の3つの責任があります：

\begin{enum}
  \eitem{\code{ListWorkers}を通じて、その\ascii{Job}に対応する\ascii{Task}名リストを返します。例えば、\code{/job:ps}は\code{[/job:ps/replica:0/task:0, /job:ps/replica:0/task:1]}を返します。}
  \eitem{\code{TranslateTask}を通じて、特定の\ascii{Task}名に基づいて\code{host:port}のアドレス情報をインデックスします。例えば、\code{/job:ps/replica:0/task:0}のインデックスされたアドレスは\code{ps0:2222}です。}
  \eitem{\code{FindChannelOnce}を通じて、特定の\ascii{Task}名に基づいて対応する\code{grpc::Channel}インスタンスを作成します。例えば、\code{/job:ps/replica:0/task:0}に対して、\code{ps0:2222}をアドレスとする\code{grpc::Channel}インスタンスを作成します。}
\end{enum}


\begin{leftbar}
\begin{c++}
static string MakeAddress(const string& job, int task) {
  return strings::StrCat("/job:", job, "/replica:0/task:", task);
}

struct SparseGrpcChannelCache : CachingGrpcChannelCache {
  SparseGrpcChannelCache(
      const string& job_id,
      const std::map<int, string>& host_ports,
      ChannelCreationFunction channel_func)
      : job_id_(job_id), host_ports_(host_ports),
        channel_func_(std::move(channel_func)) {
  }

  void ListWorkers(std::vector<string>* workers) const override {
    workers->reserve(workers->size() + host_ports_.size());
    for (const auto& id_host_port : host_ports_) {
      workers->emplace_back(MakeAddress(job_id_, id_host_port.first));
    }
  }

  string TranslateTask(const string& target) override {
    DeviceNameUtils::ParsedName parsed;
    if (!DeviceNameUtils::ParseFullName(target, &parsed)) {
      return "";
    }
    auto iter = host_ports_.find(parsed.task);
    return iter == host_ports_.end() ? "" : iter->second;
  }

 protected:
  SharedGrpcChannelPtr FindChannelOnce(const string& target) override {
    auto host_port = TranslateTask(target);
    if (host_port.empty()) {
      return nullptr;
    }
    return channel_func_(host_port);
  }

 private:
  const string job_id_;
  const std::map<int, string> host_ports_;
  const ChannelCreationFunction channel_func_;
};
\end{c++}
\end{leftbar}

\subsubsection{非リーフノード}

\code{MultiGrpcChannelCache}は\code{caches\_}を通じて複数の\code{SparseGrpcChannelCache}インスタンスを保持し、クラスター内のすべての\ascii{Worker}ノードの\code{grpc::Channel}の組み合わせ作成を実現します。\code{SparseGrpcChannelCache}インスタンスの検索プロセスをさらに高速化するために、\code{MultiGrpcChannelCache}はアクセスされたことのある\code{SparseGrpcChannelCache}インスタンスをキャッシュしています。\code{SparseGrpcChannelCache}インスタンスのキャッシュ検索に失敗した場合にのみ、\code{caches\_}リストから対応する\code{SparseGrpcChannelCache}インスタンスをインデックスし、自動的にキャッシュに追加します。


\begin{leftbar}
\begin{c++}
class MultiGrpcChannelCache : public CachingGrpcChannelCache {
 public:
  explicit MultiGrpcChannelCache(
      const std::vector<GrpcChannelCache*>& caches) 
      : caches_(caches) {}

  ~MultiGrpcChannelCache() override {
    for (auto cache : caches_) {
      delete cache;
    }
  }

  void ListWorkers(std::vector<string>* workers) const override {
    for (auto cache : caches_) {
      cache->ListWorkers(workers);
    }
  }

  string TranslateTask(const string& target) override {
    mutex_lock l(mu_);  // could use reader lock
    auto cache = gtl::FindPtrOrNull(target_caches_, target);
    if (cache == nullptr) {
      for (auto c : caches_) {
        string r = c->TranslateTask(target);
        if (!r.empty()) {
          target_caches_.insert({target, c});
          cache = c;
          break;
        }
      }
    }
    return cache->TranslateTask(target);
  }

 protected:
  SharedGrpcChannelPtr FindChannelOnce(const string& target) override {
    for (auto cache : caches_) {
      auto ch = cache->FindWorkerChannel(target);
      if (ch) {
        mutex_lock l(mu_);
        target_caches_.insert({target, cache});
        return ch;
      }
    }
    return nullptr;
  }

 private:
  // List of channels used by this MultiGrpcChannelCache.
  const std::vector<GrpcChannelCache*> caches_;

  mutex mu_;
  // The same GrpcChannelCache can appear multiple times in the cache.
  std::unordered_map<string, GrpcChannelCache*> target_caches_;
};
\end{c++}
\end{leftbar}

\subsection{WorkerInterfaceの作成}

図\refig{dist-worker-cache-interface}に示すように、\code{GrpcWorkerCache}は\code{GrpcChannelCache}オブジェクトを保持し、それを通じて\code{grpc::Channel}インスタンスを作成し、\code{GrpcRemoteWorker}インスタンスの動的作成を実現しています。

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{figures/dist-worker-cache-interface.png}
\caption{\code{WorkerInterface}インスタンスの多態的作成}
 \label{fig:dist-worker-cache-interface}
\end{figure}

\begin{leftbar}
\begin{c++}
struct GrpcWorkerCache : WorkerCachePartial {
  GrpcWorkerCache(
      GrpcChannelCache* channel_cache,
      WorkerInterface* local_worker,
      const string& local_target)
      : local_target_(local_target),
        local_worker_(local_worker),
        channel_cache_(channel_cache) {}

  ~GrpcWorkerCache() override {
    live_rpc_counter_.WaitUntilUnused();
    delete channel_cache_;
  }

  void ListWorkers(std::vector<string>* workers) const override {
    channel_cache_->ListWorkers(workers);
  }

  WorkerInterface* CreateWorker(const string& target) override {
    if (target == local_target_) {
      return local_worker_;
    } else {
      auto channel = channel_cache_->FindWorkerChannel(target);
      if (!channel) return nullptr;
      return new GrpcRemoteWorker(&live_rpc_counter_, std::move(channel),
                                  &completion_queue_, &logger_);
    }
  }

  void ReleaseWorker(const string& target, 
      WorkerInterface* worker) override {
    if (target != local_target_) {
      WorkerCacheInterface::ReleaseWorker(target, worker);
    }
  }

 private:
  string local_target_;
  WorkerInterface* local_worker_;  // Not owned.
  GrpcCounter live_rpc_counter_;
  GrpcChannelCache* channel_cache_;  // Owned.
  ::grpc::CompletionQueue completion_queue_;
  WorkerCacheLogger logger_;
};
\end{c++}
\end{leftbar}

\end{content}

\section{セッション制御}

\begin{content}

\emph{セッション制御}は\tf{}分散ランタイムの核心であり、\tf{}実行エンジン全体の重要なパスでもあります。セッション制御の流れを整理するために、以下の記事ではセッション制御の詳細なプロセス全体について重点的に説明します。

\subsection{セッションの協調}

図\refig{dist-session-overview}に示すように、分散モードでは、セッション制御は\code{GrpcSession, MasterSession, WorkerSession}間の協調によって実現されます。これらはそれぞれ\code{Client, Master, Worker}に配置され、同じ\code{session\_handle}を使用して協調作業を実現します。

そのうち、\code{tf.Session}は\ascii{Python}で実装され、\tf{}が外部に提供する\ascii{API}です。これは\code{GrpcSession}と同じプロセス内にあり、\code{GrpcSession}のハンドル（またはポインタ）を直接保持して実現されています。

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/dist-session-overview-1.png}
\caption{セッションの協調}
 \label{fig:dist-session-overview}
\end{figure}

図\refig{dist-multi-client-conn}に示すように、分散モードでは、複数の\ascii{Client}が同時に1つの\ascii{Master}にアクセスする可能性があり、\ascii{Master}はアクセスする各\ascii{Client}に対して\code{MasterSession}インスタンスを作成します。\ascii{Worker}も同時に複数の\ascii{Master}に計算サービスを提供する可能性があり、\ascii{Worker}は計算をリクエストする各\ascii{Master}に対して\code{WorkerSession}インスタンスを作成します。異なる\ascii{Client}の計算サービスを区別するために、異なる\code{session\_handle}を使用して区別します。

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/dist-multi-client-conn.png}
\caption{セッション制御：ドメインモデル}
 \label{fig:dist-multi-client-conn}
\end{figure}

\subsection{ライフサイクル}

\code{GrpcSession}は\ascii{Client}のセッションライフサイクルを制御し、\code{MasterSession}は\ascii{Master}のセッションライフサイクルを制御し、\code{WorkerSession}は\ascii{Worker}のセッションライフサイクルを制御します。これらは\code{session\_handle}を通じて協調を実現します。

\subsubsection{GrpcSessionライフサイクル}

分散モードでは、\code{Client}のランタイムは\code{GrpcSession}によって制御されます。\code{GrpcSession}のライフサイクルプロセスは図\refig{dist-grpc-session-life-cycle}に示すとおりです。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/dist-grpc-session-life-cycle.png}
\caption{\code{GrpcSession}ライフサイクル}
 \label{fig:dist-grpc-session-life-cycle}
\end{figure}

\subsubsection{MasterSessionライフサイクル}

分散モードでは、\code{Master}のランタイムは\code{MasterSession}によって制御されます。\code{MasterSession}のライフサイクルプロセスは図\refig{dist-master-session-life-cycle}に示すとおりです。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/dist-master-session-life-cycle.png}
\caption{\code{MasterSession}ライフサイクル}
 \label{fig:dist-master-session-life-cycle}
\end{figure}

\subsubsection{WorkerSessionライフサイクル}

分散モードでは、\code{Worker}のランタイムは\code{WorkerSession}によって制御されます。\code{WorkerSession}のライフサイクルプロセスは図\refig{dist-worker-session-life-cycle}に示すとおりです。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/dist-worker-session-life-cycle.png}
\caption{\code{WorkerSession}ライフサイクル}
 \label{fig:dist-worker-session-life-cycle}
\end{figure}

\subsection{セッションプロセス}

ユーザープログラミング環境では、\ascii{Client}は\code{tf.Session(target)}を起点とし、\code{Session.run}を通じて反復実行を開始し、最終的に計算が完了した後に\code{Session.close}を呼び出してセッションを閉じます。しかし、分散実行エンジンの実装では、そのプロセスはより複雑です。

\begin{nitemize}
  \eitem{セッションの作成}    
    \begin{enum}
      \eitem{\code{GrpcSession}の作成;}  
      \eitem{リモートデバイスセットの取得;} 
      \eitem{\code{MasterSession}の作成;}
      \eitem{\code{WorkerSession}の作成;}      
    \end{enum}
  \eitem{反復実行}          
    \begin{enum}
      \eitem{実行の開始;}  
      \eitem{グラフの剪定;}  
      \eitem{グラフの分割;}        
      \eitem{サブグラフの登録;}              
      \eitem{サブグラフの実行;}                         
    \end{enum}      
  \eitem{セッションの終了}          
    \begin{enum}          
      \eitem{\code{GrpcSession}の終了;}  
      \eitem{\code{MasterSession}の終了;}
      \eitem{\code{WorkerSession}の終了;}      
    \end{enum}  
\end{nitemize}

\end{content}

\section{セッションの作成}

\begin{content}

計算を開始する前に、\ascii{Client}側で\code{GrpcSession}インスタンスを作成し、\ascii{Master}側で\code{MasterSession}インスタンスを作成する必要があります。各\ascii{Worker}上で\code{WorkerSession}インスタンスを作成し、これら3つは\code{MasterSession}の\code{session\_handle}を通じて協調を実現し、そのアクセスした\ascii{Client}インスタンスにサービスを提供します。

\subsection{GrpcSessionの作成}

\ascii{Client}が\code{tf.Session(target)}を呼び出すと、\code{TF\_NewDeprecatedSession}の\ascii{C API}インターフェースを呼び出すことで、\code{GrpcSession}インスタンスの作成がトリガーされます。そのうち、\ascii{C API}は\tf{}バックエンドシステムが外部に提供する多言語プログラミングの標準インターフェースです。最終的に、\code{tf.Session}は直接\code{GrpcSession}のハンドルを保持します。図\refig{dist-create-grpc-session-1}に示すとおりです。

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/dist-create-grpc-session-1.png}
\caption{\code{GrpcSession}の作成: \code{tf.Session}が\code{GrpcSession}ハンドルを保持}
 \label{fig:dist-create-grpc-session-1}
\end{figure}


\begin{leftbar}
\begin{c++}
Status NewSession(const SessionOptions& options, Session** out_session) {
  SessionFactory* factory;
  Status s = SessionFactory::GetFactory(options, &factory);
  if (!s.ok()) {
    *out_session = nullptr;
    return s;
  }
  *out_session = factory->NewSession(options);
  if (!*out_session) {
    return errors::Internal("Failed to create session.");
  }
  return Status::OK();
}

TF_DeprecatedSession* TF_NewDeprecatedSession(
  const TF_SessionOptions* opt, TF_Status* status) {
  Session* session;
  status->status = NewSession(opt->options, &session);
  if (status->status.ok()) {
    return new TF_DeprecatedSession({session});
  } else {
    return nullptr;
  }
}
\end{c++}
\end{leftbar}

図\refig{dist-grpc-session-factory}に示すように、\code{GrpcSession}は\code{GrpcSessionFactory}によって多態的に作成されます。\code{target}が\code{grpc://}で始まる場合、\code{SessionFactory::GetFactory}は\code{GrpcSessionFactory}インスタンスを返し、\code{GrpcSessionFactory::NewSession}のファクトリメソッドは\code{GrpcSession::Create}の静的ファクトリメソッドに\code{GrpcSession}インスタンスの作成を委託します。

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{figures/dist-grpc-session-factory.png}
\caption{GrpcSessionの多態的作成}
 \label{fig:dist-grpc-session-factory}
\end{figure}

\begin{leftbar}
\begin{c++}
const char* kSchemePrefix = "grpc://";

struct GrpcSessionFactory : SessionFactory {
  bool AcceptsOptions(const SessionOptions& options) override {
    return StringPiece(options.target).starts_with(kSchemePrefix);
  }

  Session* NewSession(const SessionOptions& options) override {
    std::unique_ptr<GrpcSession> ret;
    Status s = GrpcSession::Create(options, &ret);
    if (s.ok()) {
      return ret.release();
    } else {
      return nullptr;
    }
  }
};
\end{c++}
\end{leftbar}

\code{GrpcSession::Create}静的ファクトリメソッドは主に\code{GrpcSession}インスタンスの作成と、対応する初期化作業の完了を担当します。初期化プロセスで最も重要なのは\code{MasterInterface}インスタンスの構築です。そのうち、\code{MasterInterface}は\ascii{Client}が\ascii{Master}上の\code{MasterService}リモートサービスにアクセスするために使用され、2つのサブクラス実装が存在し、それぞれ2つの異なる応用シナリオに対応しています：

\begin{enum}
  \eitem{\code{LocalMaster}：\ascii{Client}と\ascii{Master}が同じプロセス内にある場合、\code{LocalMaster::Lookup}を呼び出して直接\code{LocalMaster}インスタンスを取得します。}
  \eitem{\code{GrpcRemoteMaster}：\ascii{Client}と\ascii{Master}が同じプロセス内にない場合、ファクトリメソッド\code{NewGrpcMaster}を呼び出して\code{GrpcRemoteMaster}インスタンスを生成します。}
\end{enum}

\code{GrpcRemoteMaster}インスタンスは\ascii{RPC}のクライアント実装であり、\code{GrpcRemoteMaster}インスタンスを作成する際には、まず\code{target}で指定された\ascii{Master}のアドレスとサービスポートに基づいて、それに接続する\ascii{RPC}チャネルを作成する必要があります。

\begin{leftbar}
\begin{c++}
Status GrpcSession::Create(
    const SessionOptions& options,
    std::unique_ptr<GrpcSession>* out_session) {
  std::unique_ptr<GrpcSession> session(new GrpcSession(options));
  std::unique_ptr<MasterInterface> master;
  // intra-process between client and master.
  if (!options.config.rpc_options().use_rpc_for_inprocess_master()) {
    master = LocalMaster::Lookup(options.target);
  }
  // inter-process between client and master.
  if (!master) {
    SharedGrpcChannelPtr master_channel;
    TF_RETURN_IF_ERROR(NewHostPortGrpcChannel(
        options.target.substr(strlen(kSchemePrefix)), &master_channel));
    master.reset(NewGrpcMaster(master_channel));
  }
  session->SetRemoteMaster(std::move(master));
  *out_session = std::move(session);
  return Status::OK();
}
\end{c++}
\end{leftbar}

\subsection{MasterSessionの作成}

図\refig{dist-create-master-session-1}に示すように、\code{GrpcSession}インスタンスが作成された後、続いて\code{GprcSession::Create}の呼び出しがトリガーされ、初期の計算グラフを\code{CreateSessionRequst}メッセージを通じて\ascii{Master}に送信します。\ascii{Master}が\code{CreateSessionRequst}メッセージを受信すると、対応する\code{MasterSession}インスタンスを生成し、グローバルに一意の\code{session\_handle}を使用してそのインスタンスを識別し、最終的に\code{CreateSessionResponse}メッセージを通じて\code{GrpcSession}に返します。

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/dist-create-master-session-1.png}
\caption{\code{MasterSession}の作成}
 \label{fig:dist-create-master-session-1}
\end{figure}

% \begin{figure}[H]
% \centering
% \includegraphics[width=1.0\textwidth]{figures/dist-create-master-session.png}
% \caption{创建\code{MasterSession}}
%  \label{fig:dist-create-master-session}
% \end{figure}

\subsubsection{GrpcSesion::Create(graph\_def)}

\code{GrpcSession::Create(graph\_def)}メソッドは主に\code{Client}が\code{Master}に\code{MasterSession}インスタンスの作成をリクエストするために使用されます。まず、\code{GrpcSession::Create}メソッドは\code{CreateSessionRequst}メッセージの構築を完了し、次に\code{GrpcRemoteMaster}を通じてそれを\ascii{Master}に送信します。

\code{GrpcSession}が\code{CreateSessionResponse}メッセージを受信すると、\code{MasterSession}の\code{handle}と初期計算グラフのバージョン番号\code{graph\_version}を保存します。そのうち、\code{handle}は\ascii{Master}側の\code{MasterSession}インスタンスを識別するために使用され、\code{graph\_version}は後続の計算グラフの拡張に使用されます。

\begin{leftbar}
\begin{c++}
void GrpcSession::BuildCreateSessionReq(
    const GraphDef& graph,
    CreateSessionRequest& req) {
  *req.mutable_config() = options_.config;
  *req.mutable_graph_def() = graph;
  req.set_target(options_.target);
}

void GrpcSession::SaveCreateSessionRsp(
    CreateSessionResponse& rsp) {
  mutex_lock l(mu_);
  swap(handle_, *(resp.mutable_session_handle()));
  current_graph_version_ = resp.graph_version();
}

Status GrpcSession::CreateImpl(CallOptions* call_options,
                               const GraphDef& graph) {
  CreateSessionRequest req;
  CreateSessionResponse resp;

  BuildCreateSessionReq(graph, req);
  Status s = master_->CreateSession(call_options, &req, &resp);
  if (s.ok()) {
    SaveCreateSessionRsp(resp);
  }
  return s;
}

Status GrpcSession::Create(const RunOptions& run_options,
                           const GraphDef& graph) {
  CallOptions call_options;
  call_options.SetTimeout(run_options.timeout_in_ms());
  return CreateImpl(&call_options, graph);
}

Status GrpcSession::Create(const GraphDef& graph) {
  CallOptions call_options;
  call_options.SetTimeout(options_.config.operation_timeout_in_ms());
  return CreateImpl(&call_options, graph);
}
\end{c++}
\end{leftbar}

\subsubsection{GrpcRemoteMaster::CreateSession}

\code{GrpcRemoteMaster}は\ascii{gRPC}のクライアント実装です。その実装は非常にシンプルで、\ascii{gRPC}の\code{stub}を通じてリモートの\ascii{Master}の対応するサービスインターフェースを呼び出します。

\begin{leftbar}
\begin{c++}
Status GrpcRemoteMaster::CreateSession(
    CallOptions* call_options,
    const CreateSessionRequest* request,
    CreateSessionResponse* response) override {
  ::grpc::ClientContext ctx;
  SetClientContext(*call_options, ctx);
  return FromGrpcStatus(stub_->CreateSession(&ctx, *request, response));
}
\end{c++}
\end{leftbar}

\subsubsection{GrpcMasterService::CreateSessionHandler}

\code{GrpcMasterService}は\ascii{gRPC}サービスで、\code{MasterService}の\ascii{RPC}サービスインターフェースを実装しています。\code{CreateSession}メッセージを受信すると、\code{GrpcMasterService::CreateSessionHandler}がコールバックされてそのメッセージを処理し、\code{Master}にそのメッセージの処理を委託します。

\code{Master}が処理を完了すると、完了時の\ascii{lambda}式をコールバックし、\ascii{Client}に\code{CreateSessionResponse}の応答メッセージを返します。

\begin{leftbar}
\begin{c++}
void GrpcMasterService::CreateSessionHandler(
  MasterCall<CreateSessionRequest, CreateSessionResponse>* call) {
  master_impl_->CreateSession(
    &call->request, &call->response,
    [call](const Status& status) {
        call->SendResponse(ToGrpcStatus(status));
    });
  ENQUEUE_REQUEST(CreateSession, true);
}
\end{c++}
\end{leftbar}

\subsubsection{Master::CreateSession}

\code{Master::CreateSession}はスレッドプール内で1つのスレッドを起動し、スレッド内で\code{cluster\_spec}情報に従ってすべての\ascii{Worker}を探し、リモートデバイスセットの情報を収集します。最後に、\code{MasterSession}を作成します。

\begin{remark}
リモートデバイスセットの検索プロセスについては、次のセクションで詳しく説明します。本セクションのサンプルコードではこの部分の実装を省略しています。
\end{remark}

\code{MasterSession}の作成が成功すると、\ascii{Master}は\code{(handle, master\_session)}の2つ組情報を保存し、後続の\ascii{Master}が\code{handle}を通じて対応する\code{MasterSession}インスタンスをインデックスできるようにします。

\begin{leftbar}
\begin{c++}
using RemoveDevices = unique_ptr<vector<unique_ptr<Device>>>;

void Master::CreateSession(const CreateSessionRequest* req,
                           CreateSessionResponse* resp, MyClosure done) {
  SchedClosure([this, req, resp, done]() {
    // 1. Find all remote devices. 
    WorkerCacheInterface* worker_cache = env_->worker_cache;
    RemoveDevices remote_devices(new vector<unique_ptr<Device>>());

    Status status = DeviceFinder::GetRemoteDevices(
        req->config().device_filters(), env_,
        worker_cache, remote_devices.get())

    if (!status.ok()) return;

    // 2. Build DeviceSet
    std::unique_ptr<DeviceSet> device_set(new DeviceSet);
    for (auto&& d : *remote_devices) {
      device_set->AddDevice(d.get());
    }

    int num_local_devices = 0;
    for (Device* d : env_->local_devices) {
      device_set->AddDevice(d);
      if (num_local_devices == 0) {
        // Uses the first local device as the client device.
        device_set->set_client_device(d);
      }
      num_local_devices++;
    }

    // 3. Create MasterSession
    SessionOptions options;
    options.config = req->config();
    
    MasterSession* session = env_->master_session_factory(
        options, env_, std::move(remote_devices), 
        std::move(worker_cache_ptr), std::move(device_set));

    GraphDef* gdef =
        const_cast<CreateSessionRequest*>(req)->mutable_graph_def();
    
    // ignore worker\_cache\_factory\_options implements.
    WorkerCacheFactoryOptions worker_cache_factory_options;
    Status status = session->Create(gdef, worker_cache_factory_options);
    resp->set_session_handle(session->handle());
    
    // 4. Store <handle, master\_session> pair.
    {
      mutex_lock l(mu_);
      CHECK(sessions_.insert({session->handle(), session}).second);
    }
  });
}
\end{c++}
\end{leftbar}

\subsubsection{MasterSession::Create(graph\_def)}

\code{MasterSession::Create(graph\_def)}は主に2つのことを行います。

\begin{enum}
  \eitem{計算グラフを初期化し、\code{SimpleGraphExecutionState}インスタンスを生成します。}
  \eitem{クラスターが動的に設定されている場合、すべての\ascii{Worker}に対応する\code{WorkerSession}インスタンスを作成するようブロードキャストします。}
\end{enum}

そのうち、\code{SimpleGraphExecutionState::MakeForBaseGraph}の実装はローカルモードと同じであり、ここでは再度説明しません。
\begin{leftbar}
\begin{c++}
Status MasterSession::Create(
    GraphDef* graph_def,
    const WorkerCacheFactoryOptions& options) {
  SimpleGraphExecutionStateOptions execution_options;
  execution_options.device_set = devices_.get();
  execution_options.session_options = &session_opts_;
  {
    mutex_lock l(mu_);
    TF_RETURN_IF_ERROR(SimpleGraphExecutionState::MakeForBaseGraph(
        graph_def, execution_options, &execution_state_));
  }

  // CreateWorkerSessions should be called only with
  // dynamic cluster membership.
  if (options.cluster_def != nullptr) {
    return CreateWorkerSessions(options);
  }
  return Status::OK();
}
\end{c++}
\end{leftbar}

\subsection{リモートデバイスセットの取得}

図\refig{dist-worker-get-status}に示すように、\code{MasterSession}の作成前に、\code{MasterSession}はすべての\code{Worker}インスタンスをポーリングし、リモートのすべての\code{Worker}のデバイス情報を取得します。\code{DeviceFinder}のデバイスファインダーを利用して、\code{DeviceFinder::GetRemoteDevices}を呼び出してリモートデバイスセットを取得します。

その動作原理は非常にシンプルです。\code{GrpcWorkerCache::ListWorkers}を使用してクラスター内のすべての\ascii{Worker}の名前リストを取得し、次に\code{worker\_name}の名前に基づいて\code{GrpcWorkerCache::CreateWorker}ファクトリメソッドを呼び出して\code{WorkerInterface}インスタンスを作成します。後者はリモートの\code{WorkerService}サービスにアクセスするために使用されます。最後に、\code{WorkerInterface}を通じてリモートの\ascii{Worker}リストに\code{GetStatusRequest}リクエストメッセージをブロードキャスト送信し、リモートデバイスセットの取得を実現します。

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/dist-worker-get-status.png}
\caption{リモートデバイスセットの取得}
 \label{fig:dist-worker-get-status}
\end{figure}

\subsubsection{デバイスファインダー}

\code{DeviceFinder}は関数オブジェクトを実装し、リモートデバイス検索のアルゴリズムを実装しています。そのプロセスは3つのステップに分かれています：

\begin{enum}
  \eitem{\code{Start}: 並行してクラスター内のすべての\code{Worker}インスタンスに\code{GetStatusRequest}をブロードキャストします。}
  \eitem{\code{Wait}: すべての\code{Worker}から返された\code{GetStatusResponse}メッセージを収集します。}
  \eitem{\code{GetRemoteDevices}: クエリ結果を取得し、クライアントに返します。}
\end{enum}

\begin{leftbar}
\begin{c++}
struct DeviceFinder {
  static Status DeviceFinder::GetRemoteDevices(
      MasterEnv* env,
      WorkerCacheInterface* worker_cache,
      std::vector<std::unique_ptr<Device>>* out_remote) {
    DeviceFinder finder(env, worker_cache);
    finder.Start();
    TF_RETURN_IF_ERROR(finder.Wait());
    finder.GetRemoteDevices(env->local_devices, out_remote);
    return Status::OK();
  }
};
\end{c++}
\end{leftbar}

複数の\code{Worker}からの\code{GetStatusResponse}メッセージを収集制御するために、ここでは\code{num\_pending\_}カウンターを使用しています。\code{DeviceFinder::Start}で初期値を\code{Worker}の数に設定します。

ある\code{Worker}からの\code{GetStatusResponse}メッセージを受信すると、\code{WhenDone}がコールバックされ、カウンターを1減少させます。カウンターが0になると、\code{pending\_zero\_.notify\_all}を呼び出して\code{pending\_zero\_.wait\_for}文を起こし、その後\code{finder.GetRemoteDevices}を通じてクエリ結果を取得できます。

そのうち、\code{DeviceFinder::Start}では、\code{NewRemoteDevices}を通じてすべての\code{Worker}に\code{GetStatusRequest}メッセージをブロードキャストし、対応するデバイス情報をクエリします。次のセクションでその実装プロセスについて詳しく説明します。
\begin{leftbar}
\begin{c++}
struct DeviceFinder {
 private:
  explicit DeviceFinder(
      MasterEnv* env,
      WorkerCacheInterface* worker_cache)
      : env_(env), worker_cache_(worker_cache) {
    worker_cache->ListWorkers(&targets_);
    seen_targets_.assign(targets_.size(), false);
  }

  ~DeviceFinder() {
    for (auto dev : found_) delete dev;
  }

  void Start() {
    {
      mutex_lock l(mu_);
      num_pending_ = targets_.size();
    }

    // Talk to all workers to get the list of available devices.
    using std::placeholders::_1;
    using std::placeholders::_2;
    for (size_t i = 0; i < targets_.size(); ++i) {
      NewRemoteDevices(env_->env, worker_cache_, targets_[i],
                       std::bind(&ME::WhenFound, this, i, _1, _2));
    }
  }

  // The caller takes the ownership of returned remote devices.
  void GetRemoteDevices(
      const std::vector<Device*>& local,
      std::vector<std::unique_ptr<Device>>* remote) {
    std::unordered_set<string> names(local.size());
    for (auto dev : local) {
      names.insert(dev->name());
    }

    mutex_lock l(mu_);
    for (auto dev : found_) {
      auto& name = dev->name();
      if (names.insert(name).second) {
        remote->push_back(std::unique_ptr<Device>(dev));
      } else {
        delete dev;
      }
    }
    found_.clear();
  }

  Status Wait() {
    mutex_lock l(mu_);
    while (num_pending_ != 0) {
      pending_zero_.wait_for(l, std::chrono::milliseconds(10 * 1000));
      if (num_pending_ != 0) {
        for (size_t i = 0; i < targets_.size(); ++i) {
          if (!seen_targets_[i]) {
            LOG(INFO)
                << "CreateSession still waiting for response from worker: "
                << targets_[i];
          }
        }
      }
    }
    return status_;
  }

  void WhenFound(int target_index, const Status& s,
                 std::vector<Device*>* devices) {
    mutex_lock l(mu_);
    seen_targets_[target_index] = true;
    if (!s.ok()) {
      status_.Update(s);
    } else {
      found_.insert(found_.end(), devices->begin(), devices->end());
      devices->clear();
    }
    --num_pending_;
    if (num_pending_ == 0) {
      pending_zero_.notify_all();
    }
  }

  typedef DeviceFinder ME;
  const MasterEnv* env_;
  WorkerCacheInterface* worker_cache_;

  mutex mu_;
  int num_pending_ GUARDED_BY(mu_);
  condition_variable pending_zero_;
  std::vector<Device*> found_ GUARDED_BY(mu_);

  std::vector<string> targets_;
  std::vector<bool> seen_targets_ GUARDED_BY(mu_);
  Status status_;
};
\end{c++}
\end{leftbar}

注意点として、\code{num\_pending\_}カウンターがゼロでない場合、メインスレッドは10秒ごとに周期的にスリープし、起動時に\ascii{Worker}がまだ応答メッセージを返していないことを発見した場合、それらの\ascii{Worker}の名前を出力します。以下のような情報が繰り返し出力されるのを見た場合、\code{/job:worker/task:2}に対応する\code{Server}が異常終了したか、\ascii{Master}とそれに対応する\ascii{Worker}間のネットワークに異常が発生したかなどを、具体的な状況に応じて分析し対処する必要があります。

\begin{leftbar}
\begin{python}
CreateSession still waiting for response from worker: /job:worker/task:2
\end{python}
\end{leftbar}

\subsubsection{NewRemoteDevices}

\code{NewRemoteDevices}は\code{worker\_name}に基づいて\code{WorkerInterface}インスタンスを検索し、\code{GetStatusRequest}メッセージを対応する\code{Worker}に送信してそのデバイス情報を取得します。メッセージが返されると、\code{cb}関数オブジェクトがコールバックされます。そのうち、リモートの\code{Worker}から取得したデバイス情報は完全ではなく、\code{worker\_name}の情報が含まれていないため、手動で追加する必要があります。

\begin{leftbar}
\begin{c++}
void NewRemoteDevices(
    Env* env, WorkerCacheInterface* worker_cache,
    const string& worker_name, NewRemoteDevicesDone done) {
  struct Call {
    GetStatusRequest req;
    GetStatusResponse resp;
  };

  WorkerInterface* wi = worker_cache->CreateWorker(worker_name);
  Call* call = new Call;
  auto cb = [env, worker_cache, &worker_name, &done, wi, call](
      const Status& status) {
    Status s = status;
    std::vector<Device*> remote_devices;
    auto cleanup = gtl::MakeCleanup(
        [worker_cache, &worker_name, wi, &done, &remote_devices, &s, call] {
          worker_cache->ReleaseWorker(worker_name, wi);
          done(s, &remote_devices);
          delete call;
        });
    if (s.ok()) {
      DeviceNameUtils::ParsedName worker_name_parsed;
      DeviceNameUtils::ParseFullName(worker_name, &worker_name_parsed);

      remote_devices.reserve(call->resp.device_attributes_size());

      for (auto& da : call->resp.device_attributes()) {
        DeviceNameUtils::ParsedName device_name_parsed;
        DeviceNameUtils::ParseFullName(da.name(), &device_name_parsed);
        
        DeviceAttributes da_rewritten = da;
        da_rewritten.set_name(DeviceNameUtils::FullName(
            worker_name_parsed.job, worker_name_parsed.replica,
            worker_name_parsed.task, device_name_parsed.type,
            device_name_parsed.id));
        auto d = new RemoteDevice(env, da_rewritten);
        remote_devices.push_back(d);
      }
    }
  };
  wi->GetStatusAsync(&call->req, &call->resp, cb);
}
\end{c++}
\end{leftbar}

\subsubsection{GrpcRemoteWorker::GetStatusAsync}

\code{GrpcRemoteWorker}は\code{WorkerInterface}の具体的な実装で、\ascii{gRPC}のクライアント実装です。\code{stub}を通じてリモートの\code{WorkerService}の対応するサービスインターフェースを呼び出します。

\begin{leftbar}
\begin{c++}
struct GrpcRemoteWorker : WorkerInterface {
  void GetStatusAsync(const GetStatusRequest* request,
                      GetStatusResponse* response,
                      StatusCallback done) override {
    IssueRequest(request, response, getstatus_, std::move(done));
  }
}
\end{c++}
\end{leftbar}

\subsubsection{GrpcWorkerService::GetStatusHandler}

\code{GrpcWorkerService}は\code{WorkerService}の具体的な実装です。\code{GetStatusRequest}メッセージを受信すると、\code{GetStatusHandler}がコールバックされて処理します。

\begin{leftbar}
\begin{c++}
struct GrpcWorkerService : AsyncServiceInterface {
  void GetStatusHandler(WorkerCall<GetStatusRequest, GetStatusResponse>* call) {
    Schedule([this, call]() {
      Status s = worker_->GetStatus(&call->request, &call->response);
      call->SendResponse(ToGrpcStatus(s));
    });
    ENQUEUE_REQUEST(GetStatus, false);
  }
};
\end{c++}
\end{leftbar}

\subsubsection{Worker::GetStatusAsync}

\code{Worker::GetStatusAsync}は\code{DeviceMgr}にローカルデバイス情報の集約を委託し、最終的に\code{GetStatusResponse}メッセージを通じて対向側に返します。

\begin{leftbar}
\begin{c++}
void Worker::GetStatusAsync(const GetStatusRequest* request,
                            GetStatusResponse* response, StatusCallback done) {
  std::vector<DeviceAttributes> devices;
  env_->device_mgr->ListDeviceAttributes(&devices);
  response->mutable_device_attributes()->Reserve(devices.size());
  for (auto& d : devices) {
    response->add_device_attributes()->Swap(&d);
  }
  done(Status::OK());
}
\end{c++}
\end{leftbar}

\code{DeviceMgr}はローカルデバイスセットを保持しており、その実装は非常にシンプルです。

\begin{leftbar}
\begin{c++}
void DeviceMgr::ListDeviceAttributes(
    std::vector<DeviceAttributes>* devices) const {
  devices->reserve(devices_.size());
  for (auto dev : devices_) {
    devices->emplace_back(dev->attributes());
  }
}
\end{c++}
\end{leftbar}

\subsection{WorkerSessionの作成}

\code{MasterSession}の作成が成功した後、クラスターが動的に設定されていない場合（デフォルトの分散設定環境）、すべての\ascii{Worker}に\code{WorkerSession}を動的に作成するようブロードキャストしません。実際、各\ascii{Worker}には\code{SessionMgr}インスタンスが存在し、\code{legacy\_session\_}という名前の\code{WorkerSession}インスタンスを保持しています。したがって、各\ascii{Worker}にはグローバルに一意の\code{WorkerSession}インスタンスが存在します。

\begin{leftbar}
\begin{c++}
SessionMgr::SessionMgr(
    WorkerEnv* worker_env, 
    const string& default_worker_name,
    std::unique_ptr<WorkerCacheInterface> default_worker_cache,
    WorkerCacheFactory worker_cache_factory)
    : worker_env_(worker_env),
      legacy_session_(
          default_worker_name, 
          std::move(default_worker_cache),
          std::unique_ptr<DeviceMgr>(worker_env->device_mgr),
          std::unique_ptr<GraphMgr>(
              new GraphMgr(worker_env, 
              worker_env->device_mgr))),
      worker_cache_factory_(std::move(worker_cache_factory)) {}
\end{c++}
\end{leftbar}

図\refig{dist-create-worker-session}に示すように、動的クラスター設定が存在する場合、\ascii{Master}は各\ascii{Worker}にそれぞれ\code{WorkerSession}インスタンスを作成するようブロードキャストし、\code{sessin\_handle}を使用してその\code{WorkerSession}を識別します。これらの\code{WorkerSession}はこの\code{MasterSession}インスタンスに属しています。なぜなら、それらは\code{MasterSession}インスタンスと同じ\code{session\_handle}識別子を使用しているからです。

そのうち、\code{MasterSession}はすべての\ascii{Worker}から返された\code{CreateWorkerSessionResponse}メッセージを収集するために、\code{BlockingCounter}カウンターを導入しています。\code{BlockingCounter}カウンターの初期値は\ascii{Worker}の数で、各\ascii{Worker}からの応答メッセージを受信するたびにカウンターを1減少させ、カウンターが0になるまで\code{done.Wait()}が起こされます。

また、\code{WorkerInterface}インスタンスは\code{WorkerCacheInterface}を通じてクエリまたは作成されます。後でその動作原理について詳しく説明します。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/dist-create-worker-session.png}
\caption{\code{WorkerSession}の動的作成}
 \label{fig:dist-create-worker-session}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/dist-create-worker-session.png}
\caption{动态创建\code{WorkerSession}}
 \label{fig:dist-create-worker-session}
\end{figure}

\begin{leftbar}
\begin{c++}
struct MasterSession::Worker {
  Worker(MasterSession* sess, const string& name,
         const DeviceNameUtils::ParsedName& parsed_name,
         const WorkerCacheFactoryOptions& opts)
      : sess(sess), name(&name), worker(GetOrCreateWorker()) {
    BuildRequest(parsed_name, opts);
  }

  void CreateWorkerSession(BlockingCounter& done, Status& status) {
    auto cb = [&status, &done](const Status& s) {
      status.Update(s);
      done.DecrementCount();
    };
    // IMPORTANT: notify worker to create worker session.
    worker->CreateWorkerSessionAsync(&request, &response, cb);
  }

  void Release() {
    if (worker != nullptr) {
      sess->worker_cache_->ReleaseWorker(*name, worker);
    }
  }

 private:
  WorkerInterface* GetOrCreateWorker() {
    return sess->worker_cache_->CreateWorker(*name);
  }

  void BuildRequest(const DeviceNameUtils::ParsedName& parsed_name,
                    const WorkerCacheFactoryOptions& opts) {
    request.set_session_handle(sess->handle_);
    BuildServerDef(parsed_name, opts, request.mutable_server_def());
  }

  void BuildServerDef(const DeviceNameUtils::ParsedName& parsed_name,
                      const WorkerCacheFactoryOptions& opts,
                      ServerDef* server_def) {
    *server_def->mutable_cluster() = *opts.cluster_def;
    server_def->set_protocol(*opts.protocol);
    server_def->set_job_name(parsed_name.job);
    server_def->set_task_index(parsed_name.task);
  }

 private:
  MasterSession* sess;

  // The worker name. (Not owned.)
  const string* name;

  // The worker referenced by name. (Not owned.)
  WorkerInterface* worker = nullptr;

  // Request and responses used for a given worker.
  CreateWorkerSessionRequest request;
  CreateWorkerSessionResponse response;
};

struct MasterSession::WorkerGroup {
  WorkerGroup(MasterSession* sess) : sess(sess) {}

  Status CreateWorkerSessions(const WorkerCacheFactoryOptions& opts) {
    TF_RETURN_IF_ERROR(CreateWorkers(opts));
    TF_RETURN_IF_ERROR(BroadcastWorkers());
    return Status::OK();
  }

  void ReleaseWorkers() {
    for (auto& worker : workers) {
      worker.Release();
    }
  }

 private:
  Status CreateWorkers(const WorkerCacheFactoryOptions& opts) {
    sess->worker_cache_->ListWorkers(&worker_names);
    for (auto& worker_name : worker_names) {
      TF_RETURN_IF_ERROR(AppendWorker(worker_name, opts));
    }
    return Status::OK();
  }

  // broadcast all workers to create worker session.
  Status BroadcastWorkers() {
    Status status = Status::OK();
    BlockingCounter done(workers.size());
    for (auto& worker : workers) {
      worker.CreateWorkerSession(done, status);
    }
    done.Wait();
    return status;
  }

  Status AppendWorker(const string& worker_name,
                    const WorkerCacheFactoryOptions& opts) {
    DeviceNameUtils::ParsedName parsed_name;
    TF_RETURN_IF_ERROR(ParseWorkerName(worker_name, &parsed_name));
    workers.emplace_back(Worker(sess, worker_name, parsed_name, opts));
    return Status::OK();
  }

  Status ParseWorkerName(const string& worker_name,
                         DeviceNameUtils::ParsedName* parsed_name) {
    if (!DeviceNameUtils::ParseFullName(worker_name, parsed_name)) {
      return errors::Internal("Could not parse name ", worker_name);
    }
    if (!parsed_name->has_job || !parsed_name->has_task) {
      return errors::Internal("Incomplete worker name ", worker_name);
    }
    return Status::OK();
  }

 private:
  MasterSession* sess;
  std::vector<string> worker_names;
  std::vector<Worker> workers;
};

Status MasterSession::CreateWorkerSessions(
    const WorkerCacheFactoryOptions& options) {
  CHECK(worker_cache_) << "CreateWorkerSessions should be called only with "
                       << "dynamic cluster membership.";

  WorkerGroup worker_group(this);

  // Release the workers.
  auto cleanup = gtl::MakeCleanup([&worker_group] {
    worker_group.ReleaseWorkers();
  });

  return worker_group.CreateWorkerSessions(options);
}
\end{c++}
\end{leftbar}

\subsubsection{GrpcRemoteWorker}

\code{GrpcRemoteWorker}はリモートの\ascii{Worker}にアクセスする\ascii{gRPC}クライアントです。対応する\code{stub}を呼び出してリモートサービスを呼び出します。

\begin{leftbar}
\begin{c++}
struct GrpcRemoteWorker : WorkerInterface {
  void CreateWorkerSessionAsync(
      const CreateWorkerSessionRequest* request,
      CreateWorkerSessionResponse* response,
      StatusCallback done) override {
    IssueRequest(request, response, createworkersession_, std::move(done));
  }
};
\end{c++}
\end{leftbar}

\subsubsection{GrpcWorkerService::CreateWorkerSessionHandler}

\ascii{Worker}側では、\code{CreateWorkerSession}メッセージは\code{CreateWorkerSessionHandler}によってコールバック処理されます。これは実行可能なスレッドをスレッドプールで起動し、\code{Worker}が動的に\code{WorkerSession}インスタンスを作成するようトリガーします。

\begin{leftbar}
\begin{c++}
struct GrpcWorkerService : AsyncServiceInterface {
  void CreateWorkerSessionHandler(
      WorkerCall<CreateWorkerSessionRequest, CreateWorkerSessionResponse>*
          call) {
    Schedule([this, call]() {
      Status s = worker_->CreateWorkerSession(&call->request, &call->response);
      call->SendResponse(ToGrpcStatus(s));
    });
    ENQUEUE_REQUEST(CreateWorkerSession, false);
  }
};
\end{c++}
\end{leftbar}

\subsubsection{WorkerSessionインスタンスの作成}

\code{Worker}は\code{WorkerSession}インスタンスの作成責任を\code{SessionMgr}に委託し、すべての\code{WorkerSession}インスタンスのライフサイクルを統一的に管理および維持します。図\refig{dist-worker-session-manager}に示すように、\code{SessionMgr}は複数の\code{WorkerSession}インスタンスを保持する可能性があり、各\code{WorkerSession}インスタンスは\code{session\_handle}で識別されます。

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/dist-worker-session-manager.png}
\caption{\code{Session}マネージャ}
 \label{fig:dist-worker-session-manager}
\end{figure}

\begin{leftbar}
\begin{c++}
void Worker::CreateWorkerSessionAsync(
    const CreateWorkerSessionRequest* request,
    CreateWorkerSessionResponse* response,
    StatusCallback done) {
  Status s = env_->session_mgr->CreateSession(
      request->session_handle(),
      request->server_def());
  done(s);
}
\end{c++}
\end{leftbar}

図\refig{dist-worker-session-model}に示すように、\code{WorkerSession}は\code{GraphMgr}インスタンスを保持しており、複数のグラフインスタンスを登録および実行するために使用されます。そのうち、各グラフインスタンスは\code{graph\_handle}で識別されます。同時に、各\code{WorkerSession}は\code{DeviceMgr}インスタンスを保持しており、ローカル計算デバイスのコレクションを管理するために使用されます。

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{figures/dist-worker-session-model.png}
\caption{\code{WorkerSession}ドメインモデル：複数のグラフインスタンスを登録および実行可能}
 \label{fig:dist-worker-session-model}
\end{figure}

\begin{leftbar}
\begin{c++}
Status SessionMgr::CreateSession(const string& session,
                                 const ServerDef& server_def) {
  mutex_lock l(mu_);

  // 1. Create WorkerCacheInterface
  WorkerCacheInterface* worker_cache = nullptr;
  TF_RETURN_IF_ERROR(worker_cache_factory_(server_def, &worker_cache));

  // 2. Rename local devices  
  auto worker_name = WorkerNameFromServerDef(server_def);
  std::vector<Device*> renamed_devices;
  for (Device* d : worker_env_->local_devices) {
    renamed_devices.push_back(
        RenamedDevice::NewRenamedDevice(worker_name, d, false));
  }
  std::unique_ptr<DeviceMgr> device_mgr(new DeviceMgr(renamed_devices));

  // 3. Create GraphMgr
  std::unique_ptr<GraphMgr> graph_mgr(
      new GraphMgr(worker_env_, device_mgr.get()));
  
  // 4. Create WorkerSession
  std::unique_ptr<WorkerSession> worker_session(new WorkerSession(
      worker_name, std::unique_ptr<WorkerCacheInterface>(worker_cache),
      std::move(device_mgr), std::move(graph_mgr)));

  // 5. Store (session\_handle, WorkerSession) pair.
  sessions_.insert(std::make_pair(session, std::move(worker_session)));
  return Status::OK();
}
\end{c++}
\end{leftbar}

\section{反復実行}

\begin{content}

\subsection{実行の開始}

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/dist-run-step-stage-1.png}
\caption{GprcSession: RunStepの開始}
 \label{fig:dist-run-step-stage-1}
\end{figure}

\subsubsection{GrpcSession::Run}

\begin{leftbar}
\begin{c++}
namespace {
  using TensorIndex = std::unordered_map<string, int>;

  void BuildReqOptions(const SessionOptions& sess_options,
      const RunOptions& run_options, 
      RunOptions& options) {
    options = run_options;
    if (run_options.timeout_in_ms() == 0) {
      options.set_timeout_in_ms(
          sess_options.config.operation_timeout_in_ms());
    }    
  }

  void BuildReqFeeds(const vector<pair<string, Tensor>>& inputs,
      MutableRunStepRequestWrapper* req) {
    for (auto& it : inputs) {
      req->add_feed(it.first, it.second);
    }
  }

  void BuildReqFetches(const std::vector<string>& output_names,
      MutableRunStepRequestWrapper* req) {
    for (int i = 0; i < output_names.size(); ++i) {
      req->add_fetch(output_names[i]);
  }

  void BuildReqTargets(const std::vector<string>& target_names,
      MutableRunStepRequestWrapper* req) {
    for (string& target : target_names) {
      req->add_target(target);
    }
  }

  void BuildRunStepReq(
      const SessionOptions& sess_options,
      const RunOptions& run_options,
      const vector<pair<string, Tensor>>& inputs,
      const std::vector<string>& output_names,
      const std::vector<string>& target_names,
      MutableRunStepRequestWrapper* req) {
    BuildReqOptions(sess_options, run_options, 
        req->mutable_options());
    BuildReqFeeds(inputs, req);
    BuildReqFetches(output_names, req);
    BuildReqTargets(target_names, req); 
  }

  void BuildOuputNamesIndex(
      const std::vector<string>& output_names,
      TensorIndex& tensor_index) {
    for (int i = 0; i < output_names.size(); ++i) {
      const string& name = output_names[i];
      tensor_index.insert(make_pair(name, i));
    }
  }

  void BuildCallOptions(const RunOptions& options, 
      CallOptions& call_options) {
    call_options.SetTimeout(options.timeout_in_ms());
  }

  Status DoSaveOutputs(const TensorIndex& tensor_index,
      const std::vector<string>& output_names,
      MutableRunStepResponseWrapper* resp,
      std::vector<Tensor>* outputs) {
    for (size_t i = 0; i < resp->num_tensors(); ++i) {
      auto fetch_it = tensor_index.find(resp->tensor_name(i));
      if (fetch_it == tensor_index.end()) {
        return errors::Internal(
           "unrequested fetch: ", resp->tensor_name(i));
      }

      Tensor output;
      TF_RETURN_IF_ERROR(resp->TensorValue(i, &output));
      (*outputs)[fetch_it->second] = output;
    }  
  }

  Status SaveOutputs(const TensorIndex& tensor_index,
      const std::vector<string>& output_names,
      MutableRunStepResponseWrapper* resp,
      std::vector<Tensor>* outputs) {
    if (!output_names.empty()) {
      outputs->resize(output_names.size());
    }
    return DoSaveOutputs(tensor_index, 
        output_names, rsep, outputs);
  }

  void SaveRunMetaData(MutableRunStepResponseWrapper* resp,
      RunMetadata* run_metadata) {
    if (run_metadata) {
      run_metadata->Swap(resp->mutable_metadata());
    }
  }
  
  Status SaveRspToOutputs(const TensorIndex& tensor_index,
      const std::vector<string>& output_names,
      MutableRunStepResponseWrapper* resp,
      std::vector<Tensor>* outputs,
      RunMetadata* run_metadata) {
    SaveRunMetaData(resp, run_metadata);
    return SaveOutputs(tensor_index, output_names, rsep, outputs);
  }
}

Status GrpcSession::Run(
    const RunOptions& run_options,
    const vector<pair<string, Tensor>>& inputs,
    const vector<string>& output_names,
    const vector<string>& target_names,
    std::vector<Tensor>* outputs,
    RunMetadata* run_metadata) {
  // 1. Build run step request.
  unique_ptr<MutableRunStepRequestWrapper> req(
      master_->CreateRunStepRequest());

  unique_ptr<MutableRunStepResponseWrapper> resp(
      master_->CreateRunStepResponse());

  BuildRunStepReq(options_, run_options, inputs, 
      output_names, target_names, req.get());

  // 2. Build output tensor names index.
  TensorIndex tensor_index;
  BuildOuputNamesIndex(output_names, tensor_index);

  // 3. Build call options.
  CallOptions call_options;
  BuildCallOptions(req->options(), call_options)

  // 4. Do run step.
  TF_RETURN_IF_ERROR(RunProto(&call_options, 
      req.get(), resp.get()));

  // 5. Save response to outputs.
  return SaveRspToOutputs(tensor_index, output_names, 
      resp.get(), outputs, run_metadata);
}
\end{c++}
\end{leftbar}

\begin{leftbar}
\begin{c++}
Status GrpcSession::RunProto(
    CallOptions* call_options,
    MutableRunStepRequestWrapper* req,
    MutableRunStepResponseWrapper* resp) {
  {
    mutex_lock l(mu_);
    req->set_session_handle(handle_);
  }
  return master_->RunStep(call_options, req, resp);
}
\end{c++}
\end{leftbar}

\subsubsection{GrpcRemoteMaster::RunStep}

\begin{leftbar}
\begin{c++}
struct GrpcRemoteMaster : MasterInterface {
  using MasterServiceStub = ::grpc::MasterService::Stub;

  Status RunStep(CallOptions* call_options, RunStepRequestWrapper* request,
                 MutableRunStepResponseWrapper* response) override {
    ::grpc::ClientContext ctx;
    return Call(&ctx, call_options, &request->ToProto(),
                get_proto_from_wrapper(response),
                &MasterServiceStub::RunStep);
  }
};
\end{c++}
\end{leftbar}

\begin{leftbar}
\begin{c++}
struct GrpcMasterService : AsyncServiceInterface {
  using RunStepCall = MasterCall<RunStepRequest, RunStepResponse>;
 
  void RunStepHandler(RunStepCall* call) {
    CallOptions* call_opts = CreateCallOptions(call);

    RunStepRequestWrapper* wrapped_request =
        new ProtoRunStepRequest(&call->request);

    MutableRunStepResponseWrapper* wrapped_response =
        new NonOwnedProtoRunStepResponse(&call->response);
  
    call->SetCancelCallback([call_opts]() { 
        call_opts->StartCancel(); 
    });

    master_impl_->RunStep(call_opts, wrapped_request, wrapped_response,
      [call, call_opts, wrapped_request, wrapped_response](
          const Status& status) {
        call->ClearCancelCallback();
        delete call_opts;
        delete wrapped_request;
        call->SendResponse(ToGrpcStatus(status));
      });
    ENQUEUE_REQUEST(RunStep, true);
  }

 private:
  CallOptions* CreateCallOptions(RunStepCall* call) {
    CallOptions* call_opts = new CallOptions;
    if (call->request.options().timeout_in_ms() > 0) {
      call_opts->SetTimeout(call->request.options().timeout_in_ms());
    } else {
      call_opts->SetTimeout(default_timeout_in_ms_);
    }
    return call_opts; 
  }
};
\end{c++}
\end{leftbar}

\subsubsection{Master::RunStep}

\begin{leftbar}
\begin{c++}
void Master::RunStep(CallOptions* opts, 
    const RunStepRequestWrapper* req,
    MutableRunStepResponseWrapper* resp, 
    DoneClosure done) {
  auto session = FindMasterSession(req->session_handle());
  SchedClosure([this, session, opts, req, resp, done]() {
    Status status = session->Run(opts, *req, resp);
    session->Unref();
    done(status);
  });
}
\end{c++}
\end{leftbar}

\subsubsection{MasterSession::Run}

\begin{leftbar}
\begin{c++}
Status MasterSession::Run(
    CallOptions* opts, 
    const RunStepRequestWrapper& req,
    MutableRunStepResponseWrapper* resp) {
  Status status;
  if (!req.partial_run_handle().empty()) {
    status = DoPartialRun(opts, req, resp);
  } else {
    status = DoRunWithLocalExecution(opts, req, resp);
  }
  return status;
}
\end{c++}
\end{leftbar}

\begin{leftbar}
\begin{c++}
Status MasterSession::DoRunWithLocalExecution(
    CallOptions* opts, const RunStepRequestWrapper& req,
    MutableRunStepResponseWrapper* resp) {

  // 1. Prune: build ReffedClientGraph. 
  BuildGraphOptions bgopts;
  BuildBuildGraphOptions(req, &bgopts);
  
  ReffedClientGraph* rcg = nullptr;
  int64 count = 0;
  TF_RETURN_IF_ERROR(StartStep(bgopts, &count, &rcg, false));

  // 2. Build and Register partitions to workers. 
  core::ScopedUnref unref(rcg);
  TF_RETURN_IF_ERROR(BuildAndRegisterPartitions(rcg));

  // 3. Run partitions: notify all of workers to run partitions.
  uint64 step_id = (random::New64() & ((1uLL << 56) - 1)) | (1uLL << 56);
  Status s = rcg->RunPartitions(env_, step_id, count, &pss, opts, req, resp,
                                &cancellation_manager_, false);
  // 4. Cleaup Partitions: notify all of workers to clearup partitions.
  Ref();
  rcg->Ref();
  rcg->CleanupPartitionsAsync(step_id, [this, rcg](const Status& s) {
    rcg->Unref();
    Unref();
  });
  return s;
}
\end{c++}
\end{leftbar}

\subsubsection{MasterSession::BuildAndRegisterPartitions}

\begin{leftbar}
\begin{c++}
Status MasterSession::BuildAndRegisterPartitions(ReffedClientGraph* rcg) {
  PartitionOptions popts;
  popts.node_to_loc = SplitByWorker; // IMPORTANT
  popts.flib_def = rcg->client_graph()->flib_def.get();
  popts.control_flow_added = false;

  popts.new_name = [this](const string& prefix) {
    mutex_lock l(mu_);
    return strings::StrCat(prefix, "_S", next_node_id_++);
  };

  popts.get_incarnation = [this](const string& name) -> int64 {
    auto d = devices_->FindDeviceByName(name);
    return d->attributes().incarnation();
  };

  TF_RETURN_IF_ERROR(rcg->RegisterPartitions(popts));
  return Status::OK();
}
\end{c++}
\end{leftbar}

\subsubsection{ReffedClientGraph::RegisterPartitions}

\begin{leftbar}
\begin{c++}
Status ReffedClientGraph::RegisterPartitions(
    const PartitionOptions& popts) {
  { 
    mu_.lock();
    if (!init_started_) {
      init_started_ = true;
      mu_.unlock();

      std::unordered_map<string, GraphDef> graph_defs;
      Status s = DoBuildPartitions(popts, &graph_defs);
      if (s.ok()) {
        s = DoRegisterPartitions(popts, std::move(graph_defs));
      }

      mu_.lock();
      init_result_ = s;
      init_done_.Notify();
    } else {
      mu_.unlock();
      init_done_.WaitForNotification();
      mu_.lock();
    }
    Status result = init_result_;
    mu_.unlock();
    return result;
  }
}
\end{c++}
\end{leftbar}

\subsection{グラフ分割：SplitByWorker}

\subsubsection{ReffedClientGraph::DoBuildPartitions}

\begin{leftbar}
\begin{c++}
Status MasterSession::ReffedClientGraph::DoBuildPartitions(
    PartitionOptions popts,
    std::unordered_map<string, GraphDef>* out_partitions) {
  // split full graph by worker name.
  return Partition(popts, &client_graph_->graph, out_partitions);
}
\end{c++}
\end{leftbar}

\subsection{グラフの登録}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/dist-run-step-stage-2.png}
\caption{RegisterGraph}
 \label{fig:dist-run-step-stage-2}
\end{figure}

\subsubsection{ReffedClientGraph::DoRegisterPartitions}

\begin{leftbar}
\begin{c++}
Status ReffedClientGraph::DoRegisterPartitions(
    const PartitionOptions& popts,
    std::unordered_map<string, GraphDef> graph_partitions) {
  partitions_.reserve(graph_partitions.size());
  Status s;
  for (auto& name_def : graph_partitions) {
    partitions_.resize(partitions_.size() + 1);
    Part* part = &partitions_.back();
    part->name = name_def.first;
    TrackFeedsAndFetches(part, name_def.second, popts);
    part->worker = worker_cache_->CreateWorker(part->name);
  }

  struct Call {
    RegisterGraphRequest req;
    RegisterGraphResponse resp;
    Status status;
  };

  const int num = partitions_.size();
  gtl::InlinedVector<Call, 4> calls(num);

  BlockingCounter done(num);
  for (int i = 0; i < num; ++i) {
    const Part& part = partitions_[i];
    Call* c = &calls[i];
    
    c->req.set_session_handle(session_handle_);
    c->req.mutable_graph_def()->Swap(&graph_partitions[part.name]);
    *c->req.mutable_graph_options() = session_opts_.config.graph_options();
    *c->req.mutable_debug_options() = debug_opts_;

    auto cb = [c, &done](const Status& s) {
      c->status = s;
      done.DecrementCount();
    };
    part.worker->RegisterGraphAsync(&c->req, &c->resp, cb);
  }
  done.Wait();

  for (int i = 0; i < num; ++i) {
    Call* c = &calls[i];
    s.Update(c->status);
    partitions_[i].graph_handle = c->resp.graph_handle();
  }
  return s;
}
\end{c++}
\end{leftbar}

\subsubsection{GrpcRemoteWorker::RegisterGraphAsync}

\begin{leftbar}
\begin{c++}
class GrpcRemoteWorker : public WorkerInterface {
  void RegisterGraphAsync(const RegisterGraphRequest* request,
                          RegisterGraphResponse* response,
                          StatusCallback done) override {
    IssueRequest(request, response, registergraph_, std::move(done));
  }

  void IssueRequest(const protobuf::Message* request,
                    protobuf::Message* response, const ::grpc::string& method,
                    StatusCallback done, CallOptions* call_opts = nullptr) {
    new RPCState<protobuf::Message>(counter_, &stub_, cq_, method, *request,
                                    response, std::move(done), call_opts);
  }
};
\end{c++}
\end{leftbar}

\subsubsection{GrpcWorkerService::RegisterGraphHandler}

\begin{leftbar}
\begin{c++}
class GrpcWorkerService : public AsyncServiceInterface {
  void RegisterGraphHandler(
      WorkerCall<RegisterGraphRequest, RegisterGraphResponse>* call) {
    Schedule([this, call]() {
      Status s = worker_->RegisterGraph(&call->request, &call->response);
      call->SendResponse(ToGrpcStatus(s));
    });
    ENQUEUE_REQUEST(RegisterGraph, false);
  }
};
\end{c++}
\end{leftbar}

\subsubsection{Worker::RegisterGraphAsync}

\begin{leftbar}
\begin{c++}
void Worker::RegisterGraphAsync(
    const RegisterGraphRequest* request,
    RegisterGraphResponse* response,
    StatusCallback done) {
  auto session = FindWorkerSession(request);
  Status s = session->graph_mgr->Register(
      request->session_handle(), 
      request->graph_def(), 
      request->graph_options(),
      response->mutable_graph_handle());
  done(s);
}
\end{c++}
\end{leftbar}

\subsubsection{GraphMgr::Register}

\begin{leftbar}
\begin{c++}
Status GraphMgr::Register(
    const string& session, 
    const GraphDef& gdef,
    const GraphOptions& graph_options,
    string* handle) {
  Item* item = new Item;
  Status s = InitItem(session, gdef, graph_options, item);
  if (!s.ok()) {
    item->Unref();
    return s;
  }

  // Generate unique graph\_handle, 
  // and register [graph\_handle, graph\_def] to table.
  {
    mutex_lock l(mu_);
    *handle = strings::Printf("%016llx", ++next_id_);
    item->handle = *handle;
    CHECK(table_.insert({*handle, item}).second);
  }
  return Status::OK();
}
\end{c++}
\end{leftbar}

\subsection{グラフ分割：SplitByDevice}

\begin{leftbar}
\begin{c++}
Status GraphMgr::InitItem(
    const string& session, const GraphDef& gdef,
    const GraphOptions& graph_options,
    Item* item) {
  item->session = session;
  item->lib_def.reset(
      new FunctionLibraryDefinition(OpRegistry::Global(), gdef.library()));

  item->proc_flr.reset(new ProcessFunctionLibraryRuntime(
      device_mgr_, worker_env_->env, gdef.versions().producer(),
      item->lib_def.get(), graph_options.optimizer_options()));

  // 1. Constructs the full graph out of "gdef"
  Graph graph(OpRegistry::Global());
  GraphConstructorOptions opts;
  opts.allow_internal_ops = true;
  opts.expect_device_spec = true;
  TF_RETURN_IF_ERROR(ConvertGraphDefToGraph(opts, gdef, &graph));

  // 2. Splits "graph" into multiple subgraphs by device names.
  std::unordered_map<string, GraphDef> partitions;
  PartitionOptions popts;
  popts.node_to_loc = SplitByDevice;  // IMPORTANT.
  popts.new_name = [this](const string& prefix) {
    mutex_lock l(mu_);
    return strings::StrCat(prefix, "_G", next_id_++);
  };
  popts.get_incarnation = [this](const string& name) -> int64 {
    Device* device = nullptr;
    Status s = device_mgr_->LookupDevice(name, &device);
    if (s.ok()) {
      return device->attributes().incarnation();
    } else {
      return PartitionOptions::kIllegalIncarnation;
    }
  };
  popts.flib_def = &graph.flib_def();
  popts.control_flow_added = true;
  popts.scheduling_for_recvs = graph_options.enable_recv_scheduling();
  
  // IMPORTANT.  
  TF_RETURN_IF_ERROR(Partition(popts, &graph, &partitions));

  // 3. convert GraphDef partitions to Graph partitions.
  std::unordered_map<string, std::unique_ptr<Graph>> partition_graphs;
  for (const auto& partition : partitions) {
    std::unique_ptr<Graph> device_graph(new Graph(OpRegistry::Global()));
    GraphConstructorOptions device_opts;
    // There are internal operations (e.g., send/recv) that we now allow.
    device_opts.allow_internal_ops = true;
    device_opts.expect_device_spec = true;
    TF_RETURN_IF_ERROR(ConvertGraphDefToGraph(device_opts, partition.second,
                                              device_graph.get()));
    partition_graphs.emplace(partition.first, std::move(device_graph));
  }

  // 4. Build executors\_and\_partitions(item->units) = [(e0, p0), 
  // (e1, p1), ...], and (e\_n, p\_n) is called ExecutionUnit.
  LocalExecutorParams params;
  item->units.reserve(partitions.size());
  item->graph_mgr = this;

  for (auto& p : partition_graphs) {
    const string& device_name = p.first;
    std::unique_ptr<Graph>& subgraph = p.second;
    item->units.resize(item->units.size() + 1);
    ExecutionUnit* unit = &(item->units.back());

    // Construct the root executor for the subgraph.
    params.device = unit->device;
    params.function_library = lib;
    params.create_kernel = [session, lib, opseg](
        const NodeDef& ndef, OpKernel** kernel) {
      // Caches the kernel only if the node is stateful.
      if (!lib->IsStateful(ndef.op())) {
        return lib->CreateKernel(ndef, kernel);
      }
      auto create_fn = [lib, &ndef](OpKernel** kernel) {
        return lib->CreateKernel(ndef, kernel);
      };
      // Kernels created for subgraph nodes need to be cached.  On
      // cache miss, create\_fn() is invoked to create a kernel based
      // on the function library here + global op registry.
      return opseg->FindOrCreate(session, ndef.name(), kernel, create_fn);
    };

    params.delete_kernel = [lib](OpKernel* kernel) {
      // If the node is stateful, opseg owns it. Otherwise, delete it.
      if (kernel && !lib->IsStateful(kernel->type_string())) {
        delete kernel;
      }
    };

    unit->graph = subgraph.get();
    TF_RETURN_IF_ERROR(
        NewLocalExecutor(params, subgraph.release(), &unit->root));
  }
  return Status::OK();
}
\end{c++}
\end{leftbar}

\subsection{グラフの実行}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/dist-run-step-stage-3.png}
\caption{RunGraph}
 \label{fig:dist-run-step-stage-3}
\end{figure}

\subsubsection{ReffedClientGraph::RunPartitions}

\begin{leftbar}
\begin{c++}
Status MasterSession::ReffedClientGraph::RunPartitions(
    const MasterEnv* env, int64 step_id, int64 execution_count,
    PerStepState* pss, CallOptions* call_opts, const RunStepRequestWrapper& req,
    MutableRunStepResponseWrapper* resp, CancellationManager* cm,
    const bool is_last_partial_run) {


  // 1. Prepares a number of calls to workers. 
  //    One call per partition.
  const int num = partitions_.size();
  RunManyGraphs calls(num);

  for (int i = 0; i < num; ++i) {
    const Part& part = partitions_[i];
    RunManyGraphs::Call* c = calls.get(i);
    c->req.reset(part.worker->CreateRunGraphRequest());
    c->resp.reset(part.worker->CreateRunGraphResponse());
    if (is_partial_) {
      c->req->set_is_partial(is_partial_);
      c->req->set_is_last_partial_run(is_last_partial_run);
    }
    c->req->set_session_handle(session_handle_);
    c->req->set_graph_handle(part.graph_handle);
    c->req->set_step_id(step_id);
    
    for (const auto& feed_key : part.feed_key) {
      const string& feed = feed_key.first;
      const string& key = feed_key.second;
      const int64 feed_index = feeds[feed];
      TF_RETURN_IF_ERROR(
          c->req->AddSendFromRunStepRequest(req, feed_index, key));
    }

    for (const auto& key_fetch : part.key_fetch) {
      const string& key = key_fetch.first;
      c->req->add_recv_key(key);
    }
  }

  // 2. Issues RunGraph calls.
  for (int i = 0; i < num; ++i) {
    const Part& part = partitions_[i];
    RunManyGraphs::Call* call = calls.get(i);
    part.worker->RunGraphAsync(
        &call->opts, call->req.get(), call->resp.get(),
        std::bind(&RunManyGraphs::WhenDone, &calls, i, std::placeholders::_1));
  }

  // 3. Waits for the RunGraph calls.
  call_opts->SetCancelCallback([&calls]() { calls.StartCancel(); });
  auto token = cm->get_cancellation_token();
  bool success =
      cm->RegisterCallback(token, [&calls]() { calls.StartCancel(); });
  if (!success) {
    calls.StartCancel();
  }

  calls.Wait();

  call_opts->ClearCancelCallback();
  if (success) {
    cm->DeregisterCallback(token);
  } else {
    return errors::Cancelled("Step was cancelled");
  }

  // 4. Collects fetches.
  Status status = calls.status();
  if (status.ok()) {
    for (int i = 0; i < num; ++i) {
      const Part& part = partitions_[i];
      MutableRunGraphResponseWrapper* run_graph_resp = calls.get(i)->resp.get();
      for (size_t j = 0; j < run_graph_resp->num_recvs(); ++j) {
        auto iter = part.key_fetch.find(run_graph_resp->recv_key(j));
        if (iter == part.key_fetch.end()) {
          status.Update(errors::Internal("Unexpected fetch key: ",
                                         run_graph_resp->recv_key(j)));
          break;
        }
        const string& fetch = iter->second;
        status.Update(
            resp->AddTensorFromRunGraphResponse(fetch, run_graph_resp, j));
        if (!status.ok()) {
          break;
        }
      }
    }
  }
  return status;
}
\end{c++}
\end{leftbar}

\subsubsection{GrpcRemoteWorker::RunGraphAsync}

\begin{leftbar}
\begin{c++}
struct GrpcRemoteWorker : public WorkerInterface {
  void RunGraphAsync(
      CallOptions* call_opts, 
      RunGraphRequestWrapper* request,
      MutableRunGraphResponseWrapper* response,
      StatusCallback done) override {
    IssueRequest(&request->ToProto(), 
        get_proto_from_wrapper(response),
        rungraph_, std::move(done), call_opts);
  }
};
\end{c++}
\end{leftbar}


\subsubsection{GrpcWorkerService::RunGraphHandler}

\begin{leftbar}
\begin{c++}
struct GrpcWorkerService : AsyncServiceInterface {
  void RunGraphHandler(WorkerCall<RunGraphRequest, RunGraphResponse>* call) {
    Schedule([this, call]() {
      auto wrapped_req = new ProtoRunGraphRequest(&call->request);
      auto wrapped_rsp = new NonOwnedProtoRunGraphResponse(&call->response);
      
      auto call_opts = new CallOptions;
      call->SetCancelCallback([call_opts]() { 
          call_opts->StartCancel(); 
      });

      worker_->RunGraphAsync(call_opts, wrapped_req, wrapped_rsp, 
        [call, call_opts, wrapped_req, wrapped_rsp](const Status& s) {
            call->ClearCancelCallback();
            delete call_opts;
            delete wrapped_req;
            delete wrapped_rsp;
            call->SendResponse(ToGrpcStatus(s));
        });
    });
    ENQUEUE_REQUEST(RunGraph, true);
  }
};
\end{c++}
\end{leftbar}

\subsubsection{Worker::RunGraphAsync}

\begin{leftbar}
\begin{c++}
void Worker::RunGraphAsync(
    CallOptions* opts, 
    RunGraphRequestWrapper* request,
    MutableRunGraphResponseWrapper* response,
    StatusCallback done) {
  if (request->is_partial()) {
    DoPartialRunGraph(opts, request, response, std::move(done));
  } else {
    DoRunGraph(opts, request, response, std::move(done));
  }
}
\end{c++}
\end{leftbar}

\begin{leftbar}
\begin{c++}
void Worker::DoRunGraph(
    CallOptions* opts, 
    RunGraphRequestWrapper* request,
    MutableRunGraphResponseWrapper* response,
    StatusCallback done) {
  const int64 step_id = request->step_id();

  // 1. Prepare inputs and outputs.
  GraphMgr::NamedTensors in;
  GraphMgr::NamedTensors* out = new GraphMgr::NamedTensors;
  Status s = PrepareRunGraph(request, &in, out);
  if (!s.ok()) {
    delete out;
    done(s);
    return;
  }
  
  // 2. Register Cancellation callback.
  CancellationManager* cm = new CancellationManager;
  opts->SetCancelCallback([this, cm, step_id]() {
    cm->StartCancel();
    AbortStep(step_id);
  });

  CancellationToken token;
  {
    mutex_lock l(mu_);
    token = cancellation_manager_->get_cancellation_token();
    bool already_cancelled = !cancellation_manager_->RegisterCallback(
        token, [cm]() { cm->StartCancel(); });
    if (already_cancelled) {
      opts->ClearCancelCallback();
      delete cm;
      delete out;
      done(errors::Aborted("Call was aborted"));
      return;
    }
  }

  // 3. Start Execution.
  auto session =
      FindWorkerSession(request);

  session->graph_mgr->ExecuteAsync(
      request->graph_handle(), step_id, session, 
      request->exec_opts(), response, cm, in,
      [ this, step_id, response, session, cm, 
        out, token, opts, done](Status s) {
        
        // 4. Receive output tensors from grpc remote rendezvous.
        if (s.ok()) {
          s = session->graph_mgr->RecvOutputs(step_id, out);
        }

        // 5. Unregister Cancellation callback
        opts->ClearCancelCallback();
        {
          mutex_lock l(mu_);
          cancellation_manager_->DeregisterCallback(token);
        }
        delete cm;

        // 6. Save to RunStepResponse.
        if (s.ok()) {
          for (const auto& p : *out) {
            const string& key = p.first;
            const Tensor& val = p.second;
            response->AddRecv(key, val);
          }
        }
        delete out;
        done(s);
      });
}
\end{c++}
\end{leftbar}

\subsubsection{GraphMgr}

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{figures/dist-run-step-overview.png}
\caption{Worker: RunStepの形式化}
 \label{fig:dist-run-step-overview}
\end{figure}

\begin{leftbar}
\begin{c++}
void GraphMgr::ExecuteAsync(
    const string& handle, const int64 step_id,
    WorkerSession* session, const ExecutorOpts& opts,
    MutableRunGraphResponseWrapper* response,
    CancellationManager* cancellation_manager,
    const NamedTensors& in, StatusCallback done) {
  // 1. Lookup an item. Holds one ref while executing.
  //    One item per registered graph.
  Item* item = nullptr;
  {
    mutex_lock l(mu_);
    auto iter = table_.find(handle);
    if (iter != table_.end()) {
      item = iter->second;
      item->Ref();
    }
  }

  RemoteRendezvous* rendezvous = worker_env_->rendezvous_mgr->Find(step_id);
  Status s = rendezvous->Initialize(session);

  // 2. Sends inputs to rendezvous.
  if (s.ok()) {
    s = SendInputsToRendezvous(rendezvous, in);
  }

  // 3. Start parallel executors.
  StartParallelExecutors(
      handle, step_id, item, rendezvous, collector,
      cost_graph, cancellation_manager,
      [this, item, rendezvous, done](const Status& s) {
          // 4. Recvs outputs from rendezvous.
          done(s);
          rendezvous->Unref();
          item->Unref();
      });
}
\end{c++}
\end{leftbar}

\begin{leftbar}
\begin{c++}
Status GraphMgr::SendInputsToRendezvous(
    Rendezvous* rendezvous, const NamedTensors& in) {
  Rendezvous::ParsedKey parsed;
  for (auto& p : in) {
    auto& key = p.first;
    auto& val = p.second;

    Status s = Rendezvous::ParseKey(key, &parsed);
    if (s.ok()) {
      s = rendezvous->Send(parsed, Rendezvous::Args(), val, false);
    }
    if (!s.ok()) {
      return s;
    }
  }
  return Status::OK();
}
\end{c++}
\end{leftbar}

\begin{leftbar}
\begin{c++}
void GraphMgr::StartParallelExecutors(
    const string& handle, int64 step_id,
    Item* item, Rendezvous* rendezvous,
    StepStatsCollector* collector,
    CancellationManager* cancellation_manager,
    StatusCallback done) {
  
  // 1. Wait until pending == 0, with default is num\_units,
  // `pending -= 1` when one partition graph is done. 
  int num_units = item->units.size();
  ExecutorBarrier* barrier =
      new ExecutorBarrier(
          num_units, rendezvous, [done](const Status& s) {
              done(s);
          });

  Executor::Args args;
  {
    mutex_lock l(mu_);
    args.step_id = ++next_id_;
  }
  args.rendezvous = rendezvous;
  args.cancellation_manager = cancellation_manager;
  args.stats_collector = collector;
  args.step_container = step_container;
  args.sync_on_finish = sync_on_finish_;

  using std::placeholders::_1;
  args.runner = std::bind(
      &thread::ThreadPool::Schedule, 
      worker_env_->compute_pool, _1);

  2. Broadcast all partitions to run
  for (const auto& unit : item->units) {
    unit.root->RunAsync(args, barrier->Get());
  }
}
\end{c++}
\end{leftbar}

\begin{leftbar}
\begin{c++}
Status GraphMgr::RecvOutputsFromRendezvous(
    Rendezvous* rendezvous, NamedTensors* out) {
  // Receives values requested by the caller.
  Rendezvous::ParsedKey parsed;
  for (auto& p : *out) {
    auto& key = p.first;
    auto& val = p.second;

    bool is_dead = false;
    Status s = Rendezvous::ParseKey(key, &parsed);
    if (s.ok()) {
      s = rendezvous->Recv(parsed, Rendezvous::Args(), &val, &is_dead);
    }

    if (is_dead) {
      s = errors::InvalidArgument("The tensor returned for ", key,
                                  " was not valid.");
    }

    if (!s.ok()) {
      return s;
    }
  }
  return Status::OK();
}

Status GraphMgr::RecvOutputs(int64 step_id, NamedTensors* out) {
  Rendezvous* rendezvous = worker_env_->rendezvous_mgr->Find(step_id);
  Status s = RecvOutputsFromRendezvous(rendezvous, out);
  rendezvous->Unref();
  return s;
}
\end{c++}
\end{leftbar}

\subsection{Rendezvous}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/rendezvous-hierarchy.png}
\caption{Rendezvous階層構造}
 \label{fig:rendezvous-hierarchy}
\end{figure}

\subsubsection{多態的作成}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/rendezvous-remote-mgr.png}
\caption{RemoteRendezvousの多態的作成}
 \label{fig:rendezvous-remote-mgr}
\end{figure}

\subsubsection{送信}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/rendzvous-send.png}
\caption{Rendezvousの送信}
 \label{fig:rendzvous-send}
\end{figure}

\subsubsection{受信}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/rendezvous-recv-case-1.png}
\caption{Rendezvousの受信：分散送信側と受信側が同じWorker内にある場合}
 \label{fig:rendezvous-recv-case-1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{figures/rendezvous-recv-case-2.png}
\caption{Rendezvousの受信：分散送信側と受信側が同じWorker内にない場合}
 \label{fig:rendezvous-recv-case-2}
\end{figure}

\subsection{グラフの登録解除}

\end{content}

\section{セッションの終了}

\begin{content}

\subsubsection{GrpcSession}

\begin{leftbar}
\begin{c++}
Status GrpcSession::Close() {
  CloseSessionRequest req;
  {
    mutex_lock l(mu_);
    if (handle_.empty()) {
      return errors::InvalidArgument("A session is not created yet....");
    }
    req.set_session_handle(handle_);
    handle_.clear();
  }
  CloseSessionResponse resp;
  CallOptions call_options;
  call_options.SetTimeout(options_.config.operation_timeout_in_ms());
  return master_->CloseSession(&call_options, &req, &resp);
}
\end{c++}
\end{leftbar}

\subsubsection{GrpcRemoteMaster}

\begin{leftbar}
\begin{c++}
struct GrpcRemoteMaster : MasterInterface {
  Status CloseSession(CallOptions* call_options,
                      const CloseSessionRequest* request,
                      CloseSessionResponse* response) override {
    ::grpc::ClientContext ctx;
    ctx.set_fail_fast(false);
    SetDeadline(&ctx, call_options->GetTimeout());
    return FromGrpcStatus(stub_->CloseSession(&ctx, *request, response));
  }
};
\end{c++}
\end{leftbar}

\subsubsection{GrpcMasterService}

\begin{leftbar}
\begin{c++}
struct GrpcMasterService : AsyncServiceInterface {
  void CloseSessionHandler(
      MasterCall<CloseSessionRequest, CloseSessionResponse>* call) {
    master_impl_->CloseSession(&call->request, &call->response,
                               [call](const Status& status) {
                                 call->SendResponse(ToGrpcStatus(status));
                               });
    ENQUEUE_REQUEST(CloseSession, false);
  }
};
\end{c++}
\end{leftbar}

\subsubsection{Master}

\begin{leftbar}
\begin{c++}
void Master::CloseSession(const CloseSessionRequest* req,
                          CloseSessionResponse* resp, MyClosure done) {
  MasterSession* session = nullptr;
  {
    mu_.lock();
    auto iter = sessions_.find(req->session_handle());
    if (iter == sessions_.end()) {
      mu_.unlock();
      done(errors::Aborted(
          "Session ", req->session_handle(),
          " is not found. Possibly, this master has restarted."));
      return;
    }
    session = iter->second;
    sessions_.erase(iter);
    mu_.unlock();
  }

  // Session Close() blocks on thread shutdown. Therefore, we need to
  // delete it in non-critical thread.
  SchedClosure([session, done]() {
    Status s = session->Close();
    session->Unref();
    done(s);
  });
}
\end{c++}
\end{leftbar}

\subsubsection{MasterSession}

\begin{leftbar}
\begin{c++}
Status MasterSession::Close() {
  {
    mutex_lock l(mu_);
    closed_ = true;  // All subsequent calls to Run() or Extend() will fail.
  }
  cancellation_manager_.StartCancel();
  std::vector<ReffedClientGraph*> to_unref;
  {
    mutex_lock l(mu_);
    while (num_running_ != 0) {
      num_running_is_zero_.wait(l);
    }
    ClearRunsTable(&to_unref, &run_graphs_);
    ClearRunsTable(&to_unref, &partial_run_graphs_);
  }
  for (ReffedClientGraph* rcg : to_unref) rcg->Unref();
  return Status::OK();
}
\end{c++}
\end{leftbar}

\subsubsection{ReffedClientGraph}

\begin{leftbar}
\begin{c++}
ReffedClientGraph::~ReffedClientGraph() { 
  DeregisterPartitions(); 
}
\end{c++}
\end{leftbar}

\begin{leftbar}
\begin{c++}
void ReffedClientGraph::DeregisterPartitions() {
  struct Call {
    DeregisterGraphRequest req;
    DeregisterGraphResponse resp;
  };
  for (Part& part : partitions_) {
    if (!part.graph_handle.empty()) {
      Call* c = new Call;
      c->req.set_session_handle(session_handle_);
      c->req.set_graph_handle(part.graph_handle);

      WorkerCacheInterface* worker_cache = worker_cache_;
      const string name = part.name;
      WorkerInterface* w = part.worker;

      auto cb = [worker_cache, c, name, w](const Status& s) {
        if (!s.ok()) {
          // This error is potentially benign, so we don't log at the
          // error level.
          LOG(INFO) << "DeregisterGraph error: " << s;
        }
        delete c;
        worker_cache->ReleaseWorker(name, w);
      };
      w->DeregisterGraphAsync(&c->req, &c->resp, cb);
    }
  }
}
\end{c++}
\end{leftbar}

\subsubsection{GrpcWorkerService}

\begin{leftbar}
\begin{c++}
struct GrpcWorkerService : AsyncServiceInterface {
  void CreateWorkerSessionHandler(
      WorkerCall<CreateWorkerSessionRequest, CreateWorkerSessionResponse>*
          call) {
    Schedule([this, call]() {
      Status s = worker_->CreateWorkerSession(&call->request, &call->response);
      call->SendResponse(ToGrpcStatus(s));
    });
    ENQUEUE_REQUEST(CreateWorkerSession, false);
  }
};
\end{c++}
\end{leftbar}

\subsubsection{Worker}

\begin{leftbar}
\begin{c++}
void Worker::DeregisterGraphAsync(const DeregisterGraphRequest* request,
                                  DeregisterGraphResponse* response,
                                  StatusCallback done) {
  WorkerSession* session =
      env_->session_mgr->WorkerSessionForSession(request->session_handle());
  Status s = session->graph_mgr->Deregister(request->graph_handle());

  done(s);
}
\end{c++}
\end{leftbar}

\subsubsection{GraphMgr}

\begin{leftbar}
\begin{c++}
Status GraphMgr::Deregister(const string& handle) {
  Item* item = nullptr;
  {
    mutex_lock l(mu_);
    auto iter = table_.find(handle);
    if (iter == table_.end()) {
      return errors::Aborted("Graph handle is not found: ", handle,
                             ". Possibly, this worker just restarted.");
    }
    item = iter->second;
    table_.erase(iter);
  }
  item->Unref();
  return Status::OK();
}
\end{c++}
\end{leftbar}

\begin{leftbar}
\begin{c++}
GraphMgr::Item::~Item() {
  for (const auto& unit : this->units) {
    delete unit.root;
    unit.device->op_segment()->RemoveHold(this->session);
  }
}
\end{c++}
\end{leftbar}

\end{content}
