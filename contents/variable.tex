\begin{savequote}[45mm]
\ascii{Any fool can write code that a computer can understand. Good programmers write code that humans can understand.}
\qauthor{\ascii{- Martin Flower}}
\end{savequote}

\chapter{変数} 
\label{ch:variable}

\begin{content}

\ascii{Variable}は特別な\ascii{OP}であり、状態\ascii{(Stateful)}を持っています。実装技術の観点から見ると、\ascii{Variable}の\ascii{Kernel}実装は直接\ascii{Tensor}インスタンスを保持し、その寿命は変数と一致します。通常の\ascii{Tensor}インスタンスの寿命が現在の反復\ascii{(Step)}でのみ有効であるのに対し、\ascii{Variable}は複数の反復にわたって有効であり、ファイルシステムに保存したり、ファイルシステムから復元したりすることさえできます。

\end{content}

\section{実践：線形モデル}

\begin{content}

簡単な線形モデルを例にとります（問題を簡略化するため、ここでは訓練サブグラフを省略しています）。まず、\code{tf.placeholder}を使用してモデルの入力を定義し、次に2つのグローバル変数を定義します。これらは両方とも訓練パラメータであり、最後に簡単な線形モデルを定義します。

\begin{leftbar}
\begin{python}
x  = tf.placeholder(tf.float32, [None, 784])
W = tf.Variable(tf.zeros([784,10]), name='W')
b = tf.Variable(tf.zeros([10]), name='b') 
y = tf.matmul(x, W) + b
\end{python}
\end{leftbar}

変数を使用する前に、変数を初期化する必要があります。慣例に従い、\code{tf.global\_variables}
\code{\_initializer()}を使用してすべてのグローバル変数の初期化子を集約し、初期化を行います。

\begin{leftbar}
\begin{python}
init = tf.global_variables_initializer()

with tf.Session() as sess:
  sess.run(init)
\end{python}
\end{leftbar}

既存の経験によると、その計算グラフは大まかに\refig{tf-linear-model}のようになります。

\begin{figure}[!h]
\centering
\includegraphics[width=0.6\textwidth]{figures/tf-linear-model.png}
\caption{計算グラフ: 線形重み付き和}
 \label{fig:tf-linear-model}
\end{figure}

実際には、\refig{tf-real-linear-model}に示すように、実際の計算グラフははるかに複雑です。最初から説明しましょう。

\begin{figure}[!h]
\centering
\includegraphics[width=1.0\textwidth]{figures/tf-real-linear-model.png}
\caption{計算グラフ: 線形重み付き和}
 \label{fig:tf-real-linear-model}
\end{figure}

\end{content}

\section{モデルの初期化}

\begin{content}

\ascii{Variable}は特別な\ascii{OP}であり、状態\ascii{(Stateful)}を持っています。実装技術の観点から見ると、\ascii{Variable}の\ascii{Kernel}実装は直接\ascii{Tensor}インスタンスを保持し、その寿命は\ascii{Variable}と一致します。通常の\ascii{Tensor}インスタンスの寿命が現在の反復\ascii{(Step)}でのみ有効であるのに対し、\ascii{Variable}は複数の反復\ascii{(Step)}にわたって有効であり、ファイルシステムに永続化したり、ファイルシステムから復元したりすることさえできます。

\subsection{変数の操作}

変数の値を変更するための特別な\ascii{OP}がいくつか存在します。例えば、\ascii{Assign, AssignAdd}などです。\ascii{Variable}が保持する\ascii{Tensor}は参照としてA\ascii{ssign}に入力され、\ascii{Assign}は初期値\ascii{(Initial Value)}または新しい値に基づいて\ascii{Tensor}内部の値をその場で修正し、最後にその\ascii{Tensor}を参照として出力します。

設計の観点から見ると、\ascii{Variable}は\ascii{Tensor}のラッパーと見なすことができ、\ascii{Tensor}がサポートするすべての操作が\ascii{Variable}によってオーバーロードされて実装されています。つまり、\ascii{Variable}は\ascii{Tensor}のあらゆる場所に現れることができます。例えば、

\begin{leftbar}
\begin{python}
# Create a variable
W = tf.Variable(tf.zeros([784,10]), name='W')

# Use the variable in the graph like any Tensor.
y = tf.matmul(x, W)

# The overloaded operators are available too.
z = tf.sigmoid(w + y)

# Assign a new value to the variable with assign/assign\_add.
w.assign(w + 1.0)
w.assign_add(1.0)
\end{python}
\end{leftbar}

\subsection{初期値}

一般的に、変数を使用する前に、変数を初期化する必要があります。実際、\ascii{TensorFlow}は巧妙な変数初期化モデルを設計しています。\ascii{Variable}は初期値\ascii{(Initial Value)}に基づいて\ascii{Variable}のデータ型を推論し、\ascii{Tensor}の形状\ascii{(Shape)}を決定します。

例えば、\code{tf.zeros}は\ascii{Variable}の初期値と呼ばれ、\ascii{Variable}の型が\code{int32}で、\ascii{Shape}が\code{[784, 10]}であることを決定します。

\begin{leftbar}
\begin{python}
# Create a variable.
W = tf.Variable(tf.zeros([784,10]), name='W')
\end{python}
\end{leftbar}

以下の表に示すように、変数の初期値を構築する一般的な\ascii{OP}には以下のようなものがあります：

\subsection{初期化子}

また、変数は初期化子\ascii{(Initializer)}を通じて、初期化期間中に初期化値を\ascii{Variable}内部が保持する\ascii{Tensor}に割り当て、\ascii{Variable}のその場での修正を完了します。

変数を使用する前に、変数が初期化子によって初期化されていることを確認する必要があります。実際、変数の初期化プロセスは、変数の初期化子を実行することです。

上記の例の\code{W}の定義のように、以下のように\code{W}の初期化を完了することができます。ここで、\code{W.initializer}は実際には\ascii{Assign}の\ascii{OP}であり、これは\ascii{Variable}のデフォルトの初期化子です。

\begin{leftbar}
\begin{python}
# Run the variable initializer.
with tf.Session() as sess:
  sess.run(W.initializer)
\end{python}
\end{leftbar}

\ascii{Variable}の初期化が完了すると、その型と値が決定されます。その後、\ascii{Assign}族の\ascii{OP}（例えば\ascii{Assign, AssignAdd}など）を使用して\ascii{Variable}の値を修正することができます。

注意すべきは、\ascii{TensorBoard}で\ascii{Assign}の入力を表示する際、その辺は特別な\ascii{ref}識別子を使用することです。データの流れの方向はちょうど逆になっています。そうでなければ、計算グラフは必然的に循環を含むことになり、明らかに\ascii{DAG}（有向非巡回グラフ）の基本要件に違反します。

\subsection{スナップショット}

変数の値を読み取る場合は、\ascii{Identity}恒等変換を通じて、変数が保持する\ascii{Tensor}を直接出力します。\ascii{Identity}は\ascii{Variable}の参照識別子を削除すると同時に、メモリのコピーも回避します。

\ascii{Identity}による\ascii{Variable}の操作は、通常スナップショット\ascii{(Snapshot)}と呼ばれ、\ascii{Variable}の現在の値を表します。

実際、\ascii{Identity}を通じて\ascii{Variable}を通常の\ascii{Tensor}に変換することで、すべての\ascii{Tensor}操作と互換性を持たせることができます。

\subsection{変数サブグラフ}

例えば、変数\ascii{W}の定義は以下の通りです。

\begin{leftbar}
\begin{python}
W = tf.Variable(tf.zeros([784,10]), name='W')
\end{python}
\end{leftbar}

\code{tf.zeros([784,10])}は初期値と呼ばれ、初期化子\ascii{Assign}を通じて、\code{W}内部が保持する\ascii{Tensor}を参照の形でその場で修正して初期値にします。同時に、\ascii{Identity}が\ascii{Variable}の参照識別子を削除し、\ascii{Variable}の読み取りを実現します。

\begin{figure}[!h]
\centering
\includegraphics[width=0.8\textwidth]{figures/variable-initialization-model.png}
\caption{変数サブグラフ}
 \label{fig:variable-initialization-model}
\end{figure}

\subsection{初期化プロセス}

より一般的なのは、\code{tf.global\_variables\_initializer()}を呼び出して、すべての変数の初期化子を集約し、その後\ascii{Session}を起動してその\ascii{OP}を実行することです。

\begin{leftbar}
\begin{python}
init = tf.global_variables_initializer()
\end{python}
\end{leftbar}

実際、すべてのグローバル変数の初期化子を収集する\ascii{OP}は\ascii{NoOp}であり、つまり入力も出力も存在しません。すべての変数の初期化子は制御依存エッジを通じてこの\ascii{NoOp}に接続され、すべてのグローバル変数が初期化されることを保証します。

\begin{figure}[!h]
\centering
\includegraphics[width=0.8\textwidth]{figures/variable-initialization-no-op.png}
\caption{初期化OP}
 \label{fig:variable-initialization-no-op}
\end{figure}

\subsection{コロケーション関係}

コロケーション関係は特殊なデバイス制約関係です。明らかに、\ascii{Assign, Identity}これら2つの\ascii{OP}は\ascii{Variable}と非常に密接な関係にあり、それぞれ変数の修正と読み取りを実現します。したがって、これらは\ascii{Variable}と同じデバイス上で実行される必要があります。このような関係は、通常コロケーション関係\ascii{(Colocation)}と呼ばれます。

\ascii{Assign/Identity}ノード上で\code{\_class}属性値：\code{[s: "loc:@W"]}を指定することができます。これは、これら2つの\ascii{OP}が\code{W}と同じデバイス上で実行されることを示しています。

例えば、\code{W/read}ノードを例にとると、このノードには\code{\_class}属性が追加され、\code{W}とのコロケーション関係を示しています。

\begin{leftbar}
\begin{python}
node {
  name: "W/read"
  op: "Identity"
  input: "W"
  attr {
    key: "T"
    value {
      type: DT_FLOAT
    }
  }
  attr {
    key: "_class"
    value {
      list {
        s: "loc:@W"
      }
    }
  }
}
\end{python}
\end{leftbar}

\subsection{初期化依存関係}

ある変数の初期化が別の変数の初期値に依存している場合、特別な処理が必要です。例えば、変数\code{V}の初期値が\code{W}の初期値に依存している場合、\code{W.initialized\_value()}を通じて指定することができます。

\begin{leftbar}
\begin{python}
W = tf.Variable(tf.zeros([784,10]), name='W')
V = tf.Variable(W.initialized_value(), name='V')
\end{python}
\end{leftbar}

実際、両者は\ascii{Identity}を通じて接続され、明示的に依存制御エッジが追加され、\code{W}が\code{V}より先に初期化されることを保証します。ここでは、2つの\ascii{Identity}の\ascii{OP}が存在しますが、責任は異なり、それぞれ初期化依存関係と変数読み取りを完了します。

\begin{figure}[!h]
\centering
\includegraphics[width=0.8\textwidth]{figures/variable-initialization-dependency-1.png}
\caption{初期化依存関係}
 \label{fig:variable-initialization-dependency-1}
\end{figure}

同様に、\code{tf.global\_variables\_initializer()}を呼び出して、変数のすべての初期化子を集約し、その後\ascii{Session}を起動してすべての変数の初期化を完了することができます。

\begin{leftbar}
\begin{python}
init = tf.global_variables_initializer()
\end{python}
\end{leftbar}

依存関係に従って、\code{W/Assign}と\code{Identity}の間に制御依存エッジが追加されたため、巧妙に\code{W}が\code{V}より先に初期化を完了し、\code{W}の現在の初期化値を通じて、最終的に\code{V}の初期化を完了します。

\begin{figure}[!h]
\centering
\includegraphics[width=0.8\textwidth]{figures/variable-initialization-dependency-2.png}
\caption{初期化OP}
 \label{fig:variable-initialization-dependency-2}
\end{figure}

\subsection{初期化子リスト}

\code{variables\_initializer}を使用して、変数リストの初期化子リストを構築することができます。ここで、\code{group}は\code{\_initialier\_list()}にのみ制御依存する\ascii{NoOP}を構築します。

\begin{leftbar}
\begin{python}
def variables_initializer(var_list, name="init"):
  def _initialier_list():
    return *[v.initializer for v in var_list]
  return control_flow_ops.group(_initialier_list(), name=name)
\end{python}
\end{leftbar}

例えば、グローバル変数リストの初期化子リストは以下のように構築することができます。

\begin{leftbar}
\begin{python}
def global_variables_initializer():
  return variables_initializer(global_variables())
\end{python}
\end{leftbar}

\end{content}

\section{変数のグループ化}

\begin{content}

デフォルトで、\ascii{Variable}はグローバル変数と訓練変数のコレクションに分類されます。上記の例のように、\code{W, V}は自動的にグローバル変数と訓練変数のコレクションに分類されます。

\subsection{グローバル変数}

\code{tf.global\_variables()}を通じて、グローバル変数のコレクションを簡単に取得することができます。分散環境では、グローバル変数は異なるプロセス間でパラメータ共有を実現することができます。

\begin{leftbar}
\begin{python}
def global_variables():
  return ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES)
\end{python}
\end{leftbar}

\subsection{ローカル変数}

\code{tf.local\_variables()}を通じて、ローカル変数のコレクションを簡単に取得することができます。

\begin{leftbar}
\begin{python}
def local_variables():
  return ops.get_collection(ops.GraphKeys.LOCAL_VARIABLES)
\end{python}
\end{leftbar}

\code{local\_variable}の構文糖を使用して、ローカル変数を構築することができます。

\begin{leftbar}
\begin{python}
def local_variable(initial_value, validate_shape=True, name=None):
  return variables.Variable(
      initial_value, trainable=False,
      collections=[ops.GraphKeys.LOCAL_VARIABLES],
      validate_shape=validate_shape, name=name)
\end{python}
\end{leftbar}

ローカル変数はプロセス内の共有変数を表し、通常はチェックポイント\ascii{(Checkpoint)}を行う必要がなく、一時的なカウンターの用途にのみ使用されます。例えば、分散環境では、ローカル変数を使用してそのプロセスが既に読み込んだデータの\ascii{Epoch}数を記録します。

\subsection{訓練変数}

\code{tf.trainable\_variables()}を通じて、訓練変数のコレクションを取得することができます。機械学習では、訓練変数はモデルパラメータを表します。

\begin{leftbar}
\begin{python}
def trainable_variables():
  return ops.get_collection(ops.GraphKeys.TRAINABLE_VARIABLES)
\end{python}
\end{leftbar}

\subsection{global\_step}

\code{global\_step}は特別な\ascii{Variable}で、訓練変数ではありませんが、グローバル変数です。分散環境では、\code{global\_step}は通常、実行済みの\ascii{step}数を追跡し、異なるプロセス間でデータの同期を実現するために使用されます。

\code{global\_step}を作成するには、以下の関数を使用できます：

\begin{leftbar}
\begin{python}
def create_global_step(graph=None):
  graph = ops.get_default_graph() if graph is None else graph
  with graph.as_default() as g, g.name_scope(None):
    collections = [GLOBAL_VARIABLES, GLOBAL_STEP]
    return variable(
        GLOBAL_STEP,
        shape=[],
        dtype=dtypes.int64,
        initializer=init_ops.zeros_initializer(),
        trainable=False,
        collections=collections)
\end{python}
\end{leftbar}

\end{content}

\section{ソースコード分析：変数の構築}

\begin{content}

コード実装を簡略化するため、ここでは\ascii{Variable}を簡単に再構築しています。

\begin{leftbar}
\begin{python}
class Variable(object):
  def __init__(self, initial_value=None, trainable=True,
    collections=None, name=None, dtype=None):
    with ops.name_scope(name, "Variable", [initial_value]) as name:
      self._cons_initial_value(initial_value, dtype)
      self._cons_variable(name)
      self._cons_initializer()
      self._cons_snapshot()
    self._cons_collections(trainable, collections)
\end{python}
\end{leftbar}

\ascii{Variable}インスタンスの構築は、基本的に以下のステップを含みます：

\subsection{初期値の構築}

\begin{leftbar}
\begin{python}
  def _cons_initial_value(self, initial_value, dtype):
    self._initial_value = ops.convert_to_tensor(
        initial_value, name="initial_value", dtype=dtype)
\end{python}
\end{leftbar}

\subsection{変数OPの構築}

\ascii{Variable}は初期値の型とサイズに基づいて自動的に推論を行います。

\begin{leftbar}
\begin{python}
  def _cons_variable(self, name):
    self._variable = state_ops.variable_op_v2(
      self._initial_value.get_shape(),
      self._initial_value.dtype.base_dtype,
      name=name)
\end{python}
\end{leftbar}

\subsection{初期化子の構築}

\ascii{Variable}の初期化子は本質的に\ascii{Assign}であり、\ascii{Variable}の参照を保持し、初期値を使用して変数自体をその場で修正します。

\begin{leftbar}
\begin{python}
  def _cons_initializer(self):
    self._initializer_op = state_ops.assign(
      self._variable,
      self._initial_value).op
\end{python}
\end{leftbar}

\subsection{スナップショットの構築}

\ascii{Variable}のスナップショットは本質的に\ascii{Identity}であり、\ascii{Variable}の現在の値を表します。

\begin{leftbar}
\begin{python}
  def _cons_snapshot(self):
    with ops.colocate_with(self._variable.op):
      self._snapshot = array_ops.identity(
        self._variable, name="read")
\end{python}
\end{leftbar}

\subsection{変数のグループ化}

デフォルトで、\ascii{Variable}はグローバル変数のコレクションに分類されます。\code{trainable}が真の場合、その変数は訓練パラメータであることを示し、訓練変数のコレクションに分類されます。

\begin{leftbar}
\begin{python}
  def _cons_collections(self, trainable, collections)
    if collections is None:
      collections = [GLOBAL_VARIABLES]
    if trainable and TRAINABLE_VARIABLES not in collections:
      collections = list(collections) + [TRAINABLE_VARIABLES]
    ops.add_to_collections(collections, self)
\end{python}
\end{leftbar}

\end{content}
