\begin{savequote}[45mm]
\ascii{Any fool can write code that a computer can understand. Good programmers write code that humans can understand.}
\qauthor{\ascii{- Martin Flower}}
\end{savequote}

\chapter{はじめに} 
\label{ch:ice-breaker}

\begin{content}

\tf{}カーネルの探求を始める前に、モデルのトレーニングを実際に体験し、基本的なトレーニング方法とチューニング技術に慣れることは、以降の章の内容を理解する上で非常に有益です。この記事を通じて学習と実践を行うことで、手書き数字を認識できるニューラルネットワークの構築とトレーニング方法を理解することができます\footnote{本章の内容は\ascii{Martin G\"{o}rner}が\ascii{Codelabs}で発表した記事：\href{https://codelabs.developers.google.com/codelabs/cloud-tensorflow-mnist}{Tensorflow and deep learning, without a PhD}から抜粋されています。\ascii{Martin G\"{o}rner}の許可を得て、本書での掲載が承認されています。}。

本章では、段階的にシングルレイヤーパーセプトロンモデル、マルチレイヤーパーセプトロンモデルを使用し、最後に畳み込みニューラルネットワークを試みます。そして、トレーニング過程において、より良い活性化関数の選択、学習率減衰技術の適用、\ascii{Dropout}技術の実施など、アルゴリズムチューニングのいくつかの一般的な技術を紹介します。最終的に、モデルの精度を\ascii{99％}以上に向上させます。

各ネットワークモデルの紹介に先立ち、そのモデルのアルゴリズム理論知識を簡単に説明し、プログラムの内容をよりよく理解できるようにします。ただし、本書は機械学習アルゴリズムを紹介する専門書ではありません。関連するアルゴリズムの内容についてより詳しく知りたい場合は、関連文献や論文を参照してください。

\end{content}

\section{問題提起}

\begin{content}

本章では\ascii{MNIST}データセットを使用して手書き数字のネットワークモデルトレーニングを完了します。これには\ascii{60000}個のトレーニングサンプルデータが含まれています。そのうち、\ascii{10000}個のテストサンプルデータが含まれています。\refig{mnist-x}に示すように、任意のサンプルデータ$x$に対して、$28 \times 28$ピクセルの数字行列を使用して表現します。簡略化のため、$28 \times 28$の行列をフラット化処理し、長さ\ascii{784}の一次元ベクトルを得ます。

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/mnist-x.png}
\caption{MNISTサンプルデータ表現}
 \label{fig:mnist-x}
\end{figure}

\subsection{サンプルデータセット}

したがって、\ascii{MNIST}トレーニングデータセットでは、\code{mnist.train.images}は\code{[60000, 784]}の二次元行列です。この行列の各要素は、画像の特定のピクセルの強度値を表し、その値は\ascii{0}から\ascii{1}の間です。\refig{mnist-train-xs}に示すとおりです。

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/mnist-train-xs.png}
\caption{MNISTトレーニングデータセット：入力データセット}
 \label{fig:mnist-train-xs}
\end{figure}

それに対応して、\ascii{MNIST}データセットのラベルは\ascii{0}から\ascii{9}の数字で、\code{mnist.train.labels}は\code{[60000, 10]}の二次元行列で、各行は\ascii{\quo{one-hot}}ベクトルです。\refig{mnist-train-ys}に示すとおりです。

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/mnist-train-ys.png}
\caption{MNISTトレーニングデータセット：ラベルデータセット}
 \label{fig:mnist-train-ys}
\end{figure}

\subsection{図示説明}

\begin{content}

トレーニング全過程をより良く可視化するために、\ascii{matplotlib}ツールパッケージを使用して\ascii{5}種類のボードを描画しました。\refig{mnist-training-digits}に示すように、これは1つの\ascii{mini-batch}のトレーニングサンプルデータセットを表しています。ここで、\code{batch\_size = 100}、白い背景は数字が正しく認識されたことを示し、赤い背景は数字が誤分類されたことを示します。手書き数字の左側は正しいラベル値を示し、右側は誤った予測値を示しています。

\ascii{MNIST}は\ascii{50000}個のトレーニングサンプルを持っています。\code{batch\_size}が\ascii{100}の場合、トレーニングサンプルデータセット全体を一度完全に走査するには\ascii{500}回の反復が必要で、これは一般に1つの\ascii{epoch}周期と呼ばれます。

\begin{remark}
本章のサンプルコードでは\ascii{TensorBoard}を使用せず、代わりに\ascii{matplotlib}を使用しているため、トレーニング中にエラーと精度の曲線変化をリアルタイムで観察できます。
\end{remark}

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/mnist-training-digits.jpeg}
\caption{1回のmini-batchのトレーニングサンプルデータセット、ここで\code{batch\_size=100}}
 \label{fig:mnist-training-digits}
\end{figure}

\refig{mnist-test-digits}に示すように、\ascii{MNIST}は\ascii{10000}のスケールのテストサンプルデータセットを使用して、モデルの現在の精度をテストしています。左側は現在のモデルの大まかな精度を示しています。同様に、白い背景は数字が正しく認識されたことを示し、赤い背景は数字が誤分類されたことを示します。手書き数字の左側は正しいラベル値を示し、右側は誤った予測値を示しています。

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/mnist-test-digits.jpeg}
\caption{現在のモデル精度：テストサンプルデータセットに基づく}
 \label{fig:mnist-test-digits}
\end{figure}

\refig{mnist-cross-entropy-loss-fig}に示すように、クロスエントロピー関数を使用して予測値とラベル値の間の誤差を定量化しています。ここで、\ascii{x}軸は反復回数を表し、\ascii{y}軸は損失値を表します。また、トレーニングサンプルデータセットに基づく損失値の曲線は大きく振動していますが、テストサンプルデータセットに基づく損失値の曲線の振動は小さくなっています。

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/mnist-cross-entropy-loss-fig.jpeg}
\caption{トレーニングとテストのクロスエントロピー損失}
 \label{fig:mnist-cross-entropy-loss-fig}
\end{figure}

\refig{mnist-accuracy-fig}に示すように、現在のトレーニングデータセットとテストセットにおけるモデルの精度をリアルタイムで計算することができます。ここで、\ascii{x}軸は反復回数を表し、\ascii{y}軸は精度値を表します。同様に、トレーニングサンプルデータセットに基づく精度曲線は大きく振動していますが、テストサンプルデータセットに基づく精度曲線の振動は小さくなっています。

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/mnist-accuracy-fig.jpeg}
\caption{トレーニングとテストの精度}
 \label{fig:mnist-accuracy-fig}
\end{figure}

\refig{mnist-weight-fig}に示すように、モデルの各トレーニングパラメータ（バイアスを含む）について、対応する数値分布図を統計的に得ることができます。モデルが収束しない場合、パラメータの数値分布図は有用なヒント情報を提供することができます。

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/mnist-weight-fig.png}
\caption{重み分布図}
 \label{fig:mnist-weight-fig}
\end{figure}

\end{content}

\section{シングルレイヤーパーセプトロン}

\begin{content}

まず、\ascii{10}個のニューロンを持つシングルレイヤーパーセプトロンの構築を試みます。\refig{mnist-slp}に示すように、手書き数字認識のような多クラス分類問題に対しては、理論的には通常\ascii{softmax}活性化関数を使用します。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/mnist-slp.png}
\caption{シングルレイヤーパーセプトロン}
 \label{fig:mnist-slp}
\end{figure}

\subsection{理論的基礎}

理論的には、\ascii{softmax}回帰は\ascii{logistic}回帰の一般化拡張です。ここで、\ascii{logistic}回帰は二値分類問題を解決するためのものです。つまり、$y \in \{ 0,1\}$です。一方、\ascii{softmax}回帰は$ k $クラス分類問題を解決するためのものです。つまり、$y \in \{ 1,2,...,k\}$です。

\subsubsection{記号の定義}

\ascii{softmax}回帰問題を形式的に記述するために、ここでいくつかの一般的な記号を定義します。

 \begin{itemize}
   \item \ascii{トレーニングサンプルセット}: $ S = \{ ({x^{(i)}},{y^{(i)}});i = 1,2,...,m\} $
   \item \ascii{第$i$番目のトレーニングサンプル}: $ ({x^{(i)}},{y^{(i)}}) $
   \item \ascii{サンプル入力}: $ x = ({x_1},{x_2},...,{x_n})^{T}  \in {\mathbb{R}^n} $
   \item \ascii{サンプルラベル(one-hot)}: $ y = ({y_1},{y_2},...,{y_k})^{T} \in {\mathbb{R}^k} $
   \item \ascii{重み}: $ W \in {\mathbb{R}^{n \times k}} $   
   \item \ascii{バイアス}: $ b \in {\mathbb{R}^k} $   
   \item \ascii{softmax関数}: $ 
softmax {(z_i)} = \tfrac{{{e^{{z_i}}}}}{{\sum\limits_{j = 1}^k {{e^{{z_j}}}} }}  \quad i = 1,2,...,k
$
 \end{itemize}

\subsubsection{softmax関数}

\refig{softmax}に示すように、モデルはまず線形加重和$z$を求め、次に$e^z$を求め、最後に正規化操作を実施します。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/softmax.png}
\caption{softmax関数}
 \label{fig:softmax}
\end{figure}

\subsubsection{重みとバイアス}

重み$W$は$n \times k$の二次元行列です。

\[
W = \left( {{W_1},{W_2},...,{W_k}} \right) = \left( {\begin{array}{*{20}{c}}
  {{w_{11}}}& \ldots &{{w_{1k}}} \\ 
   \vdots & \ddots & \vdots  \\ 
  {{w_{n1}}}& \cdots &{{w_{nk}}} 
\end{array}} \right) \in {\mathbb{R}^{n \times k}}
\]

ここで、$W_j$は長さ$n$のベクトルです。

\[
{W_j} = {\left( {{w_{1j}},{w_{2j}},...,{w_{nj}}} \right)^T} \in {\mathbb{R}^n}, j = 1,2,...,k \\
\]

一方、バイアス$b$は長さ$k$の\ascii{\quo{one-hot}}ベクトルです。

\[
b = {({b_1},{b_2},...,{b_k})^T} \in {\mathbb{R}^k}
\]

\subsubsection{モデル定義}

多クラス分類問題のシングルレイヤーパーセプトロンモデルは、\ascii{softmax}活性化関数を使用して、以下のように定義できます。

\[\begin{aligned}
  y =  & {h_{W,b}}(x) = softmax (z) = softmax ({W^T}x + b) \\ 
   =  & {\left( {{y_1},{y_2},...,{y_k}} \right)^T} \\ 
   =  & \frac{1}{{\sum\limits_{j = 1}^k {{e^{{z_j}}}} }}{\left( {{e^{{z_1}}},{e^{{z_2}}},...,{e^{{z_k}}}} \right)^T} \\ 
   =  & \frac{1}{{\sum\limits_{j = 1}^k {{e^{W_j^Tx + {b_j}}}} }}{\left( {{e^{W_1^Tx + {b_1}}},{e^{W_2^Tx + {b_2}}},...,{e^{W_k^Tx + {b_k}}}} \right)^T} \ 
\end{aligned} \]

ここで、任意の与えられたサンプル$ (x, y) \in S $に対して、$ z_i $は$W_i^Tx+b_i$の線形加重和を表し、$y_i(i=1,2,...,k)$はそれをクラス$i$に分類する確率を表します。

\[\begin{gathered}
  P\left( {y = i|x;W,b} \right) = \frac{{{e^{W_i^Tx + b_i}}}}{{\sum\limits_{j = 1}^k {{e^{W_j^Tx + b_j}}} }} \hfill \\
  i = 1,2,...,k \hfill \\ 
\end{gathered} \]


\subsubsection{クロスエントロピー関数}

サンプルデータセット$ S = \{ ({x^{(i)}},{y^{(i)}});i = 1,2,...,m\} $に基づいて、クロスエントロピー損失関数は以下のように定義できます。

\[\begin{aligned}
  J(W,b) =  &  - \frac{1}{m}\sum\limits_{i = 1}^m {{y^{(i)}}\log \left( {{{\widehat y}^{(i)}}} \right)}  \\ 
   =  &  - \frac{1}{m}\sum\limits_{i = 1}^m {\sum\limits_{j = 1}^k {y_j^{(i)}\log \left( {\widehat y_j^{(i)}} \right)} }  \\
\end{aligned} \]

\ascii{softmax}多クラス分類問題は、最適解$(W^*,b^*)$を求めることです。これにより、

\[W^*,b^* = \mathop {\arg \min }\limits_{W,b} J(W,b)\]

\subsubsection{勾配の計算}

任意のサンプル$ (x,y) \in S $に対して、$ J(W,b) $の$ W $と$ b $に関する勾配公式を導出できます。

\[\begin{aligned}
  {\nabla _W}J\left( {W,b;x,y} \right) =  & \left( {\widehat y - y} \right)x \\ 
  {\nabla _b}J\left( {W,b;{x^{(i)}},{y^{(i)}}} \right) =  & \left( {\widehat y - y} \right) \\ 
\end{aligned} \]


\subsubsection{パラメータの更新}

トレーニングサンプルデータ$ S $に対して、$W, b$の勾配公式に基づいて、今回の反復でのパラメータ更新を完了します。

\[\begin{aligned}
  W \leftarrow  & W - \alpha \frac{{\sum\limits_{i = 1}^m {{\nabla _W}J\left( {W,b;{x^{(i)}},{y^{(i)}}} \right)} }}{m} \\ 
  b \leftarrow  & b - \alpha \frac{{\sum\limits_{i = 1}^m {{\nabla _b}J\left( {W,b;{x^{(i)}},{y^{(i)}}} \right)} }}{m} \\ 
\end{aligned} \]

\subsection{モデルの定義}

次に、\tf{}を使用してこのモデルの構築とトレーニングを完了します。注意すべきは、理論上の公式と\tf{}の具体的な実装には微妙な違いがあることです。理論上では、公式の$x$は通常1つのサンプルを表しますが、\tf{}の\code{x}は通常1つの\ascii{mini-batch}のサンプルデータセットを表します。したがって、\tf{}を使用してネットワークモデルを設計する際は、各テンソルのサイズの変化が期待通りかどうかに特に注意する必要があります。

\subsubsection{入力とラベル}

まず、\code{tf.placeholder}を使用してトレーニングサンプルの入力とラベルを定義します。

\begin{leftbar}
\begin{python}
x = tf.placeholder(tf.float32, [None, 28, 28, 1])
t = tf.placeholder(tf.float32, [None, 10])
\end{python}
\end{leftbar}

\code{tf.placeholder}はプレースホルダーの\ascii{OP}を定義します。\code{None}は未確定のサンプル数を表し、ここでは\code{batch\_size}の大きさを表します。\code{Session.run}時に、\code{feed\_dict}辞書を通じて1つの\ascii{mini-batch}のサンプルデータセットを提供し、\code{tf.placeholder}のサイズを自動的に推論します。

また、各画像は$ 28 \times 28 \times 1 $の3次元データ（グレースケールは\ascii{1}）で表現されます。問題を簡略化するために、ここでは入力サンプルデータをフラット化し、長さ\ascii{784}の1次元ベクトルに変換します。ここで、\ascii{-1}は\ascii{mini-batch}のサンプル数を表し、実行時に自動的にそのサイズを推論します。

\begin{leftbar}
\begin{python}
x = tf.reshape(x, [-1, 784])
\end{python}
\end{leftbar}

\subsubsection{変数の定義}

次に、\code{tf.Variable}を使用してモデルパラメータを定義します。トレーニングパラメータを定義する際は、パラメータの初期値を指定する必要があります。トレーニングパラメータは初期値に基づいて、データ型とそのサイズを自動的に推論します。

\begin{leftbar}
\begin{python}
w = tf.Variable(tf.zeros([784, 10]))
b = tf.Variable(tf.zeros([10]))
\end{python}
\end{leftbar}

さらに、変数を使用する前に、初期化を完了する必要があります。ここでは、\code{init\_op}がすべてのグローバルトレーニングパラメータを初期化します。

\begin{leftbar}
\begin{python}
init_op = tf.global_variables_initializer()
\end{python}
\end{leftbar}

\subsubsection{モデルの定義}

次に、多クラス分類問題のシングルレイヤーパーセプトロンモデルを簡単に得ることができます。

\begin{leftbar}
\begin{python}
y = tf.nn.softmax(tf.matmul(x, w) + b)
\end{python}
\end{leftbar}

\refig{mnist-linear-sum}に示すように、まず\code{x}と\code{w}の行列乗算を計算し、次に\code{b}を行列の各行にブロードキャスト（\ascii{broadcast}）して加算し、最終的にトレーニングパラメータの線形加重和を得ます。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/mnist-linear-sum.png}
\caption{線形加重和}
 \label{fig:mnist-linear-sum}
\end{figure}

\refig{mnist-softmax}に示すように、\ascii{softmax}は行ごとに計算を実施し、最終的に\code{y}のサイズは\code{[100, 10]}になります。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/mnist-softmax.png}
\caption{活性化関数：softmax}
 \label{fig:mnist-softmax}
\end{figure}

\subsubsection{損失関数}

多クラス分類問題に対しては、クロスエントロピーの損失関数を使用できます。

\begin{leftbar}
\begin{python}
cross_entropy = -tf.reduce_sum(t * tf.log(y))
\end{python}
\end{leftbar}

\refig{mnist-cross-entropy}に示すように、\code{t}と\code{y}のサイズはともに\code{[100, 10]}です。特に、\code{t}の各行は\quo{\ascii{one-hot}}ベクトルです。

\code{y}に対して\code{tf.log}操作を実施すると、サイズ\code{[100, 10]}の行列も得られます。次に、\code{t}と\code{tf.log(y)}を要素ごとに乗算（行列乗算ではない）すると、サイズ\code{[100, 10]}の行列も得られます。最後に、\code{tf.reduce\_sum}が行列内のすべての要素を加算し、スカラー（\ascii{scalar}）値を得ます。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/mnist-cross-entropy.png}
\caption{クロスエントロピー損失関数}
 \label{fig:mnist-cross-entropy}
\end{figure}

\subsubsection{精度}

\code{tf.argmax(y,1)}は第\ascii{1}次元に沿って最大値のインデックスを計算します。つまり、$ y_{100 \times 10} $の各行について、その行の中で最大値のインデックス値を計算します。したがって、\code{tf.argmax(y,1)}はサイズ\code{[100, 1]}の行列、またはサイズ\ascii{100}のベクトルを得ます。同様に、\code{tf.argmax(t,1)}もサイズ\ascii{100}のベクトルです。

次に、\code{tf.equal}を使用してこれらを要素ごと（\ascii{element-wise}）に等価性比較し、サイズ\ascii{100}のブールベクトルを得ます。精度を計算するために、まずブールベクトルを数値ベクトルに変換し、最終的にその数値ベクトルの平均を求めます。

\begin{leftbar}
\begin{python}
is_correct = tf.equal(tf.argmax(y,1), tf.argmax(t,1))
accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))
\end{python}
\end{leftbar}

\subsection{最適化アルゴリズム}

次に、勾配降下法アルゴリズムを使用してクロスエントロピー損失関数の最小化を実装します。ここで、\code{learning\_rate}は学習率を表し、パラメータ更新の速さとステップサイズを記述する典型的なハイパーパラメータです。

\begin{leftbar}
\begin{python}
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.003)
train_step = optimizer.minimize(cross_entropy)
\end{python}
\end{leftbar}

\refig{mnist-gd}に示すように、損失関数を山に例えることができます。登山者は谷底に到達するための最適な行動計画を見つけようとしています。登山者は特定の山腹に立って周囲を見回し、勾配の反対方向に小さな一歩を踏み出すことを決定し、局所的な最適点に到達するまでこれを繰り返します。

勾配降下法更新アルゴリズムを実施する際、初期点が異なれば得られる最小値も異なるため、勾配降下法で得られるのは局所的な最小値のみです。また、最小値に近づくほど、降下速度は遅くなります。降下のステップサイズも非常に重要で、小さすぎると関数の最小値を見つける速度が遅くなり、大きすぎると極値点を超えてしまう可能性があります。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/mnist-gd.jpeg}
\caption{勾配降下法アルゴリズム}
 \label{fig:mnist-gd}
\end{figure}

\subsection{モデルのトレーニング}

これまでの段階では、\tf{}は計算グラフを構築しただけで、計算グラフの実行は開始していません。次に、クライアントがセッションを作成し、ローカルまたはリモートの計算デバイス集合とのチャネルを確立し、計算グラフの実行プロセスを開始します。

まず、トレーニングパラメータの初期化を完了します。モデルパラメータの初期化サブグラフを実行し、各トレーニングパラメータの初期化子を並行して実行し、初期値を対応するトレーニングパラメータにその場で修正します。

\begin{leftbar}
\begin{python}
with tf.Session() as sess:
  sess.run(init_op)
\end{python}
\end{leftbar}

次に、\code{train\_step}を繰り返し実行し始め、モデルの1回の反復トレーニングを完了します。ここで、100回の反復ごとに、現在のモデルのトレーニングデータセットおよびテストデータセットにおける精度と損失を計算します。

\begin{leftbar}
\begin{python}
with tf.Session() as sess:
  for step in range(1000):
    batch_xs, batch_ys = mnist.train.next_batch(100)        
    sess.run(train_step, feed_dict={x: batch_xs, t: batch_ys})
    
    if step % 100 == 0:
      acc, loss = sess.run([accuracy, cross_entropy], 
        feed_dict={x: batch_xs, t: batch_ys})
      acc, loss = sess.run([accuracy, cross_entropy], 
        feed_dict={x: mnist.test.images, t: mnist.test.labels}) 
\end{python}
\end{leftbar}

統計によると、1000回の反復後、約\percent{92}の精度が得られます。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/mnist-slp-accuracy.png}
\caption{可視化：シングルレイヤーパーセプトロン、1000回のステップ実行}
 \label{fig:mnist-slp-accuracy}
\end{figure}

\end{content}

\section{マルチレイヤーパーセプトロン}

\begin{content}

精度をさらに向上させるために、次は\ascii{5}層のマルチレイヤーパーセプトロンモデルの構築を試みます。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/mnist-5-layer.png}
\caption{5層パーセプトロン}
 \label{fig:mnist-5-layer}
\end{figure}

\subsection{理論的基礎}

\subsubsection{記号の定義}

マルチレイヤーパーセプトロンモデルを形式的に記述するために、ここでいくつかの一般的な記号を定義します。

\begin{itemize}
   \item \alert{$ {n_{\ell}} $}: ネットワーク層数、ここで第$0$層は入力層、第$n_{\ell}$層は出力層
   \item \alert{$ {s_{\ell}} $}: 第$\ell$層のノード数、$ \ell = 0, 1, ..., n_{\ell} $
   \item \alert{$ w_{ji}^{(\ell)} $}: 第$(\ell-1)$層のノード$i$と第$\ell$層のノード$j$の間の重み、$ \ell = 1, ..., n_{\ell} $
   \item \alert{$ b_i^{(\ell)} $}: 第$\ell$層のノード$i$のバイアス項、$ \ell = 1, ..., n_{\ell} $
   \item \alert{$ a_i^{(\ell)} $}: 第$\ell$層のノード$i$の出力、$ \ell = 1, ..., n_{\ell}, x = a^{(0)}, y = a^{(n_{\ell})} $
   \item \alert{$ z_i^{(\ell)} $}: 第$\ell$層のノード$i$の重み和、$ \ell = 1, ..., n_{\ell} $
   \item \alert{$ \delta _i^{(\ell)} $}: 第$\ell$層のノード$i$の誤差項、$ \ell = 1, ..., n_{\ell} $
   \item \alert{$ S = \{ ({x^{(t)}},{y^{(t)}});t = 1,2,...,m\} $}: サンプル空間
 \end{itemize}

\subsubsection{順伝播}

$z^{(\ell )}$は$\ell$層の線形重み和を表し、第$\ell - 1$層の出力$a^{(\ell  - 1)}$と第$\ell$層の重み行列$w^{(\ell )}$の積に、第$\ell$層のバイアスベクトルを加えたものです。

一般化すると、第$\ell$層の出力は、活性化関数$f({z^{(\ell )}})$によって得られます。ここで、$a^{(0)}} = x, y = {a^{({n_\ell })}}$です。

\[\begin{gathered}
  {z^{(\ell )}} = {w^{(\ell )}}{a^{(\ell  - 1)}} + {b^{(\ell )}} \hfill \\
  {a^{(\ell )}} = f({z^{(\ell )}}) \hfill \\
  {a^{(0)}} = x \hfill \\
  y = {a^{({n_\ell })}} \hfill \\ 
\end{gathered} \]

\subsubsection{逆伝播}

次に、各層の誤差を逆算します。ここで、第$\ell$層の誤差は、$\ell + 1$層の誤差から計算されます。特に、出力層では、予測値$a^{({n_\ell })}$と$y$の間の誤差を直接計算できます。

\[{\delta ^{(\ell)}} = \left\{ \begin{gathered}
  {({w^{(\ell + 1)}})^T}{\delta ^{(\ell + 1)}} \circ f\,'({z^{(\ell)}});{\text{  }}\ell \ne {n_\ell} \hfill \\
  ({a^{(\ell)}} - y) \circ f\,'({z^{(\ell)}}); {\text{  }}\ell = {n_\ell} \hfill \\ 
\end{gathered}  \right.\]

損失関数$J(w,b)$の各層の重み行列とバイアスベクトルに関する勾配は以下のように計算できます。

\[\begin{gathered}
  {\nabla _{{w^{(\ell )}}}}J(w,b;x,y) = {\delta ^{(\ell )}}{\left( {{a^{(\ell  - 1)}}} \right)^T} \hfill \\
  {\nabla _{{b^{(\ell )}}}}J(w,b;x,y) = {\delta ^{(\ell )}} \hfill \\
  \ell  = 1,2,...,{n_\ell } \hfill \\ 
\end{gathered} \]

一般的に、実際のシステム実装では、下流層が勾配を上層に伝達し、上層が直接勾配の計算を完了します。

\subsubsection{パラメータの更新}

与えられたサンプルデータセット$ S = \{ ({x^{(t)}},{y^{(t)}});t = 1,2,...,m\} $に対して、勾配逆伝播公式に基づいて、パラメータ更新の変化量を計算できます。

\[\begin{aligned}
  \Delta {w^{(\ell )}} \leftarrow \Delta {w^{(\ell )}} + {\nabla _{{w^{(\ell )}}}}J\left( {w,b;{x^{(t)}},{y^{(t)}}} \right) \\ 
  \Delta {b^{(\ell )}} \leftarrow \Delta {b^{(\ell )}} + {\nabla _{{b^{(\ell )}}}}J\left( {w,b;{x^{(t)}},{y^{(t)}}} \right) \\ 
  t = 1,2,...,m;\ell  = 1,2,...,{n_\ell } \\ 
\end{aligned} \]

最後に、勾配降下法アルゴリズムを実行し、トレーニングパラメータの1回の反復更新を完了します。

\[\begin{aligned}
  {w^{(\ell )}} \leftarrow  & {w^{(\ell )}} - \alpha \left( {\frac{{\Delta {w^{(\ell )}}}}{m}} \right) \\ 
  {b^{(\ell )}} \leftarrow  & {b^{(\ell )}} - \alpha \frac{{\Delta {b^{(\ell )}}}}{m} \\ 
  \ell  = & 1,2,...,{n_\ell }  \\
\end{aligned} \]

\subsection{モデルの定義}

前節で試みたシングルレイヤーパーセプトロンと比較して、ここでは各隠れ層の重みを定義する際、定数を使用して変数の初期値を定義するのではなく、特定のデータ分布特性を満たすランダム値を使用しています。

\begin{leftbar}
\begin{python}
K = 200
L = 100
M = 60
N = 30

w1 = tf.Variable(tf.truncated_normal([28*28, K] ,stddev=0.1)) 
b1 = tf.Variable(tf.zeros([K]))

w2 = tf.Variable(tf.truncated_normal([K, L], stddev=0.1))
b2 = tf.Variable(tf.zeros([L]))

w3 = tf.Variable(tf.truncated_normal([L, M], stddev=0.1)) 
b3 = tf.Variable(tf.zeros([M]))

w4 = tf.Variable(tf.truncated_normal([M, N], stddev=0.1)) 
b4 = tf.Variable(tf.zeros([N]))

w5 = tf.Variable(tf.truncated_normal([N, 10], stddev=0.1)) 
b5 = tf.Variable(tf.zeros([10]))
\end{python}
\end{leftbar}

各隠れ層を定義する際、\ascii{sigmoid}活性化関数を採用しています。最後の出力層では、\ascii{softmax}活性化関数を採用しています。

\begin{leftbar}
\begin{python}
y1 = tf.nn.sigmoid(tf.matmul(x,  w1) + b1)
y2 = tf.nn.sigmoid(tf.matmul(y1, w2) + b2)
y3 = tf.nn.sigmoid(tf.matmul(y2, w3) + b3)
y4 = tf.nn.sigmoid(tf.matmul(y3, w4) + b4)
y  = tf.nn.softmax(tf.matmul(y4, w5) + b5)
\end{python}
\end{leftbar}

反復的なモデルトレーニングを経て、約\percent{97}の精度を得ることができます。しかし、ネットワークの層が増えるにつれて、モデルの収束が難しくなります。次に、いくつかの一般的な最適化技術を試み、ネットワークのパフォーマンスを改善します。

\subsection{最適化技術}

\subsubsection{活性化関数：ReLU}

深層モデルでは、\ascii{sigmoid}活性化関数の使用は適していません。これはすべての値を\ascii{0}から\ascii{1}の間に押し込めてしまいます。ネットワークの層が増えるにつれて、勾配消失問題を引き起こします。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/mnist-relu.png}
\caption{ReLU活性化関数}
 \label{fig:mnist-relu}
\end{figure}

\ascii{sigmoid}の代わりに\ascii{ReLU(Rectified Linear Unit)}を使用することで、\ascii{sigmoid}によって引き起こされるいくつかの問題を避けるだけでなく、初期の収束速度を加速することができます。

\begin{leftbar}
\begin{python}
y1 = tf.nn.relu(tf.matmul(x,  w1) + b1)
y2 = tf.nn.relu(tf.matmul(y1, w2) + b2)
y3 = tf.nn.relu(tf.matmul(y2, w3) + b3)
y4 = tf.nn.relu(tf.matmul(y3, w4) + b4)
y  = tf.nn.softmax(tf.matmul(y4, w5) + b5)
\end{python}
\end{leftbar}

また、\ascii{ReLU}活性化関数を使用する場合、バイアスベクトルは通常小さな正の値で初期化され、ニューロンが最初から\ascii{ReLU}の非ゼロ領域で動作するようにします。

\begin{leftbar}
\begin{python}
K = 200
L = 100
M = 60
N = 30

w1 = tf.Variable(tf.truncated_normal([28*28, K] ,stddev=0.1)) 
b1 = tf.Variable(tf.ones([L])/10)

w2 = tf.Variable(tf.truncated_normal([K, L], stddev=0.1))
b2 = tf.Variable(tf.ones([L])/10)

w3 = tf.Variable(tf.truncated_normal([L, M], stddev=0.1)) 
b3 = tf.Variable(tf.ones([L])/10)

w4 = tf.Variable(tf.truncated_normal([M, N], stddev=0.1)) 
b4 = tf.Variable(tf.ones([L])/10)

w5 = tf.Variable(tf.truncated_normal([N, 10], stddev=0.1)) 
b5 = tf.Variable(tf.ones([L])/10)
\end{python}
\end{leftbar}

\refig{mnist-sigmoid-to-relu}に示すように、最初の\ascii{300}回の反復において、\ascii{sigmoid}を使用する場合と比較して、\ascii{ReLU}を使用すると初期収束速度が顕著に向上しています。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/mnist-sigmoid-to-relu.png}
\caption{ReLU活性化関数の適用：初期収束速度が顕著に向上}
 \label{fig:mnist-sigmoid-to-relu}
\end{figure}

\subsubsection{不定値}

安定した数値計算結果を得るため、精度が突然\ascii{0}になるような状況を避けます。実装コードを追跡すると、\code{log(0)}の計算により\code{NaN}不定値の問題が発生する可能性があります。クロスエントロピー損失の計算には\code{softmax\_cross\_entropy\_with\_logits}を使用し、線形重み和をその入力（通常\ascii{logits}と呼ばれる）として使用することができます。

\begin{leftbar}
\begin{python}
logits = tf.matmul(y4, w5) + b5
y = tf.nn.softmax(logits)

cross_entropy = tf.nn.softmax_cross_entropy_with_logits(
  logits=logits, labels=t)
\end{python}
\end{leftbar}

\subsubsection{学習率減衰}

ネットワークの層が増え、関連する最適化技術を適用した後、モデルの精度は約\percent{98}程度に達することができますが、安定した精度を得ることは難しいです。\refig{mnist-lr-too-larger}に示すように、精度と損失の変動が非常に顕著です。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/mnist-lr-too-larger.png}
\caption{ノイズ変動：学習率が大きすぎる}
 \label{fig:mnist-lr-too-larger}
\end{figure}

より良い最適化アルゴリズム、例えば\code{AdamOptimizer}を採用することができます。反復回数が増えるにつれて、学習率は指数関数的に減衰し、モデルトレーニングの後期でより安定した精度と損失曲線を得ることができます。

\begin{leftbar}
\begin{python}
lr = tf.placeholder(tf.float32)
train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)
\end{python}
\end{leftbar}

各反復トレーニングプロセスで、現在の\code{step}値に基づいて、現在の反復の学習率\code{lr}をリアルタイムで計算し、\code{feed\_dict}を通じて\code{Session.run}に渡して実行します。ここで、学習率減衰方程式は以下のコードに示すように、反復回数が増えるにつれて学習率が指数関数的に減衰します。

\begin{leftbar}
\begin{python}
def lr(step):
  max_lr, min_lr, decay_speed = 0.003, 0.0001, 2000.0
  return min_lr + (max_lr - min_lr) * math.exp(-step/decay_speed)

with tf.Session() as sess:
  for step in range(10000):
    batch_xs, batch_ys = mnist.train.next_batch(100)
    sess.run(train_step, 
      feed_dict={x: batch_xs, t: batch_ys, pkeep: 0.75, lr: lr(step)})
\end{python}
\end{leftbar}

\refig{mnist-apply-learning-rate-decay}に示すように、学習率減衰方法を適用した後、より安定した精度と損失曲線を得ることができます。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/mnist-apply-learning-rate-decay.png}
\caption{Adam最適化アルゴリズムを適用後、精度と損失が安定化}
 \label{fig:mnist-apply-learning-rate-decay}
\end{figure}

\subsubsection{Dropoutの適用}

しかし、損失曲線がトレーニングセットとテストセットで分離し、明らかな過学習現象が発生しています。つまり、モデルはトレーニングデータセットでは良好な性能を示しますが、テストデータセットでは反発が起こり、モデルに十分な汎化能力がありません。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/mnist-overfitting.png}
\caption{過学習}
 \label{fig:mnist-overfitting}
\end{figure}

\refig{mnist-dropout}に示すように、トレーニング時に隠れ層の出力に\ascii{dropout}操作を実施し、\code{1 - pkeep}の確率でニューロンの出力をランダムに削除し、逆伝播時にそれらの重みを更新しません。一方、推論時にはすべてのニューロンの出力を復元し、間接的にネットワークの汎化能力を改善します。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/mnist-dropout.png}
\caption{Dropout手法}
 \label{fig:mnist-dropout}
\end{figure}

\tf{}を使用して\ascii{dropout}操作を実装する際、まずハイパーパラメータ\code{pkeep}を定義します。これは隠れ層のニューロンが確率\code{pkeep}でランダムに保持され、確率\code{1 - pkeep}でランダムに削除されることを表します。

\begin{leftbar}
\begin{python}
pkeep = tf.placeholder(tf.float32)

y1 = tf.nn.relu(tf.matmul(x,  w1) + b1)
y1d = tf.nn.dropout(y1, pkeep)

y2 = tf.nn.relu(tf.matmul(y1d, w2) + b2)
y2d = tf.nn.dropout(y2, pkeep)

y3 = tf.nn.relu(tf.matmul(y2d, w3) + b3)
y3d = tf.nn.dropout(y3, pkeep)

y4 = tf.nn.relu(tf.matmul(y3d, w4) + b4)
y4d = tf.nn.dropout(y4, pkeep)

logits = tf.matmul(y4d, w5) + b5
y = tf.nn.softmax(Ylogits)
\end{python}
\end{leftbar}

トレーニング時には、ハイパーパラメータ\code{pkeep}の値を1未満に設定します。一方、推論時には、ハイパーパラメータ\code{pkeep}の値を1に設定します。

\begin{leftbar}
\begin{python}
with tf.Session() as sess:
  for step in range(10000):
    batch_xs, batch_ys = mnist.train.next_batch(100)
    sess.run(train_step, 
      feed_dict={x: batch_xs, t: batch_ys, pkeep: 0.75, lr: lr(step)})

    if step % 100 == 0:
      acc, loss = sess.run([accuracy, cross_entropy], 
        feed_dict={x: batch_xs, t: batch_ys, pkeep: 1})
      acc, loss = sess.run([accuracy, cross_entropy], 
        feed_dict={x: mnist.test.images, t: mnist.test.labels, pkeep: 1})
\end{python}
\end{leftbar}

各隠れ層に\ascii{dropout}操作を適用した後、トレーニングセットとテストセットの損失曲線が再び交差します。しかし、精度と損失曲線は小幅の変動を示し、トレーニングセットとテストセットの損失曲線の重なり具合は理想的ではなく、過学習問題は依然として顕著です。

つまり、過学習問題にはより深刻な他の原因が存在します。例えば、$ 28 \times 28 $の画像をフラット化操作して長さ\ascii{784}の一次元ベクトルに変換することで、ピクセルの空間配置情報が完全に失われてしまいます。

次に、畳み込みニューラルネットワークを構築することで、元の画像から特徴を抽出し、ピクセルの空間配置情報を保持し、ネットワークのパフォーマンスを向上させます。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/mnist-apply-dropout-result.png}
\caption{Dropout適用後、トレーニングセットとテストセットの損失曲線が再び重なる}
 \label{fig:mnist-apply-dropout-result}
\end{figure}

\end{content}

\section{畳み込みネットワーク}

\begin{content}

\subsection{特徴と利点}

ネットワークの層が増えるにつれて、全結合ネットワークの勾配消失問題はますます顕著になり、収束速度が遅くなります。全結合ネットワークと比較して、畳み込みネットワークには3つの主な特徴があり、これによりネットワークパラメータの数が減少し、ネットワークの汎化能力が向上します。

\subsubsection{局所接続}

全結合ネットワークと比較して、畳み込みネットワークは局所接続を実現しています。つまり、各ニューロンは前の層のすべてのニューロンとは接続していません。\refig{mnist-conv-local-conn}の左側に示すように、$ 1000 \times 1000 $ピクセルの画像と$ 10^6 $個の隠れ層ニューロンがあると仮定します。全結合ネットワークでは、$ 10^3 \times 10^3 \times 10^6 = 10^{12} $個のトレーニングパラメータを持つことになります。

実際には、各ニューロンが前の層のすべてのニューロンと接続する必要はありません。\refig{mnist-conv-local-conn}の右側に示すように、各隠れ層のニューロンが前の層の$ 10 \times 10 $の局所画像領域とのみ接続していると仮定すると、$ 10^6 $個の隠れ層ニューロンには$ 10^6 \times 10^2 = 10^8$個のネットワーク接続が必要となり、4桁のオーダーで減少します。

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/mnist-conv-local-conn.png}
\caption{局所接続}
 \label{fig:mnist-conv-local-conn}
\end{figure}

\subsubsection{重み共有}

ネットワーク接続をさらに減らすために、畳み込みネットワークは重み共有も実装しています。つまり、各接続グループが同じ重みを共有し、各接続が異なる重みを持つのではありません。\refig{mnist-conv-local-conn-2}の右側に示すように、各隠れ層のニューロンは$ 10 \times 10 $の局所画像領域とのみ接続し、$ 10 \times 10 $の重み行列を共有しており、隠れ層のニューロンの数に関係ありません。\refig{mnist-conv-local-conn-2}の左側に示す局所接続ネットワークと比較すると、$10^8$個のパラメータが必要なところを、畳み込み層では$10^2$個のパラメータしか必要としません。

\refig{mnist-conv-local-conn-2}の右側に示すように、異なる特徴、例えば異なるエッジの画像特徴を抽出するために、複数のフィルターを使用できます。例えば、100個のフィルターがある場合、$10^4$個のパラメータが必要になります。

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/mnist-conv-local-conn-2.png}
\caption{重み共有、複数のフィルター}
 \label{fig:mnist-conv-local-conn-2}
\end{figure}

\subsubsection{ダウンサンプリング}

\refig{mnist-subsample}に示すように、オプションでダウンサンプリングを実施し、ネットワークのパラメータをさらに減少させ、モデルの堅牢性を向上させることができます。

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/mnist-subsample.png}
\caption{ダウンサンプリング}
 \label{fig:mnist-subsample}
\end{figure}

\subsection{畳み込み演算}

畳み込み演算は計算集約型の\ascii{OP}です。\refig{mnist-conv2d-gif}に示すように、重みベクトル\code{w[3,3,3,2]}があり、入力チャネル数が3、出力チャネル数が2、畳み込みカーネルサイズが$3 \times 3$です。

明らかに、入力画像のチャネル数は畳み込みカーネルの深さと等価であり、畳み込みカーネルの数は\ascii{Feature Map}の出力チャネル数と等しいです。また、画像のエッジ特徴を捉えるために、元の画像の周囲にゼロ値のパディング（\ascii{padding}）が追加されています。各畳み込み計算では、移動のステップ（\ascii{stride}）は2です。したがって、最終的な出力\ascii{Feature Map}のサイズは$3 \times 3$になります。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/mnist-conv2d-gif.png}
\caption{畳み込み演算}
 \label{fig:mnist-conv2d-gif}
\end{figure}

\subsubsection{例}

$32 \times 32 \times 3$の画像があり、畳み込みカーネルのサイズが$5 \times 5 \times 3$だとします。ここで、畳み込みカーネルの深さは画像の入力チャネル数と等しいです。\refig{mnist-conv-1dot}に示すように、畳み込みカーネルは画像の$5 \times 5 \times 3$のブロックと内積演算を行い、1つの値を得ます。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/convolutional-layer-2.png}
\caption{畳み込み演算：畳み込みカーネルと画像ブロックの内積演算}
 \label{fig:mnist-conv-1dot}
\end{figure}

\refig{mnist-conv-ndot}に示すように、畳み込みカーネルは画像空間全体を走査し、最終的に$28 \times 28 \times 1$のサイズの\ascii{Feature Map}を得ます。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/convolutional-layer-3.png}
\caption{畳み込み演算：畳み込みカーネルが画像を走査、ストライドは1}
 \label{fig:mnist-conv-ndot}
\end{figure}

\refig{mnist-conv-multi-filters}に示すように、複数の畳み込みカーネルがある場合、複数の\ascii{Feature Map}が得られます。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/convolutional-layer-4.png}
\caption{畳み込み演算：複数の畳み込みカーネル}
 \label{fig:mnist-conv-multi-filters}
\end{figure}

\subsection{公式の導出}

\subsubsection{順伝播}

$Z^{(\ell )}$は$\ell$層の線形重み和を表し、第$\ell - 1$層の出力$A^{(\ell  - 1)}$と第$\ell$層の重み行列$W^{(\ell )}$の畳み込み、そして第$\ell$層のバイアスベクトルを加えたものです。

一般化すると、第$\ell$層の出力は、活性化関数$f({Z^{(\ell )}})$によって得られます。ここで、$A^{(0)}} = x, y = {A^{({n_\ell })}}$です。

\[\begin{gathered}
  {Z^{(\ell )}} = {A^{(\ell  - 1)}} * {W^{(\ell )}} + {b^{(\ell )}} \hfill \\
  {A^{(\ell )}} = f\left( {{Z^{(\ell )}}} \right) \hfill \\ 
\end{gathered} \]

\subsubsection{逆伝播}

次に、各層の誤差を逆算します。ここで、第$\ell$層の誤差は、$\ell + 1$層の誤差から計算されます。全結合ネットワークと比較して、ここでは行列乗算ではなく畳み込み演算を使用しています。

\[
{\delta ^{(\ell )}} = {\delta ^{(\ell  + 1)}} * {W^{(\ell  + 1)}} \circ f\,'\left( {{z^{(\ell )}}} \right)
\]

損失関数$J(w,b)$の各層の重み行列とバイアスベクトルに関する勾配は以下のように計算できます。

\[\begin{aligned}
  {\nabla _{{W^{(\ell )}}}}J(W,b) =  & {A^{(\ell  - 1)}} * {\delta ^{(\ell )}} \\ 
  {\nabla _{{b^{(\ell )}}}}J(W,b) =  & {\delta ^{(\ell )}} \\ 
\end{aligned} \]

\subsection{畳み込みネットワークの実装}

畳み込みネットワークを実装する際、まず各層のフィルターの重み行列を定義する必要があります。これは画像の特徴を抽出するために使用されます。重み行列は画像内で、元の画像行列から特定の情報を抽出するフィルターのように機能します。1つの重み行列は画像のエッジ情報を抽出し、別の重み行列は特定の色を抽出し、また別の重み行列は不要なノイズをぼかす可能性があります。

複数の畳み込み層がある場合、初期層は一般的により多くの一般的な特徴を抽出します。ネットワーク構造が深くなるにつれて、重み行列が抽出する特徴はますます複雑になり、目前の具体的な問題により適したものになります。

一般的に、フィルターは4次元のテンソルで表現されます。最初の2次元はフィルターのサイズを表し、3次元目は入力チャネル数を表し、4次元目は出力チャネル数を表します。\refig{mnist-filter}に示すとおりです。

\begin{figure}[H]
\centering
\includegraphics[width=0.4\textwidth]{figures/mnist-filter.png}
\caption{畳み込み層のフィルター}
 \label{fig:mnist-filter}
\end{figure}

\refig{mnist-conv2d-1}に示すように、3つの畳み込み層と2つの全結合層を構築しました。ここで、中間の隠れ層では\ascii{ReLU}活性化関数を使用し、最後の出力層では\ascii{softmax}活性化関数を採用しています。

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/mnist-conv2d-1.png}
\caption{畳み込みニューラルネットワークの実装}
 \label{fig:mnist-conv2d-1}
\end{figure}

\tf{}を使用して畳み込みネットワークを実装する方法は以下のコードに示すとおりです。

\begin{leftbar}
\begin{python}
K = 4 
L = 8
M = 12
N = 200

w1 = tf.Variable(tf.truncated_normal([5, 5, 1, K], stddev=0.1))
b1 = tf.Variable(tf.ones([K])/10)

w2 = tf.Variable(tf.truncated_normal([5, 5, K, L], stddev=0.1))
b2 = tf.Variable(tf.ones([L])/10)

w3 = tf.Variable(tf.truncated_normal([4, 4, L, M], stddev=0.1))
b3 = tf.Variable(tf.ones([M])/10)

w4 = tf.Variable(tf.truncated_normal([7 * 7 * M, N], stddev=0.1))
b4 = tf.Variable(tf.ones([N])/10)

w5 = tf.Variable(tf.truncated_normal([N, 10], stddev=0.1))
b5 = tf.Variable(tf.ones([10])/10)

y1 = tf.nn.relu(tf.nn.conv2d(
       x,  w1, strides=[1, 1, 1, 1], padding='SAME') + b1)
y2 = tf.nn.relu(tf.nn.conv2d(
       y1, w2, strides=[1, 2, 2, 1], padding='SAME') + b2)
y3 = tf.nn.relu(tf.nn.conv2d(
       y2, w3, strides=[1, 2, 2, 1], padding='SAME') + b3)

yy = tf.reshape(Y3, shape=[-1, 7 * 7 * M])
y4 = tf.nn.relu(tf.matmul(yy, w4) + b4)

logits = tf.matmul(y4, w5) + b5
y = tf.nn.softmax(logits)
\end{python}
\end{leftbar}

\refig{mnist-conv2d-1-result}に示すように、$10^4$回のトレーニングを経て、約\percent{98.9}の精度を得ることができます。

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/mnist-conv2d-1-result.png}
\caption{畳み込みネットワークの実装：\percent{98.9}の精度を達成}
 \label{fig:mnist-conv2d-1-result}
\end{figure}

\subsection{畳み込みネットワークの強化}

\refig{mnist-conv2d-2}に示すように、以前のネットワーク層構造を維持し、3つの畳み込み層と2つの全結合層を構築しました。ここで、中間の隠れ層では\ascii{ReLU}活性化関数を使用し、最後の出力層では\ascii{softmax}活性化関数を採用しています。

しかし、以前の畳み込みネットワークと比較して、より多くのチャネルを使用してより多くの特徴を抽出しています。同時に、全結合の隠れ層で\ascii{dropout}操作を実施し、ネットワークの汎化能力を強化しています。

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/mnist-conv2d-2.png}
\caption{畳み込みニューラルネットワークの改善}
 \label{fig:mnist-conv2d-2}
\end{figure}

\tf{}を使用してより大きな畳み込みネットワークを実装する方法は以下のコードに示すとおりです。

\begin{leftbar}
\begin{python}
K = 6
L = 12
M = 24
N = 200

w1 = tf.Variable(tf.truncated_normal([6, 6, 1, K], stddev=0.1))
b1 = tf.Variable(tf.ones([K])/10)

w2 = tf.Variable(tf.truncated_normal([5, 5, K, L], stddev=0.1))
b2 = tf.Variable(tf.ones([L])/10)

w3 = tf.Variable(tf.truncated_normal([4, 4, L, M], stddev=0.1))
b3 = tf.Variable(tf.ones([M])/10)

w4 = tf.Variable(tf.truncated_normal([7 * 7 * M, N], stddev=0.1))
b4 = tf.Variable(tf.ones([N])/10)

w5 = tf.Variable(tf.truncated_normal([N, 10], stddev=0.1))
b5 = tf.Variable(tf.ones([10])/10)

y1 = tf.nn.relu(tf.nn.conv2d(
       x,  w1, strides=[1, 1, 1, 1], padding='SAME') + b1)
y2 = tf.nn.relu(tf.nn.conv2d(
       y1, w2, strides=[1, 2, 2, 1], padding='SAME') + b2)
y3 = tf.nn.relu(tf.nn.conv2d(
       y2, w3, strides=[1, 2, 2, 1], padding='SAME') + b3)

yy = tf.reshape(Y3, shape=[-1, 7 * 7 * M])
y4 = tf.nn.relu(tf.matmul(yy, w4) + b4)
y4d = tf.nn.dropout(y4, pkeep)

logits = tf.matmul(y4d, w5) + b5
y = tf.nn.softmax(logits)
\end{python}
\end{leftbar}

\refig{mnist-conv2d-2-result}に示すように、$10^4$回のトレーニングを経て、約\percent{99.3}の精度を得ることができます。

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/mnist-conv2d-2-result.png}
\caption{強化された畳み込みネットワーク：
\percent{99.3}の精度を達成}
 \label{fig:mnist-conv2d-2-result}
\end{figure}

同時に、以前に実装した畳み込みネットワークと比較して、過学習問題が明らかに改善されました。\refig{mnist-conv2d-3-result}に示すとおりです。

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/mnist-conv2d-3-result.png}
\caption{強化された畳み込みネットワーク：過学習問題が明らかに改善}
 \label{fig:mnist-conv2d-3-result}
\end{figure}

\end{content}
