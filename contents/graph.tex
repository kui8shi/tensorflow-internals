\begin{savequote}[45mm]
\ascii{Any fool can write code that a computer can understand. Good programmers write code that humans can understand.}
\qauthor{\ascii{- Martin Flower}}
\end{savequote}

\chapter{計算グラフ} 
\label{ch:computation-graph}

\begin{content}

TensorFlowの計算グラフでは、ノードを\ascii{OP}で表し、\ascii{OP}間の計算とデータ依存関係に基づいて、\ascii{OP}間のデータ生成と消費の関係を構築し、有向エッジで表現します。有向エッジには2種類あり、1つはデータを運ぶもので\code{Tensor}で表され、もう1つはデータを運ばず、計算の依存関係のみを表すものです。

本章では、TensorFlowの最も重要なドメインオブジェクトである\emph{計算グラフ}について説明します。計算グラフの主要な実装技術を包括的に説明するために、フロントエンドとバックエンドのシステム設計と実装をそれぞれ説明し、さらにフロントエンドとバックエンドシステム間の計算グラフ変換のワークフローの原理を探ります。

\end{content}

\section{Pythonフロントエンド}

\begin{content}

Pythonのフロントエンドシステムでは、\code{Node, Edge}の概念は存在せず、\code{Operation, Tensor}の概念のみが存在します。実際、フロントエンドのPythonシステムでは、\code{Operation}がグラフの\code{Node}インスタンスを表し、\code{Tensor}がグラフの\code{Edge}インスタンスを表します。

\subsection{Operation}

\code{OP}は抽象的な数学的計算を表現するために使用され、計算グラフのノードを表します。\code{Operation}はフロントエンドのPythonシステムで最も重要なドメインオブジェクトの1つであり、TensorFlowランタイムの最小の計算単位でもあります。

\subsubsection{ドメインモデル}

図\refig{py-operation}に示すように、\code{Operation}は抽象的な計算を表し、上流ノードの出力である0個以上の\code{Tensor}を入力として受け取り、計算後に0個以上の\code{Tensor}を下流のノードに出力します。これにより、上流と下流の\code{Operation}間にデータ依存関係が生まれます。特別に、\code{Operation}は上流の制御依存エッジの集合を持つことがあり、潜在的な計算依存関係を表します。

計算グラフの構築中、\ascii{OP}コンストラクタ（\ascii{OP Constructor}）を通じて\code{Operation}インスタンスが構築され、デフォルトのグラフインスタンスに登録されます。同時に、\code{Operation}は逆に\ascii{graph}を通じて直接そのグラフインスタンスを保持します。

\code{Operation}のメタデータは\code{OpDef}と\code{NodeDef}によって保持され、これらは\ascii{ProtoBuf}形式で存在し、\code{Operation}の本質的な部分を記述します。\code{OpDef}は\ascii{OP}の静的属性情報を記述し、例えば\ascii{OP}の名前、入力/出力パラメータリスト、属性セット定義などの情報を含みます。一方、\code{NodeDef}は\ascii{OP}の動的属性値情報を記述し、例えば属性値などの情報を含みます。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/py-operation.png}
\caption{ドメインオブジェクト：Operation}
 \label{fig:py-operation}
\end{figure}

\subsubsection{コンストラクタ}

\begin{leftbar}
\begin{python}
class Operation(object):
  def __init__(self, node_def, g, inputs=None, output_types=None,
               control_inputs=None, input_types=None, original_op=None,
               op_def=None):
    # 1. NodeDef
    self._node_def = copy.deepcopy(node_def)
    
    # 2. OpDef
    self._op_def = op_def

    # 3. Graph
    self._graph = g

    # 4. Input types
    if input_types is None:
      input_types = [i.dtype.base_dtype for i in self._inputs]
    self._input_types = input_types

    # 5. Output types
    if output_types is None:
      output_types = []
    self._output_types = output_types
    
    # 6. Inputs
    if inputs is None:
      inputs = []
    self._inputs = list(inputs)

    # 7. Control Inputs.
    if control_inputs is None:
      control_inputs = []
    
    self._control_inputs = []
    for c in control_inputs:
      c_op = self._get_op_from(c)
      self._control_inputs.append(c_op)

    # 8. Outputs
    self._outputs = [Tensor(self, i, output_type)
                     for i, output_type in enumerate(output_types)]

    # 9. Build producter-consumer relation.
    for a in self._inputs:
      a._add_consumer(self)

    # 10. Allocate unique id for opeartion in graph.
    self._id_value = self._graph._next_id()
\end{python}
\end{leftbar}

\subsubsection{属性セット}

\code{Operation}は一般的な属性メソッドを定義し、その\ascii{OP}のメタデータを取得するために使用します。\code{name}はグラフ内のノードの名前を表し、\code{name\_scope}の階層名を含み、グラフインスタンスの範囲内で一意です。例えば\code{layer\_2/MatMul}のようになります。\code{type}はその\code{OP}タイプの一意の名前を表し、例えば\code{MatMul, Variable}などです。

\begin{leftbar}
\begin{python}
class Operation(object):
  @property
  def name(self):
    """The full name of this operation."""
    return self._node_def.name

  @property
  def type(self):
    """The type of the op (e.g. `"MatMul"`)."""
    return self._node_def.op

  @property
  def graph(self):
    """The `Graph` that contains this operation."""
    return self._graph

  @property
  def node_def(self):
    """Returns the `NodeDef` proto that represents this operation."""
    return self._node_def

  @property
  def op_def(self):
    """Returns the `OpDef` proto that represents the type of this op."""
    return self._op_def

  @property
  def device(self):
    """The name of the device to which this op has been assigned."""
    return self._node_def.device    
\end{python}
\end{leftbar}

\subsubsection{OPの実行}

この\ascii{OP}を終点としてグラフを逆方向にたどり、最小の依存関係を持つサブグラフを見つけ、デフォルトの\code{Session}でそのサブグラフを実行することができます。

\begin{leftbar}
\begin{python}
class Operation(object):
  def run(self, feed_dict=None, session=None):
    """Runs this operation in a `Session`.

    Calling this method will execute all preceding operations that
    produce the inputs needed for this operation.
    """
    _run_using_default_session(self, feed_dict, session)
\end{python}
\end{leftbar}

ここで、\code{\_run\_using\_default\_session}はデフォルトの\code{Session}を使用してその\ascii{OP}を実行します。

\begin{leftbar}
\begin{python}
def _run_using_default_session(operation, feed_dict, session=None):
  """Uses the default session to run "operation".
  """
  if session is None:
    session = get_default_session()
  session.run(operation, feed_dict)
\end{python}
\end{leftbar}

\subsection{Tensor}

グラフ構築時、\code{Tensor}はグラフ内でデータを保持せず、\code{Operation}の出力のシンボリックハンドルのみを表します。実際、\code{Tensor}が保持する実際のデータを得るには、\code{Session.run}による計算が必要です。

\subsubsection{プロデューサーとコンシューマー}

図\refig{py-tensor-producter-consumer}に示すように、\code{Tensor}は2つの\code{Operation}間のデータ交換の橋渡しをし、それらの間に典型的な\emph{プロデューサーとコンシューマー}の関係を構築します。上流の\code{Operation}はプロデューサーとして、ある種の抽象的な計算を経て\code{Tensor}を生成し、それをその上流\code{Operation}の出力の1つとし、\code{output\_index}を識別子として使用します。この\code{Tensor}は下流の\code{Operation}に渡され、下流\code{Operation}の入力となり、下流\code{Operation}はその\code{Tensor}のコンシューマーとなります。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/py-tensor-producter-consumer.png}
\caption{Tensor: プロデューサー-コンシューマー}
 \label{fig:py-tensor-producter-consumer}
\end{figure}

\subsubsection{ドメインモデル}

図\refig{py-tensor}に示すように、\code{Tensor}は\ascii{op}を通じてプロデューサーの役割を果たす\code{Operation}を保持し、\code{index}を使用してその\code{Tensor}がその\code{Operation}の出力リスト内のどのインデックスにあるかを示します。つまり、\code{op:index}の2つ組の情報を使用して、グラフ内で\code{Tensor}インスタンスを一意に識別できます。

さらに、\code{Tensor}は\code{Operation}のコンシューマーリストを保持し、その\code{Tensor}がどの\code{Operation}インスタンスに出力されたかを追跡するために使用されます。したがって、\code{Tensor}は計算グラフのエッジとして機能し、\code{Operation}間のデータ依存関係を構築します。

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/py-tensor.png}
\caption{ドメインオブジェクト：Tensor}
 \label{fig:py-tensor}
\end{figure}

\subsubsection{関連付けの確立}

最後に、\code{Operation}と\code{Tensor}の一部の実装を参照すると、両者間のプロデューサー-コンシューマーの関連関係を簡単に見つけることができます。\code{Tensor}リストが入力として\code{Operation}に流れ込むとき、下流の\code{Operation}と入力の\code{Tensor}リスト間の消費関係が確立されます。

\begin{leftbar}
\begin{python}
class Operation(object):
  def __init__(self, node_def, graph, inputs=None, output_types=None):
    # self(Operation) as consumer for input tensors.
    self._inputs = list(inputs)
    for a in self._inputs:
      a._add_consumer(self)

    # self(Operation) as producer for output tensors.
    self._output_types = output_types
    self._outputs = [Tensor(self, i, output_type)
                     for i, output_type in enumerate(output_types)]
\end{python}
\end{leftbar}

同様に、\code{Tensor}はコンストラクタ内で上流のプロデューサー\code{Operation}、およびその\code{Tensor}インスタンスがその\code{Operation}の\code{outputs}リスト内のインデックスを保持します。さらに、\code{\_add\_consumer}が呼び出されると、その下流の\code{Operation}がコンシューマーリストに追加されます。

\begin{leftbar}
\begin{python}
class Tensor(_TensorLike):
  def __init__(self, op, value_index, dtype):    
    # Index of the OP's endpoint that produces this tensor.
    self._op = op
    self._value_index = value_index
    
    # List of operations that use this Tensor as input.  
    # We maintain this list to easily navigate a computation graph.
    self._consumers = []

  def _add_consumer(self, consumer):
    if not isinstance(consumer, Operation):
      raise TypeError("Consumer must be an Operation: %s" % consumer)
    self._consumers.append(consumer)
\end{python}
\end{leftbar}

\subsubsection{属性セット}

\code{Tensor}を通じて上流の\code{Operation}をたどり、関連するメタデータを取得することができます。推測できるように、計算グラフのトラバーサルアルゴリズムは逆方向であり、トポロジカルソートアルゴリズムのトラバーサル方向とちょうど逆になります。ここで、\code{name}は\code{(node:output\_index)}の2つ組の情報を返し、計算グラフの範囲内でその\code{Tensor}インスタンスを一意に識別します。

\begin{leftbar}
\begin{python}
class Tensor(_TensorLike):
  @property
  def op(self):
    """The `Operation` that produces this tensor as an output."""
    return self._op

  @property
  def dtype(self):
    """The `DType` of elements in this tensor."""
    return self._dtype

  @property
  def graph(self):
    """The `Graph` that contains this tensor."""
    return self._op.graph

  @property
  def name(self):
    """The string name of this tensor."""
    return "%s:%d" % (self._op.name, self._value_index)

  @property
  def device(self):
    """The name of the device on which this tensor will be produced."""
    return self._op.device

  @property
  def shape(self):
    """Returns the `TensorShape` that represents the shape of this tensor.
    """
    return self._shape

  @property
  def value_index(self):
    """The index of this tensor in the outputs of its `Operation`."""
    return self._value_index
\end{python}
\end{leftbar}

\subsubsection{評価}

\begin{leftbar}
\begin{python}
class Tensor(_TensorLike):
  def eval(self, feed_dict=None, session=None):
    """Evaluates this tensor in a `Session`.

    Calling this method will execute all preceding operations that
    produce the inputs needed for the operation that produces this
    tensor.
    """
    return _eval_using_default_session(self, feed_dict, self.graph, session)
\end{python}
\end{leftbar}

ここで、\code{\_eval\_using\_default\_session}はデフォルトの\code{Session}を使用してその\ascii{Tensor}インスタンスを評価します。注意すべきは、\code{tf.Session.run}の\code{fetches}リストが\code{Operation, Tensor}インスタンスを混在して受け取ることができるということです。

\begin{leftbar}
\begin{python}
def _eval_using_default_session(tensors, feed_dict, graph, session=None):
  """Uses the default session to evaluate one or more tensors."""
  if session is None:
    session = get_default_session()
  return session.run(tensors, feed_dict)
\end{python}
\end{leftbar}

\subsection{TensorShape}

\code{Tensor}は\code{TensorShape}を使用してその形状情報を記述します。これはその\code{Tensor}のデータ型と\code{Dimension}リストを保持し、各\code{Dimension}はその次元のサイズを記述します。\code{TensorShape}と\code{Dimension}はともに値オブジェクトであり、カウント、マージ、互換性チェックなどのいくつかの実用的な数学的計算メソッドを含んでいます。

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/py-tensor-shape.png}
\caption{TensorShape}
 \label{fig:py-tensor-shape}
\end{figure}

明らかに、\code{TensorShape}を使用して\code{Tensor}に含まれる要素の数を計算することができます。

\begin{leftbar}
\begin{python}
class TensorShape(object):
  def num_elements(self):
    if self.is_fully_defined():
      size = 1
      for dim in self._dims:
        size *= dim.value
      return size
    else:
      return None
\end{python}
\end{leftbar} 

\subsubsection{ファクトリーメソッド}

いくつかの実用的なファクトリーメソッドがあり、\code{scalar, vector, matrix}はそれぞれ\ascii{0}次元、\ascii{1}次元、\ascii{2}次元の\code{TensorShape}インスタンスを構築するために使用されます。

\begin{leftbar}
\begin{python}
def scalar():
  return TensorShape([])

def vector(length):
  return TensorShape([length])

def matrix(rows, cols):
  return TensorShape([rows, cols])
\end{python}
\end{leftbar}

\subsubsection{部分定義}

計算グラフを構築する際、その\code{TensorShape}が一時的に決定できない場合、\code{None}を使用して表すことができます。2つの状況があり、\code{rank}のサイズが不明な場合、その\code{TensorShape}は不明と呼ばれます。\code{rank}のサイズが既知の場合、その\code{TensorShape}は\emph{部分定義}と呼ばれます。

\begin{leftbar}
\begin{python}
def unknown_shape(ndims=None):
  if ndims is None:
    return TensorShape(None)
  else:
    return TensorShape([Dimension(None)] * ndims)
\end{python}
\end{leftbar}

\subsubsection{完全定義}

逆に、\code{TensorShape}の各次元のサイズがすべて決定している場合、\emph{完全定義}と呼ばれます。

\begin{leftbar}
\begin{python}
class TensorShape(object):
  def is_fully_defined(self):
    return (self._dims is not None and all(dim.value is not None
                                           for dim in self._dims))
\end{python}
\end{leftbar}

\subsubsection{属性セット}

\code{ndims}属性を使用して\code{TensorShape}の\code{rank}サイズを返し、\code{dims}属性を使用して\code{Dimension}リストを返すことができます。

\begin{leftbar}
\begin{python}
class TensorShape(object):
  @property
  def dims(self):
    return self._dims

  @property
  def ndims(self):
    if self._dims is None:
      return None
    else:
      return len(self._dims)
\end{python}
\end{leftbar}

\subsubsection{変換}

\code{as\_proto}を使用して\code{TensorShapeProto}表現に変換することができます。特に、ある\code{Dimension}が不明な場合、シリアライズを可能にするために、\code{None}を\ascii{-1}に変換する必要があります。

\begin{leftbar}
\begin{python} 
class TensorShape(object):
  def _dims_as_proto(self): 
    def _size(dim):
      return -1 if dim.value is None else dim.value
    
    return [tensor_shape_pb2.TensorShapeProto.Dim(size=_size(d))
            for d in self._dims]

  def as_proto(self):
    if self._dims is None:
      return tensor_shape_pb2.TensorShapeProto(unknown_rank=True)
    else:
      return tensor_shape_pb2.TensorShapeProto(dim=self._dims_as_proto())
\end{python}
\end{leftbar}

また、\code{as\_list}を使用して\code{Dimension}リストに変換することもできます。\code{TensorShape}の\code{rank}サイズが不明な場合、\code{ValueError}例外が発生します。

\begin{leftbar}
\begin{python} 
class TensorShape(object):
  def as_list(self):
    if self._dims is None:
      raise ValueError("as_list() is not defined on an unknown TensorShape.")
    return [dim.value for dim in self._dims]
\end{python}
\end{leftbar}

逆に、\code{as\_shape}を使用して\code{Dimension}リストまたは\code{TensorShapeProto}を\code{TensorShape}インスタンスに変換します。

\begin{leftbar}
\begin{python}
def as_shape(shape):
  if isinstance(shape, TensorShape):
    return shape
  else:
    return TensorShape(shape)
\end{python}
\end{leftbar}

特に、\code{TensorShape}を構築する際、\code{TensorShapeProto}のある次元のサイズが\ascii{-1}の場合、それを\code{None}の表現に変換します。

\begin{leftbar}
\begin{python}
class TensorShape(object):
  def __init__(self, dims):
    if dims is None:
      self._dims = None
    elif isinstance(dims, tensor_shape_pb2.TensorShapeProto):
      if dims.unknown_rank:
        self._dims = None
      else:
        self._dims = [
          as_dimension(dim.size if dim.size != -1 else None)
          for dim in dims.dim
        ]
    elif isinstance(dims, TensorShape):
      self._dims = dims.dims
    else:
      try:
        dims_iter = iter(dims)
      except TypeError:
        # Treat as a singleton dimension
        self._dims = [as_dimension(dims)]
      else:
        # Got a list of dimensions
        self._dims = [as_dimension(d) for d in dims_iter]
\end{python}
\end{leftbar}

\subsection{Graph}

\code{Graph}はTensorFlowの最も重要なドメインオブジェクトです。TensorFlowのランタイムは\code{Graph}の構築、伝達、剪定、最適化、分割、実行を完了します。したがって、\code{Graph}のドメインモデルに精通することは、TensorFlowランタイム全体を理解する上で非常に有益です。

\subsubsection{ドメインモデル}

図\refig{py-graph}に示すように、1つの\code{Graph}オブジェクトは一連の\code{Operation}オブジェクトを含み、計算単位の集合を表します。同時に、間接的に一連の\code{Tensor}オブジェクトを保持し、データ単位の集合を表します。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/py-graph.png}
\caption{ドメインオブジェクト：Graph}
 \label{fig:py-graph}
\end{figure}

グラフ内のノード情報を素早くインデックス化するために、現在のグラフのスコープ内で各\code{Operation}に一意の\code{id}を割り当て、グラフ内に\code{\_nodes\_by\_id}のデータ辞書を格納します。同時に、ノードの名前に基づいてノード情報を素早くインデックス化できるように、グラフ内に\code{\_nodes\_by\_name}のデータ辞書も格納されています。

\begin{leftbar}
\begin{python}
class Graph(object):
  def __init__(self):
    self._lock = threading.Lock()
    self._nodes_by_id = dict()    # GUARDED\_BY(self.\_lock)
    self._next_id_counter = 0     # GUARDED\_BY(self.\_lock)
    self._nodes_by_name = dict()  # GUARDED\_BY(self.\_lock)
    self._version = 0             # GUARDED\_BY(self.\_lock)
\end{python}
\end{leftbar}

グラフ構築期間中、\code{OP}は\ascii{OP}コンストラクタを通じて作成され、最終的に現在の\code{Graph}インスタンスに追加されます。グラフが凍結されると、グラフにノードを追加することはできなくなり、\code{Graph}インスタンスをマルチスレッド環境で安全に共有できるようになります。

\begin{leftbar}
\begin{python}
class Graph(object):
  def _add_op(self, op):
    self._check_not_finalized()
    with self._lock:
      self._nodes_by_id[op._id] = op
      self._nodes_by_name[op.name] = op
      self._version = max(self._version, op._id)
\end{python}
\end{leftbar}


\subsubsection{グループ化}

\code{Graph}内のノードをより適切に管理するために、各\code{Operation}に特定のタグを付け、ノードの分類を実装しています。同じタイプのノードは同じ\code{Collection}にグループ化され、一意の\code{GraphKey}を使用してそのコレクションを識別します。その後、\code{GraphKey}に基づいて関連するノード情報を素早くインデックス化できます。システムには事前定義された一般的な\code{GraphKey}があり、同時にカスタム\code{GraphKey}もサポートしています。

\begin{leftbar}
\begin{python}
class GraphKeys(object):
  # Key to collect Variable objects that are global (shared across machines).
  # Default collection for all variables, except local ones.
  GLOBAL_VARIABLES = "variables"

  # Key to collect local variables that are local to the machine and are not
  # saved/restored.
  LOCAL_VARIABLES = "local_variables"

  # Key to collect model variables defined by layers.
  MODEL_VARIABLES = "model_variables"

  # Key to collect Variable objects that will be trained by the
  # optimizers.
  TRAINABLE_VARIABLES = "trainable_variables"

  # Key to collect summaries.
  SUMMARIES = "summaries"

  # Key to collect QueueRunners.
  QUEUE_RUNNERS = "queue_runners"

  # Key to collect table initializers.
  TABLE_INITIALIZERS = "table_initializer"

  # Key to collect asset filepaths. An asset represents an external resource
  # like a vocabulary file.
  ASSET_FILEPATHS = "asset_filepaths"

  # Key to collect Variable objects that keep moving averages.
  MOVING_AVERAGE_VARIABLES = "moving_average_variables"
  # Key to collect regularization losses at graph construction.

  REGULARIZATION_LOSSES = "regularization_losses"

  # Key to collect concatenated sharded variables.
  CONCATENATED_VARIABLES = "concatenated_variables"

  # Key to collect savers.
  SAVERS = "savers"

  # Key to collect weights
  WEIGHTS = "weights"

  # Key to collect biases
  BIASES = "biases"

  # Key to collect activations
  ACTIVATIONS = "activations"

  # Key to collect update\_ops
  UPDATE_OPS = "update_ops"

  # Key to collect losses
  LOSSES = "losses"

  # Key to collect BaseSaverBuilder.SaveableObject instances for checkpointing.
  SAVEABLE_OBJECTS = "saveable_objects"

  # Key to collect all shared resources used by the graph which need to be
  # initialized once per cluster.
  RESOURCES = "resources"

  # Key to collect all shared resources used in this graph which need to be
  # initialized once per session.
  LOCAL_RESOURCES = "local_resources"

  # Trainable resource-style variables.
  TRAINABLE_RESOURCE_VARIABLES = "trainable_resource_variables"

  # Key to indicate various ops.
  INIT_OP = "init_op"
  LOCAL_INIT_OP = "local_init_op"
  READY_OP = "ready_op"
  READY_FOR_LOCAL_INIT_OP = "ready_for_local_init_op"
  SUMMARY_OP = "summary_op"
  GLOBAL_STEP = "global_step"

  # Used to count the number of evaluations performed during a 
  # single evaluation run.
  EVAL_STEP = "eval_step"
  TRAIN_OP = "train_op"

  # Key for control flow context.
  COND_CONTEXT = "cond_context"
  WHILE_CONTEXT = "while_context"
\end{python}
\end{leftbar}

\code{Operation}が作成されるとき、特定のコレクションに分類して、後で\code{GraphKey}に基づいて素早くインデックス化できるようにすることができます。

\begin{leftbar}
\begin{python}
class Graph(object):
  def add_to_collection(self, name, value):
    self._check_not_finalized()
    with self._lock:
      if name not in self._collections:
        self._collections[name] = [value]
      else:
        self._collections[name].append(value)
\end{python}
\end{leftbar}

\subsubsection{グラフインスタンス}

一般的に、\ascii{OP}はグローバルで一意の、暗黙的な、デフォルトのグラフインスタンスに登録されます。特別に、TensorFlowは新しいグラフインスタンス\code{g}を明示的に作成し、\code{g.as\_default()}を呼び出して、それを現在のスレッドで唯一のデフォルトのグラフインスタンスにすることもできます。このコンテキストマネージャ内で作成されたすべての\ascii{OP}は、自動的にそのグラフインスタンスに登録されます。

\begin{leftbar}
\begin{python}
with tf.Graph().as_default() as g:
  c = tf.constant(5.0)
  assert c.graph is g
\end{python}
\end{leftbar}

実際、\code{g.as\_default}は現在のスレッドのグラフスタックからコンテキストマネージャを返し、現在のグラフインスタンス\code{g}が元のデフォルトのグラフインスタンスを上書きするようにします。コンテキストマネージャを終了すると、元のデフォルトのグラフインスタンスが復元されます。ただし、任意の時点で、現在のスレッドには1つのグラフインスタンスのみが\emph{デフォルト}になります。\code{tf.get\_default\_graph()}を呼び出すと、そのデフォルトのグラフインスタンスが返されます。

\begin{leftbar}
\begin{python}
class _DefaultStack(threading.local):
  """A thread-local stack for providing implicit defaults."""

  def __init__(self):
    super(_DefaultStack, self).__init__()
    self.stack = []

  def get_default(self):
    return self.stack[-1] if len(self.stack) >= 1 else None

  @tf_contextlib.contextmanager
  def get_controller(self, default):
    """A context manager for manipulating a default stack."""
    try:
      self.stack.append(default)
      yield default
    finally:
      self.stack.remove(default)

class _DefaultGraphStack(_DefaultStack):
  """A thread-local stack for providing an implicit default graph."""

  def __init__(self):
    super(_DefaultGraphStack, self).__init__()
    self._global_default_graph = None

  def get_default(self):
    """Override that returns a global default if the stack is empty."""
    ret = super(_DefaultGraphStack, self).get_default()
    if ret is None:
      ret = self._GetGlobalDefaultGraph()
    return ret

  def _GetGlobalDefaultGraph(self):
    if self._global_default_graph is None:
      self._global_default_graph = Graph()
    return self._global_default_graph
\end{python}
\end{leftbar}

ここで、\code{\_DefaultStack}の\code{get\_controller}はスタックの先頭に新しいグラフインスタンスを追加します。コンテキストマネージャを終了すると、そのグラフインスタンスがスタックの先頭から削除され、以前のグラフインスタンスが復元されます。\code{get\_default}が呼び出されたとき、スタックが空の場合、\code{\_GetGlobalDefaultGraph}を通じてグローバルで一意の、暗黙的なグラフインスタンスが返されます。ほとんどのTensorFlowプログラムでは、複数のグラフインスタンスを明示的に作成していない場合、すべての\ascii{OP}はデフォルトでこのグラフインスタンスに登録されます。

\begin{leftbar}
\begin{python}
_default_graph_stack = _DefaultGraphStack()

def get_default_graph():
  """Returns the default graph for the current thread."""
  return _default_graph_stack.get_default()

class Graph(object):
  def as_default(self):
    """Returns a context manager that makes this `Graph` the default graph."""
    return _default_graph_stack.get_controller(self)
\end{python}
\end{leftbar}

\subsubsection{名前空間}

グラフ内のノードをより適切に管理するために、\code{name\_scope}を使用してグラフ内のノードに階層的な名前付けを実施します。例えば、宇宙全体で故宮の位置を特定したい場合、次のような階層的な名前付けを実施できます：宇宙/銀河系/太陽系/地球/中国/北京/故宮。これは\ascii{TensorBoard}で計算グラフを可視化する際に非常に役立ちます。計算グラフが比較的大きい場合、折りたたみ方式でグラフを表示したり、展開して詳細を見ることができます。

内部の\code{name\_scope}は外部の\code{name\_scope}を継承します。内部の\code{name\_scope}が\code{/}で終わる場合、指定された\code{name\_scope}にリセットされます。内部の\code{name\_scope}が空の文字列または\code{None}の場合、\code{name\_scope}全体がリセットされます。

\begin{leftbar}
\begin{python}
with tf.Graph().as_default() as g:
  with g.name_scope("nested") as scope:
    nested_c = tf.constant(10.0, name="c")
    assert nested_c.op.name == "nested/c"

    # Create a nested scope called "inner".
    with g.name_scope("inner"):
      nested_inner_c = tf.constant(30.0, name="c")
      assert nested_inner_c.op.name == "nested/inner/c"

      # Treats `scope` as an absolute name scope, 
      # and switches to the "nested/" scope.
      with g.name_scope(scope):
        nested_d = tf.constant(40.0, name="d")
        assert nested_d.op.name == "nested/d"

        # reset name scope
        with g.name_scope(""):
          e = tf.constant(50.0, name="e")
          assert e.op.name == "e"
\end{python}
\end{leftbar}

実際、\code{name\_scope}はコンテキストマネージャであり、ネストされた\code{name\_scope}内で\code{name\_scope}のスタック管理を実装しています。\code{name\_scope}がスコープを出ると、自動的に外部の\code{name\_scope}が復元されます。

\begin{leftbar}
\begin{python}
def _name_from_scope_name(name):
  return name[:-1] if name[-1] == "/" else name

class Graph(object):
  def __init__(self):
    self._name_stack = ""

  @tf_contextlib.contextmanager
  def name_scope(self, name):
    try:
      old_stack = self._name_stack
      if not name:
        new_stack = None
      elif name and name[-1] == "/":
        new_stack = _name_from_scope_name(name)
      else:
        new_stack = self.unique_name(name)
      self._name_stack = new_stack
      yield "" if new_stack is None else new_stack + "/"
    finally:
      self._name_stack = old_stack
\end{python}
\end{leftbar}

グラフ構築時、\ascii{OP}コンストラクタはより一般的に\code{tf.name\_scope}を使用します。これは入力の\code{Operation}または\code{Tensor}リストからグラフインスタンスを取得しようとします。取得できない場合は、デフォルトのグラフインスタンスを返します。その後、そのグラフインスタンスに新しい\code{name\_scope}を追加します。

\begin{leftbar}
\begin{python}
@tf_contextlib.contextmanager
def name_scope(name, default_name=None, values=[]):
  n = default_name if name is None else name
  g = _get_graph_from_inputs(values)
  with g.as_default(), g.name_scope(n) as scope:
    yield scope
\end{python}
\end{leftbar}

\subsubsection{制御依存関係}

内部の\code{control\_dependencies}を使用して外部の\code{control\_dependencies}を合併したり、\code{None}を使用して制御依存関係のセットを空にリセットしたりすることができます。

\begin{leftbar}
\begin{python}
with g.control_dependencies([a, b]):
  # Ops constructed here run after `a` and `b`.
  with g.control_dependencies(None):
    # Ops constructed here not waiting for either `a` or `b`.
    with g.control_dependencies([c, d]):
      # Ops constructed here run after `c` and `d`, 
      # also not waiting for either `a` or `b`.
  with g.control_dependencies([e, f]):
    # Ops constructed here run after `a, b, e, f`.
\end{python}
\end{leftbar}

実際、\code{control\_dependencies}は\ascii{OP}の制御依存関係を指定するためのコンテキストマネージャを返します。ここで、\code{control\_ops}は現在のレベルが依存する\ascii{Operation}のリストを記録し、\code{current}は現在のレベルとその外部が依存するすべての\ascii{Operation}のリストを記録します。

\begin{leftbar}
\begin{python}
class Graph(object):
  def control_dependencies(self, control_inputs):
    if control_inputs is None:
      return self._ControlDependenciesController(self, None)

    control_ops = []
    current = self._current_control_dependencies()
    for c in control_inputs:
      c = self.as_graph_element(c)
      if isinstance(c, Tensor):
        c = c.op
      if c not in current:
        control_ops.append(c)
        current.add(c)
    return self._ControlDependenciesController(self, control_ops)
\end{python}
\end{leftbar}

\code{\_ControlDependenciesController}は制御依存関係のコントローラを実装します。\code{control\_inputs}が\code{None}の場合、新しいスコープが有効になり、新しいスタックが古いスタックを置き換えます。現在のコンテキストのスコープを終了すると、以前の古いスタックが復元され、以前のすべての制御依存関係がクリアされます。そうでない場合、\code{control\_inputs}の各レイヤーに入るたびに、現在のスコープが現在のスタックに追加されます。

\begin{leftbar}
\begin{python}
class Graph(object):
  def __init__(self):
    self._control_dependencies_stack = []

  def _push_control_dependencies_controller(self, controller):
    self._control_dependencies_stack.append(controller)

  def _pop_control_dependencies_controller(self):
    self._control_dependencies_stack.pop()

  class _ControlDependenciesController(object):
    """Context manager for `control\_dependencies()`."""

    def __init__(self, graph, control_inputs):
      self._graph = graph
      if control_inputs is None:
        self._control_inputs = []
        self._new_stack = True
      else:
        self._control_inputs = control_inputs
        self._new_stack = False
      self._seen_nodes = set()
      self._old_stack = None

    def __enter__(self):
      if self._new_stack:
        # Clear the control\_dependencies.
        self._old_stack = self._graph._control_dependencies_stack
        self._graph._control_dependencies_stack = []
      self._graph._push_control_dependencies_controller(self)

    def __exit__(self, unused_type, unused_value, unused_traceback):
      self._graph._pop_control_dependencies_controller()
      if self._new_stack:
        self._graph._control_dependencies_stack = self._old_stack
\end{python}
\end{leftbar}

\code{\_current\_control\_dependencies}は、現在のレベルが依存するすべての外部の\code{control\_inputs}を\ascii{Operation}のリストに縮約するために使用されます。

\begin{leftbar}
\begin{python}
class Graph(object):
  def _current_control_dependencies(self):
    ret = set()
    for controller in self._control_dependencies_stack:
      for op in controller.control_inputs:
        ret.add(op)
    return ret
\end{python}
\end{leftbar}

\subsubsection{コンテナ}

\begin{leftbar}
\begin{python}
with g.container('experiment0'):
  # All stateful Operations constructed in this context will be placed
  # in resource container "experiment0".
  v1 = tf.Variable([1.0])
  v2 = tf.Variable([2.0])
  with g.container("experiment1"):
    # All stateful Operations constructed in this context will be
    # placed in resource container "experiment1".
    v3 = tf.Variable([3.0])
    q1 = tf.FIFOQueue(10, tf.float32)
  # All stateful Operations constructed in this context will be
  # be created in the "experiment0".
  v4 = tf.Variable([4.0])
  q1 = tf.FIFOQueue(20, tf.float32)
  with g.container(""):
    # All stateful Operations constructed in this context will be
    # be placed in the default resource container.
    v5 = tf.Variable([5.0])
    q3 = tf.FIFOQueue(30, tf.float32)

# Resets container "experiment0", after which the state of v1, v2, v4, q1
# will become undefined (such as uninitialized).
tf.Session.reset(target, ["experiment0"])
\end{python}
\end{leftbar}

\begin{leftbar}
\begin{python}
class Graph(object):
  @tf_contextlib.contextmanager
  def container(self, container_name):
    """Returns a context manager that specifies the resource container."""
    original_container = self._container
    try:
      self._container = container_name
      yield self._container
    finally:
      self._container = original_container
\end{python}
\end{leftbar}

\subsection{グラフ構築}

計算グラフの構築中、\ascii{OP}の計算は実行されません。簡単に言えば、グラフの構築プロセスは\ascii{OP}コンストラクタに基づいて\code{Operation}インスタンスの構築を完了することです。\code{Operation}インスタンスの構築前に、\code{OpDef}と\code{NodeDef}の構築プロセスを完了する必要があります。

\subsubsection{OpDefリポジトリ}

\code{OpDef}リポジトリは、システムが最初にアクセスされたときに\code{OpDef}の遅延ロードと登録を実装します。つまり、特定のタイプの\code{OpDef}リポジトリに対して、\code{\_InitOpDefLibrary}モジュールが最初にインポートされるときに、\code{op\_list\_ascii}で表現されるすべての\ascii{OP}をスキャンし、それらを\ascii{Protobuf}形式の\code{OpList}インスタンスに変換し、最終的に\code{OpDefLibrary}インスタンスに登録します。

例えば、モジュール\code{gen\_array\_ops}はビルド時に自動生成され、主にすべての\code{array\_ops}タイプの\code{OpDef}の定義を完了し、自動的に\code{OpDefLibrary}のリポジトリインスタンスに登録し、名前による\code{OpDef}の検索サービスインターフェースを提供します。

\begin{leftbar}
\begin{python}
_op_def_lib = _InitOpDefLibrary()

def _InitOpDefLibrary():
  op_list = _op_def_pb2.OpList()
  _text_format.Merge(_InitOpDefLibrary.op_list_ascii, op_list)   
  op_def_lib = _op_def_library.OpDefLibrary()
  op_def_lib.add_op_list(op_list)
  return op_def_lib

_InitOpDefLibrary.op_list_ascii = """op {
  name: "ZerosLike"
  input_arg {
    name: "x"
    type_attr: "T"
  }
  output_arg {
    name: "y"
    type_attr: "T"
  }
  attr {
    name: "T"
    type: "type"
  }
}
# ignore others
"""
\end{python}
\end{leftbar}

\subsubsection{ファクトリーメソッド}

図\refig{py-op-factory-and-repo}に示すように、\ascii{Client}が\ascii{OP}コンストラクタを使用して\code{Operation}インスタンスを作成すると、最終的に\code{Graph.create\_op}メソッドが呼び出され、その\code{Operation}インスタンスがそのグラフインスタンスに登録されます。

つまり、一方で\code{Graph}は\code{Operation}のファクトリーとして機能し、\code{Operation}の作成責任を負います。他方で、\code{Graph}は\code{Operation}のリポジトリとして機能し、\code{Operation}の保存、検索、変換などの操作を担当します。

このプロセスは一般に計算グラフの構築と呼ばれます。計算グラフの構築中、ランタイムの\ascii{OP}計算はトリガーされず、計算ノード間の依存関係を記述し、\ascii{DAG}グラフを構築し、計算プロセス全体を全体的に計画するだけです。

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/py-op-factory-and-repo.png}
\caption{Graph: OPファクトリー + OPリポジトリ}
 \label{fig:py-op-factory-and-repo}
\end{figure}

\subsubsection{OPコンストラクタ}

図\refig{py-op-constructor}に示すように、グラフ構築時に\ascii{Client}は\code{tf.zeros\_like}を使用して\code{ZerosLike}という名前の\ascii{OP}を構築します。この\ascii{OP}は1つの入力を持ち、すべて\ascii{0}の\ascii{Tensor}を出力します。\code{tf.zeros\_like}は一般に\ascii{OP}コンストラクタと呼ばれます。

次に、\ascii{OP}コンストラクタは自動生成されたコードを呼び出し、さらに\code{OpDefLibrary.apply\_op}メソッドを呼び出します。

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/py-op-constructor.png}
\caption{OPコンストラクタとコード生成器}
 \label{fig:py-op-constructor}
\end{figure}

\subsubsection{OpDefとNodeDefの構築}

次に、図\refig{py-graph-create-op}に示すように、\code{OpDefLibrary}は\ascii{OP}の名前に基づいて\code{OpDefLibrary}から対応する\code{OpDef}インスタンスを見つけます。最終的に、\code{Graph.create\_op}のファクトリーメソッドを通じて\code{NodeDef}インスタンスを作成し、さらに\code{Operation}インスタンスを作成して、それ自身をグラフインスタンスに登録します。

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/py-graph-create-op.png}
\caption{Operationインスタンスの作成: OpDef、NodeDefインスタンスの作成}
 \label{fig:py-graph-create-op}
\end{figure}

\end{content}

\section{バックエンドC++}

\begin{content}

C++バックエンドでは、計算グラフはTensorFlowドメインモデルの中核です。

\subsection{エッジ}

\code{Edge}は前任ノードと後任ノードを保持し、これにより計算グラフの接続を実現します。ノードはゼロ本または複数本の入力エッジと、ゼロ本または複数本の出力エッジを持つことができます。一般に、計算グラフには2種類のエッジが存在します：

\begin{enum}
  \eitem{通常エッジ：データ（\code{Tensor}で表現）を運び、ノード間の「生産者-消費者」データ依存関係を表し、通常は実線で表されます。}
  \eitem{制御依存：データを運ばず、ノード間の実行依存関係を表すために使用され、通常は点線で表されます。} 
\end{enum}

\subsubsection{2つの識別子}

\ascii{Edge}は2つの重要なインデックスを保持しています：

\begin{enum}
  \eitem{\code{src\_output}：このエッジが「前任ノード」の第\code{src\_output}番目の出力エッジであることを示します。}
  \eitem{\code{dst\_input}：このエッジが「後任ノード」の第\code{dst\_input}番目の入力エッジであることを示します。} 
\end{enum}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/cc-edge-model.png}
\caption{ドメインオブジェクト：Edge}
 \label{fig:cc-edge-model}
\end{figure}

例えば、2つの前任ノード\code{s1, s2}があり、それぞれ2本の出力エッジを持ち、2つの後任ノード\code{d1, d2}があり、それぞれ2本の入力エッジを持つとします。

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/cc-edge-model-example.png}
\caption{エッジの例}
 \label{fig:cc-edge-model-example}
\end{figure}

\subsubsection{制御依存}

制御依存エッジの場合、その\code{src\_output, dst\_input}はともに\code{-1(Graph::kControlSlot)}となり、制御依存エッジがデータを運ばないことを暗示しています。

\begin{leftbar}
\begin{c++}
bool Edge::IsControlEdge() const {
   // or dst\_input\_ == Graph::kControlSlot;
   return src_output_ == Graph::kControlSlot;
}
\end{c++}
\end{leftbar}

\subsubsection{Tensor識別子}

一般に、計算グラフの「通常エッジ」は\code{Tensor}を運び、\code{TensorId}で識別されます。\code{Tensor}識別子はソースノードの名前とそのエッジの\code{src\_output}によって一意に決定されます。

\begin{leftbar}
\begin{c++}
TensorId ::= node_name:src_output
\end{c++}
\end{leftbar}

デフォルトでは、\code{src\_output}は\ascii{0}とされます。つまり、\code{node\_name}と\code{node\_name:0}は同等です。特別に、\code{src\_output}が\ascii{-1}の場合、そのエッジは「制御依存エッジ」であることを示し、\code{TensorId}は\code{\^node\_name}と表記でき、このエッジが\code{node\_name}ノードに依存していることを示します。

\subsection{ノード}

\code{Node}（ノード）はゼロ本または複数本の入力/出力エッジを持つことができ、\code{in\_edges, out\_edges}でそれぞれ入力エッジと出力エッジの集合を表します。さらに、\code{Node}は\code{NodeDef, OpDef}を保持します。\code{NodeDef}にはデバイス割り当て情報と\ascii{OP}の属性値リストが含まれ、\code{OpDef}は\ascii{OP}のメタデータ（\ascii{OP}の入力出力タイプなどの情報）を保持します。

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/cc-node-model.png}
\caption{ドメインオブジェクト：Node}
 \label{fig:cc-node-model}
\end{figure}

\subsubsection{入力エッジ}

入力エッジの集合内では、インデックス\code{(dst\_input)}に従って線形検索を行うことができます。ノードの入力エッジが多い場合、これがパフォーマンスのボトルネックになる可能性があります。同様に、インデックス\code{(src\_output)}に従って出力エッジを検索する場合も、アルゴリズムは同じです。

\begin{leftbar}
\begin{c++}
Status Node::input_edge(int idx, const Edge** e) const {
  for (auto edge : in_edges()) {
    if (edge->dst_input() == idx) {
      *e = edge;
      return Status::OK();
    }
  }
  return errors::NotFound("not found input edge ", idx);
}
\end{c++}
\end{leftbar}

\subsubsection{前任ノード}

まず\code{idx}インデックスを使って入力エッジを見つけ、次にその入力エッジから前任ノードを見つけます。同様に、インデックスに従って後任ノードを検索する場合も、アルゴリズムは同じです。

\begin{leftbar}
\begin{c++}
Status Node::input_node(int idx, const Node** n) const {
  const Edge* e = nullptr;
  TF_RETURN_IF_ERROR(input_edge(idx, &e));
  *n = e == nullptr ? nullptr : e->src();
  return Status::OK();
}
\end{c++}
\end{leftbar}

\subsection{グラフ}

\code{Graph}（計算グラフ）はノードとエッジの集合です。計算グラフはDAGグラフであり、計算グラフの実行プロセスはDAGのトポロジカルソートに従って、順次OPの計算を開始します。入次数が\ascii{0}の複数のノードが存在する場合、TensorFlowランタイムは並行実行を実現し、複数のOPの計算を同時に実行して実行効率を高めることができます。

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/cc-graph-model.png}
\caption{ドメインモデル：グラフ}
 \label{fig:cc-graph-model}
\end{figure}

\subsubsection{空のグラフ}

計算グラフの初期状態は、必ずしも空のグラフではありません。実際には、2つの特別なノードが追加されています：\code{Source}ノードと\code{Sink}ノードで、それぞれDAGグラフの開始ノードと終了ノードを表します。\code{Source}の\code{id}は\ascii{0}、\code{Sink}の\code{id}は\ascii{1}です。したがって、通常のOPノードの\ascii{id}は\ascii{1}より大きくなります。

\code{Source}と\code{Sink}の間は「制御依存」エッジで接続されており、計算グラフの実行が\code{Source}ノードから始まり\code{Sink}ノードで終わることを保証します。これらの間の制御依存エッジの\code{src\_output, dst\_input}値はともに\ascii{-1}です。

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/cc-empty-graph.png}
\caption{空のグラフ}
 \label{fig:cc-empty-graph}
\end{figure}

\code{Source}と\code{Sink}は内部実装で予約されている2つのノードで、ノード名はアンダースコアで始まり、それぞれ\code{\_SOURCE}と\code{\_SINK}という名前が使用されます。また、どちらも\code{NoOp}であり、何の計算も実行しないことを示しています。

\begin{leftbar}
\begin{c++}
Node* Graph::AddInternalNode(const char* name, int id) {
  NodeDef def;
  def.set_name(name);
  def.set_op("NoOp");

  Status status;
  Node* node = AddNode(def, &status);
  TF_CHECK_OK(status);
  CHECK_EQ(node->id(), id);
  return node;
}

Graph::Graph(const OpRegistryInterface* ops)
    : ops_(ops), arena_(8 << 10 /* 8kB */) {
  auto src  = AddInternalNode("_SOURCE", kSourceId);
  auto sink = AddInternalNode("_SINK",   kSinkId);
  AddControlEdge(src, sink);
}
\end{c++}
\end{leftbar}

慣例上、\code{Source}と\code{Sink}ノードのみを含む計算グラフも空のグラフと呼ばれることがあります。

\subsubsection{非空のグラフ}

フロントエンドでは、ユーザーはOPコンストラクタを使用して任意の複雑さの計算グラフを構築できます。ランタイムでは、ユーザーが構築した計算グラフを制御依存エッジを通じて\code{Source/Sink}ノードに接続し、計算グラフの実行が\code{Source}ノードから始まり\code{Sink}ノードで終わることを保証します。

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/cc-non-empty-graph.png}
\caption{非空のグラフ}
 \label{fig:cc-non-empty-graph}
\end{figure}

\subsubsection{エッジの追加}

計算グラフの構築プロセスは非常にシンプルで、まず\code{Graph::AddNode}を使ってグラフにノードを配置し、次に\code{Graph::AddEdge}を使ってグラフにエッジを配置してノード間の接続を実現します。

\begin{leftbar}
\begin{c++}
const Edge* Graph::AllocEdge() const {
  Edge* e = nullptr;
  if (free_edges_.empty()) {
    e = new (arena_.Alloc(sizeof(Edge))) Edge;
  } else {
    e = free_edges_.back();
    free_edges_.pop_back();
  }
  e->id_ = edges_.size();
  return e;
}

const Edge* Graph::AddEdge(Node* source, int x, Node* dest, int y) {
  auto e = AllocEdge();
  e->src_ = source;
  e->dst_ = dest;
  e->src_output_ = x;
  e->dst_input_ = y;

  CHECK(source->out_edges_.insert(e).second);
  CHECK(dest->in_edges_.insert(e).second);

  edges_.push_back(e);
  edge_set_.insert(e);
  return e;
}
\end{c++}
\end{leftbar}

\subsubsection{制御依存エッジの追加}

制御依存エッジの追加は、\code{Graph::AddEdge}を転送呼び出しすることで実装できます。この時、\code{src\_output, dst\_input}はともに\ascii{-1}になります。

\begin{leftbar}
\begin{c++}
const Edge* Graph::AddControlEdge(Node* src, Node* dst) {
  return AddEdge(src, kControlSlot, dst, kControlSlot);
}
\end{c++}
\end{leftbar}

\subsection{OpDefリポジトリ}

同様に、C++システムの\code{OpDef}リポジトリは\code{main}関数が開始する前に\code{OpDef}のロードと登録を完了します。これは\ascii{REGISTER\_OP}マクロを使用して\code{OpDef}の登録を完了します。

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/cc-op-repo.png}
\caption{OpDefの登録：REGISTER\_OPの使用}
 \label{fig:cc-op-repo}
\end{figure}

\end{content}

\section{グラフの伝達}

\begin{content}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{figures/py-graph-creation.png}
\caption{グラフのシリアライズとデシリアライズ}
 \label{fig:py-graph-creation}
\end{figure}

\end{content}
