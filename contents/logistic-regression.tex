\begin{savequote}[45mm]
\ascii{Any fool can write code that a computer can understand. Good programmers write code that humans can understand.}
\qauthor{\ascii{- Martin Flower}}
\end{savequote}

\chapter{線形モデル}
\label{ch:linear-model}

\section{ロジスティック回帰}

\begin{content}

ロジスティック回帰（\ascii{Logistic Regression}）では、$y$は実数値で、$y \in \{ 0,1\}$です。しかし、ロジスティック回帰は二値分類問題を解決します。一般的に、$y \ge 0.5$なら正クラスと判定し、そうでなければ負クラスと判定します。

\[c = \left\{ \begin{gathered}
  1,y \ge 0.5 \hfill \\
  0,y < 0.5 \hfill \\ 
\end{gathered}  \right.\]

また、よく\emph{指示関数}を使用して上式を表現します。

\[
c = \mathbb I(y \ge 0.5)
\]

ここで、$\mathbb I(true) = 1$、そうでなければ$\mathbb I(false) = 0$です。

\subsection{記号の定義}

ロジスティック回帰問題を形式的に記述するために、ここでいくつかの一般的な記号を定義します。予測値とラベル値の記号を区別するために、$y, t$をそれぞれ使用することに注意してください。

 \begin{itemize}
   \item \ascii{訓練サンプルセット}: $ S = \{ ({x^{(i)}},{t^{(i)}});i = 1,2,...,m\} $
   \item \ascii{第$i$個の訓練サンプル}: $ ({x^{(i)}},{t^{(i)}}) $
   \item \ascii{サンプル入力}: $ x = ({x_1},{x_2},...,{x_n})^{T}  \in {\mathbb{R}^n} $
   \item \ascii{サンプルラベル}: $ t \in {\mathbb{R}} $
   \item \ascii{予測値}: $ y \in {\mathbb{R}} $   
   \item \ascii{誤差値}: $ e = y - t \in {\mathbb{R}} $
   \item \ascii{重み}: $ w \in {\mathbb{R}^{n}} $   
   \item \ascii{バイアス}: $ b \in {\mathbb{R}} $
   \item \ascii{線形重み付け和}: $ z = w^Tx + b \in {\mathbb{R}} $   
   \item \ascii{活性化関数}: $ f(z) = \frac{1}{{1 + {e^{ - z}}}} \in {[0, 1]} $
   \item \ascii{正則化項}: $ R(w) \in {\mathbb{R}} $
   \item \ascii{L2正則化項}: $ \parallel w\parallel _2^2 \in {\mathbb{R}} $
   \item \ascii{損失関数（正則化項なし）}: $ L(w, b) \in {\mathbb{R}} $
   \item \ascii{損失関数（正則化項あり）}: $ J(w, b) = L(w, b) + \lambda R(w)\in {\mathbb{R}} $
   \item \ascii{勾配関数}: $ {\nabla _w}L( {w,b}) \in {\mathbb{R}^n} $
 \end{itemize}

\subsection{モデルの定義}

図\refig{logistic-regression-nn}に示すように、ロジスティック回帰は1つのニューロンだけを持つネットワークモデルと等価です。

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/logistic-regression-nn.png}
\caption{ロジスティック回帰：ニューロンモデル}
 \label{fig:logistic-regression-nn}
\end{figure}

まず、ロジスティック回帰は$z = {w^T}x + b$の線形重み付け和を完了します。次に、\ascii{sigmoid}の活性化関数を使用して、非線形変換を完了します。

\[\begin{aligned}
  y =  & {h_{w,b}}(x) \\ 
   =  & f({w^T}x + b) \\ 
   =  & \frac{1}{{1 + {e^{ - {w^T}x + b}}}} \\ 
\end{aligned} \]

$w$はモデルの重みを表し、$n$次元のベクトルです。$b$はニューロンのバイアスを表し、スカラーです。

\[\begin{gathered}
  w = \left[ {\begin{array}{*{20}{c}}
  {{w_1}} \\ 
  {{w_2}} \\ 
   \vdots  \\ 
  {{w_n}} 
\end{array}} \right] \in {\mathbb{R}^n} \\ 
  b \in \mathbb{R} \\ 
\end{gathered} \]

$x$はモデルの入力を表し、$n$次元のベクトルです。ここで、$\forall {x_i} \in x,i = 1,2,...,n$は、入力ベクトル$x$の1つの特徴値を表します。$y$はモデルの出力を表し、スカラーです。

\[\begin{gathered}
  x = \left[ {\begin{array}{*{20}{c}}
  {{x_1}} \\ 
  {{x_2}} \\ 
   \vdots  \\ 
  {{x_n}} 
\end{array}} \right] \in {\mathbb{R}^n} \\ 
  {\text{y}} \in {\mathbb{R}} \\ 
\end{gathered}\]

$f(z)$はニューロンの活性化関数であり、ここでは\ascii{sigmoid}関数を使用します。

\subsection{Sigmoid関数}

\ascii{sigmoid}関数は無限の定義域空間を$[0, 1]$の値域空間に圧縮するため、自然な確率解釈の意味を持っています。

\[
f(z) = \frac{1}{{1 + {e^{ - z}}}}
\]

\subsubsection{ソフト飽和性}

図\refig{sigmoid}に示すように、\ascii{sigmoid}関数はソフト飽和性を持っています。つまり、

\[\begin{gathered}
  \mathop {\lim }\limits_{z \to  + \infty } f(z) = 1 \hfill \\
  \mathop {\lim }\limits_{z \to  - \infty } f(z) = 0 \hfill \\ 
\end{gathered} \]

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{figures/sigmoid.png}
\caption{sigmoid関数}
 \label{fig:sigmoid}
\end{figure}

\subsubsection{導関数}

\ascii{sigmoid}の導関数は特殊な表現形式を持っており、その導関数は$y$の2次関数です。

\[
y = \frac{1}{{1 + {e^{ - z}}}}
\]

連鎖律を用いると、\ascii{sigmoid}の導関数を容易に導出できます。

\[\begin{aligned}
  y' =  & \frac{d}{{dz}}\left( {\frac{1}{{1 + {e^{ - z}}}}} \right) \\ 
   =  & \frac{d}{{dz}}{\left( {1 + {e^{ - z}}} \right)^{ - 1}} \\ 
   =  &  - {\left( {1 + {e^{ - z}}} \right)^{ - 2}}\frac{d}{{dz}}\left( {{e^{ - z}}} \right) \\ 
   =  &  - {\left( {1 + {e^{ - z}}} \right)^{ - 2}}{e^{ - z}}\frac{d}{{dz}}\left( { - z} \right) \\ 
   =  &  - {\left( {1 + {e^{ - z}}} \right)^{ - 2}}{e^{ - z}}\left( { - 1} \right) \\ 
   =  & \frac{{{e^{ - z}}}}{{{{\left( {1 + {e^{ - z}}} \right)}^2}}} \\ 
   =  & \frac{1}{{1 + {e^{ - z}}}}\left( {1 - \frac{1}{{1 + {e^{ - z}}}}} \right) \\ 
   =  & y(1 - y) \\ 
\end{aligned} \]

\subsection{確率的解釈}

任意の$(x,t) \in S$に対して、\ascii{Sigmoid}の関数特性に基づいて、$t|x$が$Bernoulli(y)$の確率分布に従うと仮定します。

\[\begin{aligned}
  P(t = 1|x;w,b) = & y \\ 
  P(t = 0|x;w,b) = & 1 - y \\ 
\end{aligned} \]

周知のように、$Bernoulli(y)$確率分布は以下のようなより簡潔な表現方法で記述できます。

\[P(t|x;w,b) = {y^t}{(1 - y)^{1 - t}}\]

さらに一般化すると、$m$個の独立同分布の訓練サンプル$\{ ({x^{(i)}},{t^{(i)}});i = 1,2,...,m\}$が存在すると仮定すると、$(w,b)$をパラメータとする尤度関数は以下のように表現できます：

\[\begin{aligned}
  L(w,b) =  & P\left( {\vec y|X} \right) \\ 
   =  & \prod\limits_{i = 1}^m {P\left( {{t^{(i)}}|{x^{(i)}};w,b} \right)}  \\ 
   =  & \prod\limits_{i = 1}^m {{{\left( {{y^{(i)}}} \right)}^{{t^{(i)}}}}{{\left( {1 - {y^{(i)}}} \right)}^{1 - {t^{(i)}}}}}  \\ 
\end{aligned} \]

ここで、
${y^{(i)}} = {h_{w,b}}({x^{(i)}}), i=1,2,...,m $です。一般的に、対数関数の単調増加性に基づいて、これを対数尤度関数に変換し、積の演算を和の演算に変換することで、問題の複雑さを簡略化します。

\[l(w,b) = \log L(w,b) = \sum\limits_{i = 1}^m {{t^{(i)}}\log {y^{(i)}} + \left( {1 - {t^{(i)}}} \right)\log \left( {1 - {y^{(i)}}} \right)} \]

したがって、ロジスティック回帰問題は対数尤度関数を最大化するパラメータ推定問題に変換できます。

\[\hat w,\hat b = \arg \mathop {\max }\limits_{w,b} \sum\limits_{i = 1}^m {{t^{(i)}}\log {y^{(i)}} + \left( {1 - {t^{(i)}}} \right)\log \left( {1 - {y^{(i)}}} \right)} \]

勾配上昇法の最適化手法を使用して、最適なパラメータ$(\hat w,\hat b)$を反復的に求めることができます。

\subsection{交差エントロピー損失関数}

機械学習の分野では、よく\emph{損失関数}を使用し、\emph{勾配降下法}の最適化アルゴリズムを用いて、最適なパラメータ$(\hat w,\hat b)$を反復的に求めます。前節の導出に基づいて、与えられた訓練サンプル$(x, t)$に対して、ロジスティック回帰は以下のように定義された損失関数を使用できます。これは二値分類問題における交差エントロピー損失関数の特殊な形式です。

\[L(w,b;x,t) =  - \left\{ {t\log y + \left( {1 - t} \right)\log \left( {1 - y} \right)} \right\}\]

さらに一般化すると、訓練データセット$ S = \{ ({x^{(i)}},{t^{(i)}});i = 1,2,...,m\} $が与えられた場合、ロジスティック回帰の損失関数は以下のように表現できます：

\[L(w,b; S) =  - \sum\limits_{i = 1}^m {{t^{(i)}}\log {y^{(i)}} + \left( {1 - {t^{(i)}}} \right)\log \left( {1 - {y^{(i)}}} \right)} \]

したがって、ロジスティック回帰の学習問題は損失関数$L(w,b)$を最小化する最適化問題に変換されます。

\[\hat w,\hat b = \arg \mathop {\min }\limits_{w,b} L(w, b)\]

\subsection{正則化項}

モデルの複雑さを制御するために、$L2$正則化項を追加することができます。これはベクトル$w$の内積に等しくなります。

\[R(w) = \lambda \parallel w\parallel _2^2 = {w^T}w \]

ここで、$\lambda$は正則化項係数と呼ばれ、正則化項の影響度を制御するために使用されます。これはモデルのハイパーパラメータであり、交差検証実験を通じて最適な数値を取得できます。正則化項を追加した後、モデルの損失関数は以下のように表現できます：

\[J(w,b) = L(w,b) + \lambda \parallel w\parallel _2^2\]

はい、続けて翻訳します。

\]

その最適化問題も次のように表現されます：

\[\hat w,\hat b = \arg \mathop {\min }\limits_{w,b} J(w, b)\]

\subsection{勾配降下法}

勾配降下法アルゴリズムを使用して、最適な$(\hat w,\hat b)$を反復的に求めることができます。ここで、$\alpha$は学習率を表し、パラメータ更新の大きさを示します。

\[\begin{aligned}
  w \leftarrow & w - \alpha {\nabla _w}L(w,b) \\ 
  b \leftarrow & b - \alpha {\nabla _b}L(w,b) \\ 
\end{aligned} \]

ここで、$w,b$の勾配はそれぞれ次のように定義されます：

\[\begin{aligned}
  {\nabla _w}L(w,b) = & \frac{{\partial L}}{{\partial w}} \\ 
  {\nabla _b}L(w,b) = & \frac{{\partial L}}{{\partial b}} \\ 
\end{aligned} \]

図\refig{mnist-gd}に示すように、損失関数を山に例えることができ、登山者は最適な行動計画を見つけて谷底に到達しようとします。登山者は特定の斜面に立って周囲を見回し、勾配の反対方向に小さな一歩を踏み出すことを決定し、より良い解が得られるまでこれを続けます。勾配降下法の更新アルゴリズムを実施する際、初期点が異なると得られる最小値も異なる可能性があります。したがって、勾配降下法で得られるのは局所最小値にすぎません。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/mnist-gd.jpeg}
\caption{勾配降下法アルゴリズム}
 \label{fig:mnist-gd}
\end{figure}

勾配降下法のステップサイズは非常に重要です。小さすぎると、関数の最小値を見つける速度が遅くなります。大きすぎると、極値点を超えてしまう可能性があります。一般的に、モデルのトレーニング初期段階では、モデルの収束目標からまだ遠いため、学習率を大きめに設定します。反復回数が増えるにつれて、学習率を小さくします。したがって、学習率$\alpha$は適応的であり、現在、理論的には時間とともに減衰する学習率$\alpha$の最適化アルゴリズムが多数存在します（例：\ascii{Adagrad}など）。したがって、最適化問題の鍵は勾配の計算方法にあります。

\subsection{勾配の計算}

連鎖律に基づいて、任意のサンプル$ (x,t) \in S $に対して、$ L(y, t) $の$ w_i, i=1,2,...,n $に関する勾配の公式を導出できます。

\[\begin{aligned}
  \frac{{\partial L}}{{\partial {w_i}}}
   =  & \frac{{\partial L}}{{\partial y}}\frac{{\partial y}}{{\partial z}}\frac{{\partial z}}{{\partial {w_i}}} \\ 
   =  & \left( {\frac{{1 - t}}{{1 - y}} - \frac{t}{y}} \right)y(1 - y){x_i} \\ 
   =  & (y - t){x_i} \\ 
\end{aligned} \]

同様に、$ L(W,b) $の$ b $に関する勾配の公式を導出できます。

\[\begin{aligned}
  \frac{{\partial L}}{{\partial b}} 
   =  & \frac{{\partial L}}{{\partial y}}\frac{{\partial y}}{{\partial z}}\frac{{\partial z}}{{\partial b}} \\ 
   =  & \left( {\frac{{1 - t}}{{1 - y}} - \frac{t}{y}} \right)y(1 - y) \\ 
   =  & y - t \\ 
\end{aligned} \]

さらに、ベクトル化により$w,b$の勾配計算公式を得ることができます。ここで、$\nabla _w}L( {w,b;x,t}) \in {\mathbb{R}^{n}}$、$\nabla _b}L( {w,b;x,t}) \in {\mathbb{R}}$です。

\[\begin{gathered}
  {\nabla _w}L( {w,b;x,t}) =  \left({y - t} \right)x \\ 
  {\nabla _b}L( {w,b;x,t) =   y - t  \\ 
\end{gathered} \]

さらに一般化して、訓練データセット$ S = \{ ({x^{(i)}},{t^{(i)}});i = 1,2,...,m\} $が与えられた場合、$w,b$のベクトル化された勾配計算公式は次のようになります：

\[\begin{gathered}
  {\nabla _w}L(w,b;S) = \left( {\frac{1}{m}\sum\limits_{i = 1}^m {\left( {{y^{(i)}} - {t^{(i)}}} \right)} } \right)x \\ 
  {\nabla _b}L(w,b;S) = \frac{1}{m}\sum\limits_{i = 1}^m {\left( {{y^{(i)}} - {t^{(i)}}} \right)}  \\ 
\end{gathered} \]

\subsection{パラメータの更新}

パラメータの更新には3つの基本的なアルゴリズムが存在します。与えられた訓練サンプル$ ({x^{(i)}},{t^{(i)}}) $に対して、$w, b$の勾配公式に基づいて、この反復のパラメータ更新を完了します。このアルゴリズムは一般に\emph{確率的勾配降下法}（\ascii{SGD}）と呼ばれます。パラメータを1回更新するために、\ascii{SGD}は1つの訓練サンプルのみを読み取る必要がありますが、単一のサンプルは一般性を代表できず、多くのノイズが存在する可能性があるため、モデルの収束状況は比較的不安定です。しかし、\ascii{SGD}はモデルの高速な収束を実現でき、効率が高いです。

\[\begin{gathered}
  w \leftarrow w - \alpha \left( {{y^{(i)}} - {t^{(i)}}} \right)x \\ 
  b \leftarrow b - \alpha \left( {{y^{(i)}} - {t^{(i)}}} \right) \\ 
\end{gathered} \]

もう一つの極端な方法として、訓練サンプルデータ全体$ S $が与えられた場合、$w, b$の勾配公式に基づいて、この反復のパラメータ更新を完了します。このアルゴリズムは一般に\emph{バッチ勾配降下法}（\ascii{BGD}）と呼ばれます。パラメータを1回更新するために、訓練データセット全体を走査する必要があるため、モデルの収束は比較的安定しています。しかし、計算量が大きいため、モデルの収束速度は\ascii{SGD}と比較して比較的遅いです。

\[\begin{gathered}
  w \leftarrow w - \alpha \left( {\frac{1}{m}\sum\limits_{i = 1}^m {\left( {{y^{(i)}} - {t^{(i)}}} \right)} } \right)x \\ 
  b \leftarrow b - \alpha \left( {\frac{1}{m}\sum\limits_{i = 1}^m {\left( {{y^{(i)}} - {t^{(i)}}} \right)} } \right) \\ 
\end{gathered} \]

実際のアプリケーションでは、\ascii{BGD}も\ascii{SGD}も使用せず、\ascii{MiniBatch}の\ascii{SGD}アルゴリズムを使用します。さらに、\ascii{MiniBatch}の\ascii{SGD}が広く適用されているため、一般的に\ascii{MiniBatch}の\ascii{SGD}を単に\ascii{SGD}アルゴリズムと呼びます。

\[\begin{aligned}
  w \leftarrow w - \alpha \left( {\frac{1}{{batch\_zie}}\sum\limits_{i = 1}^{batch\_size} {\left( {{y^{(i)}} - {t^{(i)}}} \right)} } \right)x \\ 
  b \leftarrow b - \alpha \left( {\frac{1}{{batch\_size}}\sum\limits_{i = 1}^{batch\_size} {\left( {{y^{(i)}} - {t^{(i)}}} \right)} } \right) \\ 
\end{aligned} \]

\ascii{BGD}と比較して、\ascii{MiniBatch}の\ascii{SGD}は各パラメータ更新時に訓練データセット全体を走査する必要はなく、\ascii{batch\_size}個の訓練サンプルを走査するだけで済みます。したがって、\ascii{BGD}と比較して、\ascii{MiniBatch}の\ascii{SGD}の収束速度はより速くなります。一方、\ascii{SGD}と比較して、\ascii{MiniBatch}の\ascii{SGD}は各パラメータ更新時に、より一般性を持つ\ascii{batch\_size}個の訓練サンプルを読み取ります。したがって、\ascii{SGD}と比較して、\ascii{MiniBatch}の\ascii{SGD}の収束はより安定します。

\subsection{計算グラフ}

一般に、ニューラルネットワークを使用して学習モデルを表現する場合、そのトレーニングプロセスは次の2つの基本的なステップを含みます：

\begin{itemize}
  \item \ascii{順伝播}: 損失の計算
  \item \ascii{逆伝播}: 勾配の計算
\end{itemize}

順伝播での損失計算と逆伝播での勾配計算を実現するために、ネットワークを等価な計算グラフに変換できます。順伝播の計算グラフでは、ノードは行列乗算、ベクトル内積、活性化関数などの抽象的な数学演算を表し、エッジは関数の出力値を表し、下流のノード関数の入力となります。

図\refig{logistic-bp}に示すように、ロジスティック回帰の順伝播

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/logistic-bp.png}
\caption{ロジスティック回帰：順伝播と逆伝播}
 \label{fig:logistic-bp}
\end{figure}

\end{content}

\section{単層パーセプトロン}

\begin{content}

まず、\ascii{10}個のニューロンを持つ単層パーセプトロンの構築を試みます。図\refig{mnist-slp}に示すように、手書き数字認識のような多クラス分類問題に対して、理論的には通常\ascii{softmax}活性化関数を使用します。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/mnist-slp.png}
\caption{単層パーセプトロン}
 \label{fig:mnist-slp}
\end{figure}

\subsection{理論的基礎}

理論的には、\ascii{softmax}回帰は\ascii{logistic}回帰の一般化された拡張です。\ascii{logistic}回帰は二値分類問題を解決するためのものであり、$y \in \{ 0,1\}$です。一方、\ascii{softmax}回帰は$ k $クラス分類問題を解決するためのものであり、$y \in \{ 1,2,...,k\}$です。

\subsubsection{記号の定義}

\ascii{softmax}回帰問題を形式的に記述するために、ここでいくつかの一般的な記号を定義します。

 \begin{itemize}
   \item \ascii{訓練サンプルセット}: $ S = \{ ({x^{(i)}},{y^{(i)}});i = 1,2,...,m\} $
   \item \ascii{第$i$個の訓練サンプル}: $ ({x^{(i)}},{y^{(i)}}) $
   \item \ascii{サンプル入力}: $ x = ({x_1},{x_2},...,{x_n})^{T}  \in {\mathbb{R}^n} $
   \item \ascii{サンプルラベル(one-hot)}: $ y = ({y_1},{y_2},...,{y_k})^{T} \in {\mathbb{R}^k} $
   \item \ascii{重み}: $ W \in {\mathbb{R}^{n \times k}} $   
   \item \ascii{バイアス}: $ b \in {\mathbb{R}^k} $   
   \item \ascii{softmax関数}: $ 
softmax {(z_i)} = \tfrac{{{e^{{z_i}}}}}{{\sum\limits_{j = 1}^k {{e^{{z_j}}}} }}  \quad i = 1,2,...,k
$
 \end{itemize}

\subsubsection{softmax関数}

図\refig{softmax}に示すように、モデルはまず線形重み付け和$z$を求め、次に$e^z$を求め、最後に正規化操作を実施します。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/softmax.png}
\caption{softmax関数}
 \label{fig:softmax}
\end{figure}

はい、引き続き翻訳を続けます。

\subsubsection{重みとバイアス}

重み$W$は$n \times k$の二次元行列です。

\[
W = \left( {{W_1},{W_2},...,{W_k}} \right) = \left( {\begin{array}{*{20}{c}}
  {{w_{11}}}& \ldots &{{w_{1k}}} \\ 
   \vdots & \ddots & \vdots  \\ 
  {{w_{n1}}}& \cdots &{{w_{nk}}} 
\end{array}} \right) \in {\mathbb{R}^{n \times k}}
\]

ここで、$W_j$は長さ$n$のベクトルです。

\[
{W_j} = {\left( {{w_{1j}},{w_{2j}},...,{w_{nj}}} \right)^T} \in {\mathbb{R}^n}, j = 1,2,...,k \\
\]

一方、バイアス$b$は長さ$k$の\ascii{\quo{one-hot}}ベクトルです。

\[
b = {({b_1},{b_2},...,{b_k})^T} \in {\mathbb{R}^k}
\]

\subsubsection{モデルの定義}

多クラス分類問題の単層パーセプトロンモデルは、\ascii{softmax}活性化関数を使用して、以下のように定義できます。

\[\begin{aligned}
  y =  & {h_{W,b}}(x) = softmax (z) = softmax ({W^T}x + b) \\ 
   =  & {\left( {{y_1},{y_2},...,{y_k}} \right)^T} \\ 
   =  & \frac{1}{{\sum\limits_{j = 1}^k {{e^{{z_j}}}} }}{\left( {{e^{{z_1}}},{e^{{z_2}}},...,{e^{{z_k}}}} \right)^T} \\ 
   =  & \frac{1}{{\sum\limits_{j = 1}^k {{e^{W_j^Tx + {b_j}}}} }}{\left( {{e^{W_1^Tx + {b_1}}},{e^{W_2^Tx + {b_2}}},...,{e^{W_k^Tx + {b_k}}}} \right)^T} \ 
\end{aligned} \]

ここで、任意の与えられたサンプル$ (x, y) \in S $に対して、$ z_i $は$W_i^Tx+b_i$の線形重み付け和を表し、$y_i(i=1,2,...,k)$はそれをクラス$i$に分類する確率を表します。

\[\begin{gathered}
  P\left( {y = i|x;W,b} \right) = \frac{{{e^{W_i^Tx + b_i}}}}{{\sum\limits_{j = 1}^k {{e^{W_j^Tx + b_j}}} }} \hfill \\
  i = 1,2,...,k \hfill \\ 
\end{gathered} \]

\subsubsection{交差エントロピー関数}

サンプルデータセット$ S = \{ ({x^{(i)}},{y^{(i)}});i = 1,2,...,m\} $に基づいて、交差エントロピー損失関数は以下のように定義できます。

\[\begin{aligned}
  J(W,b) =  &  - \frac{1}{m}\sum\limits_{i = 1}^m {{y^{(i)}}\log \left( {{{\widehat y}^{(i)}}} \right)}  \\ 
   =  &  - \frac{1}{m}\sum\limits_{i = 1}^m {\sum\limits_{j = 1}^k {y_j^{(i)}\log \left( {\widehat y_j^{(i)}} \right)} }  \\
\end{aligned} \]

\ascii{softmax}多クラス分類問題は、以下を満たす最適解$(W^*,b^*)$を求めることです。

\[W^*,b^* = \mathop {\arg \min }\limits_{W,b} J(W,b)\]

\subsubsection{勾配の計算}

任意のサンプル$ (x,y) \in S $に対して、$ J(W,b) $の$ W $と$ b $に関する勾配の公式を導出できます。

\[\begin{aligned}
  {\nabla _W}J\left( {W,b;x,y} \right) =  & \left( {\widehat y - y} \right)x \\ 
  {\nabla _b}J\left( {W,b;{x^{(i)}},{y^{(i)}}} \right) =  & \left( {\widehat y - y} \right) \\ 
\end{aligned} \]

\subsubsection{パラメータの更新}

訓練サンプルデータ$ S $に対して、$W, b$の勾配公式に基づいて、この反復のパラメータ更新を完了します。

\[\begin{aligned}
  W \leftarrow  & W - \alpha \frac{{\sum\limits_{i = 1}^m {{\nabla _W}J\left( {W,b;{x^{(i)}},{y^{(i)}}} \right)} }}{m} \\ 
  b \leftarrow  & b - \alpha \frac{{\sum\limits_{i = 1}^m {{\nabla _b}J\left( {W,b;{x^{(i)}},{y^{(i)}}} \right)} }}{m} \\ 
\end{aligned} \]

\subsection{モデルの定義}

次に、\tf{}を使用してこのモデルの構築とトレーニングを完了します。理論上の公式と\tf{}の具体的な実装には微妙な違いがあることに注意してください。理論上、公式の$x$は通常1つのサンプルを表しますが、\tf{}の\code{x}は通常\ascii{mini-batch}のサンプルデータセットを表します。したがって、\tf{}を使用してネットワークモデルを設計する際には、各テンソルのサイズの変化が予想通りであるかどうかに特に注意する必要があります。

\subsubsection{入力とラベル}

まず、\code{tf.placeholder}を使用して、訓練サンプルの入力とラベルをそれぞれ定義します。

\begin{leftbar}
\begin{python}
x = tf.placeholder(tf.float32, [None, 28, 28, 1])
t = tf.placeholder(tf.float32, [None, 10])
\end{python}
\end{leftbar}

\code{tf.placeholder}は占位のOPを定義します。\code{None}は未確定のサンプル数を表し、ここでは\code{batch\_size}の大きさを表します。\code{Session.run}時に、\code{feed\_dict}辞書を通じて\ascii{mini-batch}のサンプルデータセットを提供することで、\code{tf.placeholder}のサイズが自動的に推論されます。

また、各画像は$ 28 \times 28 \times 1 $の三次元データ（グレースケールは\ascii{1}）で表されます。問題を簡略化するために、ここでは入力サンプルデータを平坦化し、長さ\ascii{784}の一次元ベクトルに変換します。ここで、\ascii{-1}は\ascii{mini-batch}のサンプル数を表し、実行時に自動的にサイズが推論されます。

\begin{leftbar}
\begin{python}
x = tf.reshape(x, [-1, 784])
\end{python}
\end{leftbar}

\subsubsection{変数の定義}

次に、\code{tf.Variable}を使用してモデルパラメータを定義します。トレーニングパラメータを定義する際には、パラメータの初期値を指定する必要があります。トレーニングパラメータは初期値に基づいて、データの型とサイズを自動的に推論します。

\begin{leftbar}
\begin{python}
w = tf.Variable(tf.zeros([784, 10]))
b = tf.Variable(tf.zeros([10]))
\end{python}
\end{leftbar}

さらに、変数を使用する前に初期化を完了する必要があります。ここで、\code{init\_op}はすべてのグローバルトレーニングパラメータを初期化します。

\begin{leftbar}
\begin{python}
init_op = tf.global_variables_initializer()
\end{python}
\end{leftbar}

\subsubsection{モデルの定義}

次に、多クラス分類問題の単層パーセプトロンモデルを簡単に得ることができます。

\begin{leftbar}
\begin{python}
y = tf.nn.softmax(tf.matmul(x, w) + b)
\end{python}
\end{leftbar}

図\refig{mnist-linear-sum}に示すように、まず\code{x}と\code{w}の行列乗算を計算し、次に\code{b}を行列の各行にブロードキャスト（\ascii{broadcast}）して加算し、最終的にトレーニングパラメータの線形重み付け和を得ます。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/mnist-linear-sum.png}
\caption{線形重み付け和}
 \label{fig:mnist-linear-sum}
\end{figure}

図\refig{mnist-softmax}に示すように、\ascii{softmax}は行ごとに演算を実施し、最終的に\code{y}のサイズは\code{[100, 10]}になります。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/mnist-softmax.png}
\caption{活性化関数：softmax}
 \label{fig:mnist-softmax}
\end{figure}

\subsubsection{損失関数}

多クラス分類問題に対して、交差エントロピーの損失関数を使用できます。

\begin{leftbar}
\begin{python}
cross_entropy = -tf.reduce_sum(t * tf.log(y))
\end{python}
\end{leftbar}

図\refig{mnist-cross-entropy}に示すように、\code{t}と\code{y}のサイズはどちらも\code{[100, 10]}です。特に、\code{t}の各行は\quo{\ascii{one-hot}}ベクトルです。

\code{y}に対して\code{tf.log}操作を実施すると、サイズ\code{[100, 10]}の行列が得られます。次に、\code{t}と\code{tf.log(y)}を要素ごとに乗算（行列乗算ではない）すると、サイズ\code{[100, 10]}の行列が得られます。最後に、\code{tf.reduce\_sum}が行列内のすべての要素を加算し、スカラー（\ascii{scalar}）値を得ます。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/mnist-cross-entropy.png}
\caption{交差エントロピー損失関数}
 \label{fig:mnist-cross-entropy}
\end{figure}

\subsubsection{精度}

\code{tf.argmax(y,1)}は第\ascii{1}次元に沿って最大値のインデックスを計算します。つまり、$ y_{100 \times 10} $の各行に対して、その行の最大値のインデックス値を計算します。したがって、\code{tf.argmax(y,1)}はサイズ\code{[100, 1]}の行列、またはサイズ\ascii{100}のベクトルを得ます。同様に、\code{tf.argmax(t,1)}もサイズ\ascii{100}のベクトルです。

次に、\code{tf.equal}を使用してそれらを要素ごと（\ascii{element-wise}）に等価性比較し、サイズ\ascii{100}のブールベクトルを得ます。精度を計算するために、まずブールベクトルを数値ベクトルに変換し、最終的にその数値ベクトルの平均を求めます。

\begin{leftbar}
\begin{python}
is_correct = tf.equal(tf.argmax(y,1), tf.argmax(t,1))
accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))
\end{python}
\end{leftbar}

\subsection{最適化アルゴリズム}

次に、勾配降下法アルゴリズムを使用して交差エントロピー損失関数の最小化を実現します。ここで、\code{learning\_rate}は学習率を表し、パラメータ更新の速さとステップサイズを記述する典型的なハイパーパラメータです。

\begin{leftbar}
\begin{python}
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.003)
train_step = optimizer.minimize(cross_entropy)
\end{python}
\end{leftbar}

\subsection{モデルのトレーニング}

ここまでは、\tf{}は計算グラフを構築しただけで、計算グラフの実行は開始していません。次に、クライアントがセッションを作成し、ローカルまたはリモートの計算デバイス集合とのチャネルを確立し、計算グラフの実行プロセスを開始します。

まず、トレーニングパラメータの初始化を完了します。モデルパラメータの初期化サブグラフを実行し、並行して各トレーニングパラメータの初期化子を実行し、初期値を対応するトレーニングパラメータにその場で修正します。

\begin{leftbar}
\begin{python}
with tf.Session() as sess:
  sess.run(init_op)
\end{python}
\end{leftbar}

次に、\code{train\_step}を繰り返し実行し、モデルの1回の反復トレーニングを完了します。ここで、100回の反復ごとに、現在のモデルのトレーニングデータセットとテストデータセットでの精度と損失を計算します。

\begin{leftbar}
\begin{python}
with tf.Session() as sess:
  for step in range(1000):
    batch_xs, batch_ys = mnist.train.next_batch(100)        
    sess.run(train_step, feed_dict={x: batch_xs, t: batch_ys})
    
    if step % 100 == 0:
      acc, loss = sess.run([accuracy, cross_entropy], 
        feed_dict={x: batch_xs, t: batch_ys})
      acc, loss = sess.run([accuracy, cross_entropy], 
        feed_dict={x: mnist.test.images, t: mnist.test.labels}) 
\end{python}
\end{leftbar}

統計によると、1000回の反復後、約\percent{92}の精度が得られます。

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/mnist-slp-accuracy.png}
\caption{可視化：単層パーセプトロン、1000回のステップ実行}
 \label{fig:mnist-slp-accuracy}
\end{figure}

\end{content}
