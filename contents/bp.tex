\begin{savequote}[45mm]
\ascii{コンピュータが理解できるコードを書くのは誰でもできる。優れたプログラマは人間が理解できるコードを書く。}
\qauthor{\ascii{- マーティン・ファウラー}}
\end{savequote}

\chapter{BPアルゴリズム} 
\label{ch:bp}

\begin{content}

\end{content}

\section{TensorFlowによる実装}

\begin{content}

\tf{}は自動微分を実装したソフトウェアシステムです。まず、順方向の計算グラフを構築し、計算グラフの順伝播を実現します。\code{Optimizer.minimize}メソッドが呼び出されると、\code{compute\_gradients}メソッドを使用して逆方向計算グラフの構築を実現し、\code{apply\_gradients}メソッドを使用してパラメータ更新のサブグラフ構築を実現します。

\begin{leftbar}
\begin{python}
class Optimizer(object):
  def minimize(self, loss, var_list=None, global_step=None):
    """Add operations to minimize loss by updating var_list.
    """
    grads_and_vars = self.compute_gradients(
      loss, var_list=var_list)
    return self.apply_gradients(
      grads_and_vars, 
      global_step=global_step)
\end{python}
\end{leftbar}

\subsection{勾配の計算}

\code{compute\_gradients}は\code{loss}の値に基づいて、\code{var\_list=[v1, v2, ..., vn]}の勾配を求め、最終的に次の結果を返します：\code{[(grad\_v1, v1), (grad\_v2, v2), ..., (grad\_vn, vn)]}。ここで、\code{compute\_gradients}は\code{gradients}メソッドを呼び出して、逆伝播のサブグラフを構築します。

簡単な例を用いて、逆方向サブグラフの構築プロセスを説明します。まず、順方向の計算グラフを構築します。

\begin{leftbar}
\begin{python}
X = tf.placeholder("float", name="X")
Y = tf.placeholder("float", name="Y")
w = tf.Variable(0.0, name="w")
b = tf.Variable(0.0, name="b")
loss = tf.square(Y - X*w - b)
global_step = tf.Variable(0, trainable=False, collections=[])
\end{python}
\end{leftbar}

\code{compute\_gradients}を使用して逆伝播のサブグラフを構築します。

\begin{leftbar}
\begin{python}
sgd = tf.train.GradientDescentOptimizer(0.01)
grads_and_vars = sgd.compute_gradients(loss)
\end{python}
\end{leftbar}

\subsubsection{構築アルゴリズム}

逆方向サブグラフの構築アルゴリズムは形式的に次のように記述できます：

\begin{leftbar}
\begin{python}
def gradients(loss, grad=I):
  vrg = build_virtual_reversed_graph(loss)
  for op in vrg.topological_sort():
    grad_fn = ops.get_gradient_function(op)
    grad = grad_fn(op, grad)
\end{python}
\end{leftbar}

まず、順方向サブグラフのトポロジーグラフに基づいて、仮想の逆方向サブグラフを構築します。仮想と呼ばれるのは、実際の逆方向サブグラフがそれよりもはるかに複雑だからです。より正確に言えば、仮想の逆方向サブグラフの1つのノードは、実際の逆方向サブグラフの局所的なサブグラフに対応します。

同時に、順方向サブグラフの最後のノードの出力勾配は、すべて1の\code{Tensor}となり、これが逆方向サブグラフの初期勾配値として使用され、しばしば\code{I}と記されます。

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.7\textwidth]{figures/bp-back-graph-construction.png}
\caption{逆伝播サブグラフの構築}
 \label{fig:bp-back-graph-construction}
\end{figure}

次に、仮想の逆方向サブグラフに基づいて実際の逆方向サブグラフを構築します。まず、この逆方向の仮想サブグラフに対してトポロジカルソートアルゴリズムを実行し、この仮想の逆方向サブグラフのトポロジカルソートを得ます。次に、このトポロジカルソートに従って、順方向サブグラフの各\ascii{OP}に対して「勾配関数」を探します。最後に、その勾配関数を呼び出し、その勾配関数がその\ascii{OP}に対応する逆方向の局所サブグラフを構築します。

以上をまとめると、順方向の1つの\ascii{OP}は逆方向の局所サブグラフに対応し、その\ascii{OP}の勾配関数が構築を担当します。トポロジカルソートアルゴリズム全体が完了すると、順方向サブグラフの各\ascii{OP}は逆方向サブグラフで対応する局所サブグラフを見つけることができます。

例えば、上記の例では、順方向グラフの最後の\ascii{OP}である二乗関数を例に、勾配関数の動作原理を説明します。

\subsubsection{勾配関数のプロトタイプ}

一般に、勾配関数は次のようなプロトタイプを満たします：

\begin{leftbar}
\begin{python}
@ops.RegisterGradient("op_name")
def op_grad_func(op, grad):
\end{python}
\end{leftbar}

ここで、勾配関数は\code{ops.RegisterGradient}によって登録され、勾配関数を保存するリポジトリに配置されます。その後、順方向\ascii{OP}の名前に基づいて対応する勾配関数をインデックス化できるようになります。

勾配関数の場合、最初のパラメータ\code{op}は順方向計算の\ascii{OP}を表し、これに基づいて順方向計算時の\ascii{OP}の入力と出力を取得できます。2番目のパラメータ\code{grad}は、逆方向サブグラフの上流ノードから伝播された勾配であり、これはすでに計算された勾配値です（初期勾配値はすべて1）。

\subsubsection{実践：二乗関数}

簡単な例を挙げて、入力のみを使用して勾配を計算します。\code{y=square(x)}は\code{x}の二乗を求めるために使用されます。まず、順方向計算グラフを構築します：

\begin{figure}[!h]
\centering
\includegraphics[width=0.5\textwidth]{figures/bp-square-forward-graph.png}
\caption{Square関数：順伝播サブグラフ}
 \label{fig:bp-square-forward-graph}
\end{figure}

次に、仮想の逆方向サブグラフを構築します。この仮想の逆方向サブグラフのトポロジカルソートに基づいて、実際の逆方向計算サブグラフを構築します。現在のノードが\code{Square}であると仮定すると、そのOP名に基づいて、リポジトリから対応する勾配関数\code{SquareGrad}を見つけます。

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.5\textwidth]{figures/bp-square-backward-graph.png}
\caption{Square関数：逆伝播サブグラフ}
 \label{fig:bp-square-backward-graph}
\end{figure}

\code{y=Square(x)}の導関数が\code{y'=2*x}であるため、その勾配関数\code{SquareGrad}の実装は次のようになります：

\begin{leftbar}
\begin{python}
@ops.RegisterGradient("Square")
def SquareGrad(op, grad):
  x = op.inputs[0]
  with ops.control_dependencies([grad.op]):
    x = math_ops.conj(x)
    return grad * (2.0 * x)
\end{python}
\end{leftbar}

この勾配関数を呼び出すと、順方向の\code{Square}の\ascii{OP}に対応する逆方向サブグラフ\code{SquareGrad}が得られます。これは\code{Square}の入力を使用して、対応する勾配計算を完了する必要があります。

\begin{figure}[!h]
\centering
\includegraphics[width=0.5\textwidth]{figures/bp-square-backward-graph-2.png}
\caption{Square関数：逆伝播サブグラフ}
 \label{fig:bp-square-backward-graph-2}
\end{figure}

一般に、順方向サブグラフの1つの\ascii{OP}は、逆方向サブグラフの局所サブグラフに対応します。これは、順方向\ascii{OP}の勾配関数の実装が、対応する勾配計算を完了するために複数の\ascii{OP}を必要とする可能性があるためです。例えば、\code{Square}の\ascii{OP}に対応する勾配関数は、2つの乗算\ascii{OP}を含むサブグラフを構築しています。

\subsubsection{実践：指数関数}

もう一つの簡単な例を挙げて、出力のみを使用して勾配を計算します。\code{y=exp(x)}は指数関数です。その導関数は\code{y'=exp(x)}、つまり\code{y'=y}です。したがって、その勾配関数の実装は次のようになります：

\begin{leftbar}
\begin{python}
@ops.RegisterGradient("Exp")
def _ExpGrad(op, grad):
  """Returns grad * exp(x)."""
  y = op.outputs[0]
  with ops.control_dependencies([grad.op]):
    y = math_ops.conj(y)
    return grad * y
\end{python}
\end{leftbar}

下図に示すように、順方向サブグラフのこの\ascii{OP}の出力は、対応する逆方向の局所サブグラフの勾配計算に使用されます。さらに、この局所サブグラフは1つのノードしか含んでいません。

\begin{figure}[!h]
\centering
\includegraphics[width=0.5\textwidth]{figures/bp-exp-backward-graph.png}
\caption{Exp関数：逆伝播サブグラフ}
 \label{fig:bp-exp-backward-graph}
\end{figure}

\subsection{勾配の適用}

簡単にまとめると、\code{Optimizer.minimize}メソッドが呼び出されると、\code{compute\_gradients}メソッドを使用して逆方向計算グラフの構築を実現し、\code{apply\_gradients}メソッドを使用してパラメータ更新のサブグラフ構築を実現します。

\subsubsection{構築アルゴリズム}

まず、\code{compute\_gradients}は実行時に\code{loss}の値に基づいて\code{var\_list=[v1, v2, ..., vn]}の勾配を求め、最終的に次の結果を返します：\code{vars\_and\_grads = [(grad\_v1, v1), (grad\_v2, v2), ..., (grad\_vn, vn)]}。

次に、\code{apply\_gradients}は\code{grads\_and\_vars}を反復処理し、各\code{(grad\_vi, vi)}に対して\code{vi}を更新するサブグラフを構築します。アルゴリズムは形式的に次のように記述できます：

\begin{leftbar}
\begin{python}
def apply_gradients(grads_and_vars, learning_rate):
  for (grad, var) in grads_and_vars:
    apply_gradient_descent(learning_rate, grad, var)
\end{python}
\end{leftbar}

ここで、\code{apply\_gradient\_descent}は勾配降下法を使用してパラメータを更新する計算サブグラフを構築します。\code{(grad, var)}の2つ組と\code{learning\_rate}の\code{Const OP}を\code{ApplyGradientDescent}の入力として使用します。

\code{ApplyGradientDescent}は\code{var <- var - learning*grad}の演算規則を適用し、\code{var}のインプレース更新を実現します。

\begin{figure}[!h]
\centering
\includegraphics[width=0.6\textwidth]{figures/bp-update-w.png}
\caption{パラメータ更新サブグラフ}
 \label{fig:bp-update-w}
\end{figure}

\subsubsection{パラメータ更新の集約}

複数のトレーニング用Variableが存在する場合、最終的に複数のパラメータ更新の局所サブグラフが生成されます。これらは\code{update}という名前の\code{NoOp}を通じて、制御依存エッジを使用して集約されます。各\code{Variable}は互いに独立しているため、最大限の並行性を実現できます。

\begin{figure}[!h]
\centering
\includegraphics[width=0.9\textwidth]{figures/bp-update-all-params.png}
\caption{パラメータ更新の集約}
 \label{fig:bp-update-all-params}
\end{figure}

\subsubsection{train\_opの探求}

1ラウンドの\ascii{Step}計算を経て、勾配に基づいてパラメータの更新を完了し、最終的に\code{global\_step}を1増加させます。\code{global\_step}を1増加させる\ascii{OP}は\code{AssignAdd}であり、\code{train\_op}としてマークされます。これは\code{global\_step}変数への参照を保持し、その後インプレース修正を行って値を1増加させます。

\begin{figure}[!h]
\centering
\includegraphics[width=0.9\textwidth]{figures/bp-train-op.png}
\caption{train\_op}
 \label{fig:bp-train-op}
\end{figure}

\subsubsection{ワークフロー}

\refig{bp-train-pipeline}に示すように、トレーニングプロセス全体において、1回の\ascii{Step}のトレーニングプロセスは、順伝播計算、逆伝播勾配計算、パラメータ更新、および\code{global\_step}カウントの4つの基本プロセスで構成されています。

各ラウンドの\ascii{Step}は\code{Session.run}の実行から始まります。順方向サブグラフの計算を通じて、各\ascii{OP}の出力を得て、それを下流の\ascii{OP}の入力として使用します。

順方向サブグラフの計算が完了すると、初期勾配ベクトル$ I $を入力として、各トレーニングパラメータの勾配を逆方向に計算し、最終的に各トレーニングパラメータの勾配リストを得ます。これは\code{grads\_and\_vars = [(grad\_v1, v1), ..., (grad\_vn, vn)]}の2つ組のリストとして表されます。

その後、パラメータ更新サブグラフは\code{grads\_and\_vars}を入力として、勾配降下の更新アルゴリズムを実行します。最後に、\code{train\_op}を通じて\code{global\_step}の値を1増加させ、これで1ラウンドの\ascii{Step}の実行が完了します。

\begin{figure}[!h]
\centering
\includegraphics[width=0.9\textwidth]{figures/bp-train-pipeline.png}
\caption{モデルトレーニングのワークフロー}
 \label{fig:bp-train-pipeline}
\end{figure}

\end{content}
